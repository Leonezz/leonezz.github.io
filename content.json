{"meta":{"title":"zhuwenq","subtitle":"","description":"","author":"zhuwenq","url":"https://blog.zhuwenq.cc","root":"/"},"pages":[{"title":"About Me","date":"2021-07-29T06:50:30.000Z","updated":"2023-06-21T06:51:28.948Z","comments":false,"path":"about/index.html","permalink":"https://blog.zhuwenq.cc/about/index.html","excerpt":"","text":"Intro I am currently a postgraduate major in computer science, my research interests include Natural Language Processing, Parameter Efficient Transfer Learning and Information Extraction. I am a person of passion in both technique and life (at least as what I think : ) ), my passion involves modern C++, embedded systems, deep learning, reading books, watching movies, photography and making coffee. I will write here something about programming, reading and learning. Please leave a comment or contact me directly if you have something to share, I appreciate it a lot! I hope you can enjoy my articles here. Best regards! Contact me through: Email: zhuwenqa@outlook.com I will graduate in March next year if it goes well, and I am currently in a job hunting, so please check out my cv if it may. Check out my resume at zhuwenq-resume.netlify.app!"},{"title":"categories","date":"2021-07-23T05:04:34.000Z","updated":"2023-06-21T06:51:28.948Z","comments":false,"path":"categories/index.html","permalink":"https://blog.zhuwenq.cc/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-07-23T05:03:33.000Z","updated":"2023-06-21T06:51:28.948Z","comments":false,"path":"tags/index.html","permalink":"https://blog.zhuwenq.cc/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Modern C++ Concurrency Utilities 03: atomic","slug":"Modern-C-Concurrency-Utilities-03-atomic","date":"2023-06-21T06:29:49.000Z","updated":"2023-06-21T06:51:28.832Z","comments":true,"path":"Modern-C-Concurrency-Utilities-03-atomic/","link":"","permalink":"https://blog.zhuwenq.cc/Modern-C-Concurrency-Utilities-03-atomic/","excerpt":"Atomic 提供了开发高性能无锁 (lock free) 结构的工具和接口, 对于内置类型或自定义类型, 也可以通过 atomic&lt;&gt; 模板包装成支持原子操作的类型, 以实现并发安全.","text":"Atomic 提供了开发高性能无锁 (lock free) 结构的工具和接口, 对于内置类型或自定义类型, 也可以通过 atomic&lt;&gt; 模板包装成支持原子操作的类型, 以实现并发安全. atomic 引用 cppreference 的说法: std::atomic 模板的每个实例化和全特化都定义了一个原子类型. 如果一个线程写入一个原子对象的同时另一个线程读取这个原子对象, 其行为是明确定义的. 并且, 对原子对象的访问可能会建立线程间的同步关系, 并将对非原子内存的访问排序为传入的 std::memory_order 参数所定义的顺序. (memory_order 详见 内存屏障和-C-内存模型 ) Each instantiation and full specialization of the std::atomic template defines an atomic type. If one thread writes to an atomic object while another thread reads from it, the behavior is well-defined (see memory model for details on data races). In addition, accesses to atomic objects may establish inter-thread synchronization and order non-atomic memory accesses as specified by std::memory_order. 按照这个描述, 对于任意的自定义类型 (需要 CopyConstructible and CopyAssignable), 只要给它套上 std::atomic 这个 wrapper, 就可以定义一个原子的该对象. 但需要注意的是, 原子并不意味着 lock free. C++ 会在模板实例化时决定传入的自定义类型是否可以实现 lock free 的原子操作, 如果不能, 那么其内部仍然是通过 mutex 等有锁结构实现原子操作. C++ 11 中为指针类型提供了偏特化, 为 bool 类型和 所有 的整形类型都提供了 atomic 模板的全特化, 它们中应该多数都是 lock free 的. C++ 20 则进一步为浮点类型提供了全特化以及为 std::shared_ptr&lt;U&gt; 和 std::weak_ptr&lt;U&gt; 提供了偏特化, 而是否 lock free 则取决于 U 的类型. 细节参考: std::atomic - cppreference.com 对于任意的 atomic 模板实例化或全特化, 其都会提供以下操作: is_lock_free: 检查 atomic 对象是否是 lock free 的, C++ 保证 std::atomic_flag 一定是 lock free 的, 对于其他类型在不同平台上可能有不同实现. store: 原子地 为 atomic 对象赋值, 可以指定 memory_order load: 原子地 读取 atomic 对象的值, 可以指定 memory_order exchange: 原子地 为 atomic 对象赋值并读取它原先的值, 是 RMW (Read Modify Write) 操作. compare_exchange_weak and compare_exchange_strong: 原子地 加载并 bit wise 比较 atomic 对象的值和 expected 参数值 如果它们不相等, 则把 atomic 对象的值赋给 expected 参数(expected 是个引用), 并返回 false. 如果它们相等, 则把 desired 参数值赋给 atomic 对象. 值得注意的是, weak 版本和 strong 版本有如下差异: weak 版本可能会 伪失败 (fail spuriously), 也就是说, 即使 atomic 对象的值和 expected 相等, 函数也可能产生它们不相等时的行为 (赋值 expected 并返回 false), 而 strong 版本不会. 当在循环中使用 CAS 时, weak 版本会有更好的性能: 12while(atomic.compare_and_exchange(expected, desired)) ; 对于一些值相同但位表示可能不同, 即多个位表示对应同一个值的类型 (如, 浮点数的 NaN 有多个位表示, 一些维护了内部状态但这些内部状态不决定对象的值的类), 通常使用 weak 版本更有效, 因为通常对于这些类的 CAS 操作都需要循环以使之收敛到稳定的位表示. 如果可以使用 strong 版本的 CAS 并且不需要循环时, strong 版本更加高效. C++ 20 为 atomic 对象引入了三个主动式的线程同步操作: wait: 阻塞当前线程, 直到: 相同的 atomic 对象调用了 notify 函数 atomic 对象的值与传入的参数值 old 不同. wait 方法用于多线程并发编程时实现线程对原子变量值的变化监测, 需要注意的是其中的比较是 bit wise 比较. notify_one: 如果存在因在当前 atomic 对象上调用 wait 函数而阻塞的线程, 那么就唤醒 至少一个 这样的线程, 如果不存在这样的线程该函数无效果. notify_all: 唤醒所有因在当前 atomic 对象上调用 wait 函数而阻塞的线程. 对于标准库内置的特化版本, atomic 还提供了一系列 read, write, RMW 操作, 包括算术和逻辑操作. 另外 atomic 所有的成员函数都有对应的非成员函数模板, 可以为非 atomic 类型重载出对应的操作. 详见: cppreference.com. atomic_ref C++ 20 中引入了一个新的原子类, 它的语义是对一个非原子对象的「引用」, 特殊之处在于通过该「引用」进行的操作都是原子的! 这意味着 atomic_ref 相当于在已有的非原子对象上施加了一个支持原子操作的接口, 而并非一个逻辑上独立的新对象. 当我们在程序中仅在一部分地方需要对变量施加原子操作时, atomic_ref 十分有用: 如果使用 atomic, 那么对其的访问在整个程序中都必然是原子性的. 换一个角度理解 atomic_ref: 它相当于为「原子对象」提供了引用语义. C++ 11 中的 atomic 对象是值语义的, 它的构造函数接受一个非原子对象的「值」，并且在内部维护该值. 后续对该原子变量的操作实际上都是在操作原子对象本身. 而 atomic_ref 则不然, 它相当于并不拥有背后的非原子对象, 而只是持有对其的一个引用, 特殊之处仅在于这个引用上的操作都是原子的. 以下面的程序为例说明上述概念: 123456789101112131415// In sequential_fun, there are no concurrent running,// no data races, therefore no need for atomic operationvoid sequential_fun(int&amp; cnt) &#123; ++atomic;&#125;// concurrent_fun will be running in multiple threads,// potentially multiple cores.void concurrent_fun(int&amp; cnt) &#123; std::atomic&lt;int&gt; atomic_cnt&#123;cnt&#125;; // an atomic int with the same value as cnt ++atomic_cnt; // not as expected: the increament should be performed to the reference parameter // unless do this: // cnt = atomic_cnt; // but it is non-atomic and what if the parameter is a large object&#125; 上面的程序中, 我们仅在函数 concurrent_fun 中需要原子操作, 并且原子操作的结果是通过引用传递的. 此时 atomic 的值语义就较为麻烦, 无法安全高效地实现对引用的修改. 而 atomic_ref 可以胜任这种情况: 1234void concurrent_fun(int&amp; cnt) &#123; std::atomic_ref&lt;int&gt; atomic_cnt&#123;cnt&#125;; ++cnt; // ok, increament is performed atomically to cnt&#125; 除了语义上的区别外, atomic_ref 与 atomic 在形式上区别不大, 都提供了相似的全特化和偏特化已经相似的成员函数. 详情参考: std::atomic_ref - cppreference.com memory_order memory_order 是 atomic 相关操作中常见的参数, 它用来控制在原子操作中对内存 (非原子多级缓存结构的内存) 的访问进行排序. 包括内存写入操作执行后对于其他线程的可见性顺序. C++ 中抽象了 5 中 memory_order, 详见 std::memory_order - cppreference.com, 它们是非常重要且复杂的概念, 已经在文章 内存屏障和-C-内存模型 做了初步介绍. References std::atomic - cppreference.com std::atomic_ref - cppreference.com c++ - What exactly is std::atomic? - Stack Overflow A simple guide to atomics in C++. There’s often confusion around when… | by Josh Weinstein | Dev Genius std::atomic_ref - cppreference.com C++20 atomic_ref (mariusbancila.ro)","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://blog.zhuwenq.cc/tags/Concurrency/"},{"name":"atomic","slug":"atomic","permalink":"https://blog.zhuwenq.cc/tags/atomic/"}]},{"title":"Modern C++ Concurrency Utilities 02: CPU Cache and false sharing","slug":"Modern-C-Concurrency-Utilities-02-CPU-Cache-and-false-sharing","date":"2023-06-20T08:15:09.000Z","updated":"2023-06-21T06:51:28.828Z","comments":true,"path":"Modern-C-Concurrency-Utilities-02-CPU-Cache-and-false-sharing/","link":"","permalink":"https://blog.zhuwenq.cc/Modern-C-Concurrency-Utilities-02-CPU-Cache-and-false-sharing/","excerpt":"C++ 17 提供了两个编译时常量, 用于表示 CPU Cache 的一些参数. 合理使用这两个编译时常量可以避免在多级 Cache 结构的计算机上并发程序频繁访问同一个对象不同成员时可能发生的 伪共享 (false sharing) 问题.","text":"C++ 17 提供了两个编译时常量, 用于表示 CPU Cache 的一些参数. 合理使用这两个编译时常量可以避免在多级 Cache 结构的计算机上并发程序频繁访问同一个对象不同成员时可能发生的 伪共享 (false sharing) 问题. 这两个编译时常量是: 12inline constexpr std::size_t hardware_destrutive_interference_size; // 下简称: hdisinline constexpr std::size_t hardware_constructive_interence_size; // 下简称: hcis False sharing 首先说一说什么是伪共享. 在 内存屏障和-C-内存模型 中我们知道现代 CPU 的 Cache 是多级层次的, 不同的 CPU Core 可能持有独立的 Cache, 这种结构很有可能出现 Cache 不一致问题. 那么多个 Core 同时读写同一个内存位置时就是一种 data race. MESI 协议通过为 Cache line 引入四种状态并在需要时广播消息解决了缓存一致性的问题. 具体地说, 当 Core A 和 Core B 的独立 Cache 中都持有字节 C 的缓存时, 如果 Core A 写入字节 C 的同时 Core B 也要访问字节 C, 那么 Core B 必须先 invalidate 字节 C 所在的 Cache line, 并重新从下一级 Cache 中加载包含字节 C 的 Cache line (细节参考: 内存屏障和-C-内存模型). 值得注意的是, Cache 的最小操作单元是 Cache line, 而一个 Cache line 通常包含几十上百个字节. 那么考虑一种情况: Core A 和 Core B 并发访问 (读写) 字节 C 和字节 C 的相邻字节 D 时, 会发生什么? 答案显而易见, 相邻字节 C 和 D 极有可能被分配到同一个 Cache line (细节参考 Cache), 这就造成明明并行程序没有访问同一个内存位置, 也就是并不存在 data race, 但 Core A 和 Core B 的 Cache 却需要频繁地 invalidate 整个 Cache line 并重新加载, 结果就是程序执行效率的下降. 这就是 false sharing 的情况. Scott Meyers 关于 CPU Cache 的一次 talk 中提到一个很好的例子, 说明了 false sharing 问题对多线程并行程序 scalability 的影响. 详情参考: https://www.aristeia.com/TalkNotes/codedive-CPUCachesHandouts.pdf How to avoid false sharing 的避免手段主要是将可能在不同 Core 中并发访问的数据布局成合适的距离, 使得它们不可能被塞到同一个 Cache line. 例如对于如下的结构: 1234struct ConcurrentStruct &#123; int threadA_val&#123;&#125;; int threadB_val&#123;&#125;;&#125;; 如果成员 threadA_val 和 threadB_val 分别在两个 Core 中被频繁读写, 我们可以尝试在它们中间插入一些 padding 字节以强迫它们配分配到不同 Cache line (对于 64 bytes 大小的 cache line): 1234567struct FastConcurrentStruct &#123; const char padding1[60]; int threadA_val&#123;&#125;; const char padding2[60]; int threadB_val&#123;&#125;; const char padding3[60];&#125;; 这种 padding 不仅使得 threadA_val 和 threadB_val 无法加载到同一个 Cache line, 也避免了它们和程序其它部分的值加载到同一个 cache line. 那么在不同 Cache line 大小的 CPU 上如何写出统一的避免 false sharing 的代码呢? 操作系统的内核可能会提供一些特殊的宏定义. 那么如何写出跨操作系统统一的代码呢? 这就是上面介绍的两个 C++ 编译时常量 hdis, hcis 的作用. False sharing avoiding in modern C++ 引用 cppreference 提供的例子: 1234struct keep_apart &#123; alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; cat; alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; dog;&#125;; hdis 表示要避免 false sharing 两个变量的地址所需要的 最小 差值. 下面的测试程序说明了它的效果: 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;new&gt;struct no_fs &#123; alignas(std::hardware_destructive_interference_size) int a&#123;&#125;; alignas(std::hardware_destructive_interference_size) int b&#123;&#125;;&#125;;struct fs &#123; int a&#123;&#125;; int b&#123;&#125;;&#125;;int main() &#123; no_fs nf&#123;&#125;; std::cout &lt;&lt; &quot;No false sharing: &quot; &lt;&lt; &#x27;\\n&#x27; &lt;&lt; &amp;nf &lt;&lt; &quot;\\n&quot; &lt;&lt; &amp;(nf.a) &lt;&lt; &quot;\\n&quot; &lt;&lt; &amp;(nf.b) &lt;&lt; &quot;\\n&quot; &lt;&lt; std::hardware_destructive_interference_size &lt;&lt; &quot;\\n&quot;; fs f&#123;&#125;; std::cout &lt;&lt; &quot;false sharing: &quot; &lt;&lt; &#x27;\\n&#x27; &lt;&lt; &amp;f &lt;&lt; &quot;\\n&quot; &lt;&lt; &amp;(f.a) &lt;&lt; &quot;\\n&quot; &lt;&lt; &amp;(f.b) &lt;&lt; &quot;\\n&quot;;&#125;/** output:No false sharing: 0x7ffe0174ff000x7ffe0174ff000x7ffe0174ff4064false sharing: 0x7ffe0174fef80x7ffe0174fef80x7ffe0174fefc**/ 可以看出测试平台 (https://godbolt.org/z/M3bs16cY6) 的 Cache line 大小是 64 字节, 并且 no_fs 结构体中的成员地址都按照 64 字节对齐. 而 fs 结构的成员则是 trivial layout, 即成员是紧密排列的. 另外一个常量 hcis 表示 true sharing 的最大连续内存长度, 即可能被放入同一个 cache line 的最大字节数. 用于增大整个结构或连续的变量被放入同一个 cache line 的可能性, 不详细介绍, 可以参考 cppreference 给出的例子: std::hardware_destructive_interference_size, std::hardware_constructive_interference_size - cppreference.com. References Scott Meyers. 2013. Cpu Caches and Why You Care. In code::dive conference. video. slide std::hardware_destructive_interference_size, std::hardware_constructive_interference_size - cppreference.com","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"CPU Cache","slug":"CPU-Cache","permalink":"https://blog.zhuwenq.cc/tags/CPU-Cache/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://blog.zhuwenq.cc/tags/Concurrency/"}]},{"title":"Modern C++ Concurrency Utilities 01: thread","slug":"Modern-C-Concurrency-Utilities-01-thread","date":"2023-06-19T14:35:31.000Z","updated":"2023-06-21T06:51:28.828Z","comments":true,"path":"Modern-C-Concurrency-Utilities-01-thread/","link":"","permalink":"https://blog.zhuwenq.cc/Modern-C-Concurrency-Utilities-01-thread/","excerpt":"C++ 11 及之后的版本中提供了一系列并发编程的工具和对并发控制原语的抽象封装, 如 thread, atomic, mutex, condition_varible 等. 本文介绍 thread 以及 jthread 的基本语义和功能, 它们在实际场景中的 best practice 和与之密切相关的编程模式不在本文讨论范围之内.","text":"C++ 11 及之后的版本中提供了一系列并发编程的工具和对并发控制原语的抽象封装, 如 thread, atomic, mutex, condition_varible 等. 本文介绍 thread 以及 jthread 的基本语义和功能, 它们在实际场景中的 best practice 和与之密切相关的编程模式不在本文讨论范围之内. Thread 线程提供一种允许多个过程并发执行的方式, C++ 中的 thread 则是提供一组平台无关的接口, 为用户屏蔽了不同操作系统所提供的线程 API 的差异. 在 linux 操作系统上, 使用 thread 的程序编译时往往需要 -lpthread 选项以显式链接 pthread (POSIX thread) 库. thread C++ 11 中提供的 thread 类接受一个可调用对象和参数作为构造参数, 并且 thread 对象在构造完成后立刻开始执行 (将线程标记为 ready 状态等待操作系统调度). 传入 thread 构造函数的可调用对象的返回值会被忽略, 并且如果该可调用对象异常终止, 该线程会调用 std::terminate, 导致进程终止. 因此不应该依赖返回值或异常实现多线程之间的同步和通信, 可以使用 std::promise 或 同步 的共享变量. C++ 11 中为 thread 对象提供的控制操作极少, 只有 join 和 detach 两种: join: 阻塞 当前 线程, 等待调用 join 函数的那个 thread 对象结束执行 detach: 将调用 detach 函数的线程与 当前 线程剥离并独立执行, 当前线程不再拥有该 thread 对象. 值得注意的是, thread 还提供了 joinable 操作, 用于检查一个 thread 对象是否可以调用 join 函数. 这个检查的意义在于, 对于 thread_id 与当前线程相同的 thread 对象, 一旦调用 join, 该线程就会一直阻塞: join 的语义是阻塞直到调用者执行结束, 而执行者又恰好正是这个被阻塞的线程, 那么它将永远无法醒来. 幸而在 C++ 中我们无法对 joinable() 返回 false 的 thread 对象调用 join, 否则将有多少死锁的线程出现. joinable 函数的实现其实非常简单, 就是判断当前线程和调用该函数的那个对象的 id 是否相同: this-&gt;get_id() != std::thread::id(). jthread jthread 是 C++ 20 引入的新线程类, 它的功能和 thread 一致, 并非更高级的线程, 只是对 thread API 不合理的设计提供一个 wrapper. 与 thread 相同, jthread 也提供了相同语义的 join 和 detach 函数. 不同的是, 对于一个 thread 对象, 如果你在它析构之前没有调用 join 或 detach 函数, 那么它就会在析构函数调用 terminate 终止整个进程, 这带来很多不便: 1234~thread() &#123; if (joinable()) std::terminate();&#125; 而 jthread 则会在析构函数中请求终止该线程并自动 join: 123456~jthread() &#123; if (joinable()) &#123; request_stop(); join(); &#125;&#125; 同时 jthread 还提供了一个显式终止线程执行的机制, 即 std::stop_source 和 std::stop_token. 每个 jthread 对象都持有一个 std::stop_source, 它代表一个共享的终止状态. 在线程开始执行时, jthread 会将该共享状态传入可调用对象作为第一个参数, 这就要求可调用对象的接口接受一个 std::stop_token 作为第一个参数. 当外部线程对 jthread 对象 request_stop 时, 实际上就是在尝试设置 jthread 内部的 std::stop_source 状态. 需要注意的是, 设置了 std::stop_token 并不代表真的可以停止线程执行, 真正的线程停止逻辑还需要在用户代码显式地设计. 例如下面的例子: 12345678910111213void f(std::stop_token stop_token, int value) &#123; while (!stop_token.stop_requested()) &#123; std::cout &lt;&lt; value++ &lt;&lt; &#x27; &#x27; &lt;&lt; std::flush; std::this_thread::sleep_for(200ms); &#125; std::cout &lt;&lt; std::endl;&#125;int main() &#123; std::jthread thread(f, 5); // prints 5 6 7 8... for approximately 3 seconds std::this_thread::sleep_for(3s); // The destructor of jthread calls request_stop() and join().&#125; 如果没有 std::this_thread::sleep_for(3s), 那么 jtherad 对象会在析构时自动 request_stop, 使得函数 f 及时停止, 最终可能一个数字也不会输出. 而这一行为是由于 f 函数中显式地以 stop_token 参数的状态作为循环条件实现的. 其他功能 yield: 让出 CPU, 等待下次调度. 显然地, 在某些情况下 (如当前线程是唯一线程), 该函数没有效果. get_id: 获取线程 id sleep_for: 睡眠 (阻塞) 一段时间, 由于调度策略该时间可能比传入的时间稍长 sleep_until: 睡眠 (阻塞) 直到给定的时间点, 时间点并 不 会被精确遵守 C++ 20 中为线程终止所提供的三个工具: stop_token: 用于查询是否有 stop_request 被发起的接口 stop_source: 用于表示 stop_request 的请求情况 stop_callback: 用于为线程终止注册 callback 的接口 这三个类的实现和关系比较简单. 首先 stop_token 和 stop_source 中都持有 _Stop_state_ref 类型的成员, 而这个 _Stop_state_ref 则是一个引用计数型的 pointer wrapper, 类似 shared_ptr. 其内部持有的 _Stop_state_t 类型的指针指向的是真正的 stop request state. 从这里可以看出是这个 _Stop_state_t 指针将 stop_token 和 stop_source 关联起来的, 即它们都间接通过 _Stop_state_ref 指向同一个位于堆上的 _Stop_state_t. stop_callback 则是一个 RAII 类, 在构造函数中它将可调用对象即 callback 注册到 _Stop_state_t, 并在析构函数中注销. 且注册和注销的实现则是在 _Stop_state_t 中持有一个双向链表的头节点, 双向链表的节点元素则是包装为 _Stop_cb 的 callback 本体, 这样就可以在注册时插入节点, 注销时删除节点, stop request 时执行链表中所有的节点. 同时标准保证所有节点的执行是同步的. References Concurrency support library (since C++11) - cppreference.com c++ - When should you use std:🧵:joinable? - Stack Overflow c++ - What is std::jthread in c++20? - Stack Overflow","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://blog.zhuwenq.cc/tags/Concurrency/"},{"name":"thread","slug":"thread","permalink":"https://blog.zhuwenq.cc/tags/thread/"}]},{"title":"C++ storage duration and specifiers","slug":"C-storage-duration-and-specifiers","date":"2023-06-19T08:17:18.000Z","updated":"2023-06-21T06:51:28.780Z","comments":true,"path":"C-storage-duration-and-specifiers/","link":"","permalink":"https://blog.zhuwenq.cc/C-storage-duration-and-specifiers/","excerpt":"偶然在 vscode 的代码提示中看到了 thread_local 这个关键字, 心想 C++ 中什么时候提供了这种东西. 翻了下 cppreference 才知道早在 C++ 11 中就引入了这个修饰符, 用于声明线程生命周期. 趁此机会系统地看了下 C++ 变量的 4 种生命周期, 特此记录.","text":"偶然在 vscode 的代码提示中看到了 thread_local 这个关键字, 心想 C++ 中什么时候提供了这种东西. 翻了下 cppreference 才知道早在 C++ 11 中就引入了这个修饰符, 用于声明线程生命周期. 趁此机会系统地看了下 C++ 变量的 4 种生命周期, 特此记录. C++ 11 中提供四种变量存储周期, 分别是: automatic: 自动存储周期, 变量的存储空间会在其所处的代码块开始时分配, 并在结束时释放. static: 静态存储周期, 变量的存储空间在程序开始时分配, 并在结束时释放. 在程序运行周期中该变量只存在一个实例. thread: 线程存储周期, 变量的存储空间在其所属的线程启动时分配, 并在线程结束时释放. 每个线程中该变量都存在一个实例. dynamic: 动态存储周期, 变量存储空间的分配和释放由用户代码控制. 对于上述的存储周期, 在 C++ 中可以通过以下的 specifiers 显式地声明: no specifiers: automatic. 在 C++ 中所有变量默认是自动存储周期的, 除非是全局变量 (file scope or namespace scope), 或显式声明了 static/extern 的变量. register: automatic. register 修饰符只允许用在局部变量或函数参数上, 它是一种编译器建议, 语义是建议编译器将该变量存储在寄存器中作为一种优化. 此修饰符在 C++ 17 中已经废弃. static: static. static 修饰符有多种语义, 用在 局部 变量的声明中表示该变量具有静态存储周期. extern: 不影响变量的存储周期, 只表示该变量 (符号) 具有 external linkage. 但是由于 extern 声明符只能用在全局变量或函数的 声明 中, 因此凡是被 extern 修饰的变量都具有 static 或 thread 存储周期. thread_local: thread. thread_local 是 C++ 11 中引入的关键字, 用于修饰全局变量, 局部变量或 静态 成员变量. 表示该变量具有 thread 存储周期. 用在局部变量上时, 隐含有 static 语义. 用在全局变量上时, 可以和 static 或 extern 合用, 表示 internal/external linkage. References Storage class specifiers - cppreference.com","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"storage duration","slug":"storage-duration","permalink":"https://blog.zhuwenq.cc/tags/storage-duration/"},{"name":"basic concepts","slug":"basic-concepts","permalink":"https://blog.zhuwenq.cc/tags/basic-concepts/"}]},{"title":"C++ black magic: 利用友元函数和显式实例化突破访问权限","slug":"C-black-magic-利用友元函数和显式实例化突破访问权限","date":"2023-06-03T14:34:16.000Z","updated":"2023-06-21T06:51:28.780Z","comments":true,"path":"C-black-magic-利用友元函数和显式实例化突破访问权限/","link":"","permalink":"https://blog.zhuwenq.cc/C-black-magic-%E5%88%A9%E7%94%A8%E5%8F%8B%E5%85%83%E5%87%BD%E6%95%B0%E5%92%8C%E6%98%BE%E5%BC%8F%E5%AE%9E%E4%BE%8B%E5%8C%96%E7%AA%81%E7%A0%B4%E8%AE%BF%E9%97%AE%E6%9D%83%E9%99%90/","excerpt":"C++ 中的非继承类有两种访问权限, private 和 public. 一般来说, private 只允许类型内部使用, 不允许外部访问. 然而, 由于 C++ 语言巨大的复杂性, 这一约束存在若干漏洞. 一种简单的做法就是假设 C++ 编译器不会对类对象的内存布局做奇怪的扰动, 这一假设通常是正确的, 因此可以通过猜测私有成员相对对象整体的地址偏移量访问私有成员. 或者更加方便地通过定义一个成员完全相同但访问权限相反的类, 并将原类对象的指针转换成新类的指针的方式访问原类对象的私有指针. 这两种方法本质上都基于「假设」, 即假设 C++ 编译器不会对类型的成员做特殊的处理. 本文介绍一种更加优雅的方法, 可以稳定地实现对私有成员的访问.","text":"C++ 中的非继承类有两种访问权限, private 和 public. 一般来说, private 只允许类型内部使用, 不允许外部访问. 然而, 由于 C++ 语言巨大的复杂性, 这一约束存在若干漏洞. 一种简单的做法就是假设 C++ 编译器不会对类对象的内存布局做奇怪的扰动, 这一假设通常是正确的, 因此可以通过猜测私有成员相对对象整体的地址偏移量访问私有成员. 或者更加方便地通过定义一个成员完全相同但访问权限相反的类, 并将原类对象的指针转换成新类的指针的方式访问原类对象的私有指针. 这两种方法本质上都基于「假设」, 即假设 C++ 编译器不会对类型的成员做特殊的处理. 本文介绍一种更加优雅的方法, 可以稳定地实现对私有成员的访问. 基本的思路是利用友元函数, 由于友元函数这一特殊函数不属于成员, 而它又具有将类型的成员带到全局命名空间的能力. 具体的代码如下: 对于一个含有私有成员的类: 1234class Private &#123;private: int a = 1;&#125;; 我们想通过一个「小偷」类偷取它的私有成员: 1234567891011template &lt;auto Member&gt;class theft &#123;&#125;;template &lt;typename T, typename U, T U::*Member&gt;class theft&lt;Member&gt; &#123; friend T&amp; steal(U&amp; u) &#123; return u.*Member; &#125;&#125;;int&amp; steal(Private&amp;); 可以看到上面定义的「小偷」类是一个模板, 它有一个万能的主模板, 可以匹配任意类型. 同时有一个特化模板, 特化模板的模板参数 Member 被声明为类型为指向类型为 T 的对象的指针, 同时这个 Member 指针指向的是类型 U 的成员: T U::*Member. 同时注意到, 特化的「小偷」模板中有一个友元函数 steal 的定义, 在这个定义中, 它直接返回了对未知类型 U 的未知成员指针 Member 解引用的结果. 同时需要注意, steal 有一个全局作用域声明. 显然地, 如果 Member 指向的是类型 U 的私有成员, 那么「小偷」类型将无法构造出具体的对象, 编译器会在检查到对私有成员访问的时候发出抱怨. 但 C++ 中可以对模板类 显示实例化, 而且这个显示实例化过程是不会检查成员访问权限的. 因此如果我们写下下面的代码: 1template class theft&lt;&amp;Private::a&gt;; 「小偷」模板就会实例化成功, 模板参数 T, U, Member 分别会被替换为 int, Private, &amp;Private::a. 此时我们仍然构造这个类, 但是仔细分析, 在实例化成功时, 函数 int steal(Private&amp;) 有了一个定义, 即友元定义. 那么我们在调用这个函数时, 编译器可以找到合适的定义, 自然不会发出任何抱怨 ! 12345678#include &lt;iostream&gt;using std::cout;int main() &#123; Private p; endl(cout &lt;&lt; steal(p)); // 1 steal(p) = 100; endl(cout &lt;&lt; steal(p)); // 100&#125; 整个过程具有一种声东击西的感觉, 不够聪明的编译器只能缴械投降. 代码: https://godbolt.org/z/brs6MPcfc Reference http://eel.is/c++draft/temp.spec#6 https://www.zhihu.com/question/521898260","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"黑魔法","slug":"黑魔法","permalink":"https://blog.zhuwenq.cc/tags/%E9%BB%91%E9%AD%94%E6%B3%95/"}]},{"title":"C++ 对象、虚函数与继承","slug":"C-对象、虚函数与继承","date":"2023-04-14T08:00:26.000Z","updated":"2023-06-21T06:51:28.780Z","comments":true,"path":"C-对象、虚函数与继承/","link":"","permalink":"https://blog.zhuwenq.cc/C-%E5%AF%B9%E8%B1%A1%E3%80%81%E8%99%9A%E5%87%BD%E6%95%B0%E4%B8%8E%E7%BB%A7%E6%89%BF/","excerpt":"C++ 中以继承的方式实现动态多态是存在额外开销的, 具体地说，C++ 对象使用了额外的内存保存关于基类信息才实现了运行时多态. 但这种额外信息究竟以何种方式在对象的内存中报错对我来说仍是未知. 受到前人文章的启发, 本文借助 GDB 调试器一步步地观察记录 C++ 对象的内存布局, 力求理解实际 C++ 编译器 (gcc 9.4.0) 对 C++ 对象的内存布局以及派生类的实现.","text":"C++ 中以继承的方式实现动态多态是存在额外开销的, 具体地说，C++ 对象使用了额外的内存保存关于基类信息才实现了运行时多态. 但这种额外信息究竟以何种方式在对象的内存中报错对我来说仍是未知. 受到前人文章的启发, 本文借助 GDB 调试器一步步地观察记录 C++ 对象的内存布局, 力求理解实际 C++ 编译器 (gcc 9.4.0) 对 C++ 对象的内存布局以及派生类的实现. 从空类谈起 对于空的类, 我们都知道它理论上的 size 应该是 0, 但是实际使用 sizeof 运算符的结果是 1. 这是因为为了实现对对象的引用, 对象必须在内存中占有一定的位置才能拥有一个地址. C++ 编译器对这一要求的实现就是为空类分配一个字节的内存. 12345678910#include &lt;iostream&gt;using std::cout;class EmptyBase &#123;&#125;;int main() &#123; EmptyBase empty&#123;&#125;; // output: 1 1 cout &lt;&lt; sizeof(EmptyBase) &lt;&lt; &quot; &quot; &lt;&lt; sizeof(empty); return 0;&#125; 对上述代码调试的结果也支持这一结论: 123456(gdb) p empty$1 = &#123;&lt;No data fields&gt;&#125;(gdb) p sizeof(empty)$2 = 1 # sizeof empty is 1(gdb) p sizeof(EmptyBase)$3 = 1 # sizeof EmptyBase (type) is 1 当类型非空时, 例如类型中有成员变量时, 编译器就不再需要特殊的空字节以维持对象在内存中的存在了. 12345678class NonEmptyBase &#123; int i&#123;&#125;; int j&#123;&#125;&#125;;int main() &#123; NonEmptyBase b&#123;&#125;; return 0;&#125; 对象的地址就是其第一个成员变量的地址, 并且可以看出对于多个成员变量, 其地址是按照在类中定义的顺序连续分配的. 12345678910111213(gdb) p b$3 = &#123; i = 0, j = 0&#125;(gdb) p sizeof b$4 = 8(gdb) p &amp;b$5 = (NonEmptyBase *) 0x7fffffffd7f8(gdb) p &amp;b.i$6 = (int *) 0x7fffffffd7f8(gdb) p &amp;b.j$7 = (int *) 0x7fffffffd7fc 虚函数的影响 当类的定义中存在虚函数时, 编译器会在对象的内存中额外存储一个 vptr(虚表指针), 指向一个虚函数表, 虚函数表中的条目即虚函数指针. 12345678910class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;int main()&#123; EmptyBase eb&#123;&#125;; return 0;&#125; 对于上面定义的 EmptyBase, 其运行时调试信息是: 12345678(gdb) p eb$1 = &#123; _vptr.EmptyBase = 0x555555557d78 &lt;vtable for EmptyBase+16&gt;&#125;(gdb) p sizeof(eb)$2 = 8(gdb) p &amp;eb$3 = (EmptyBase *) 0x7fffffffd7f8 可以看出对象的内存中存有一个 _vptr, 其值是一个 64 位整数, 代表一个地址. 打印这个地址处的内容: 12(gdb) p /a *(void **)0x555555557d78$2 = 0x55555555522a &lt;EmptyBase::foo()&gt; 可以看到其内容是 EmptyBase 的成员函数 EmptyBase::foo. 考虑到 EmptyBase 中定义了两个虚函数, 猜测第二个虚函数位置应该在 EmptyBase::foo 后面: 12$2 = 0x55555555522a &lt;EmptyBase::foo()&gt;(gdb) p /a *(void **)(0x555555557d78 + 8) 根据这些信息可以大致画出对象 eb 的内存布局: 派生类的内存模型 存在继承关系时, 对象的内存中应该含有基类的成员. 123456789101112131415class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class Derived : public EmptyBase &#123; public: void foo() override &#123; endl(cout &lt;&lt; &quot;Derived foo&quot;); &#125;&#125;;int main() &#123; Derived eb&#123;&#125;; return 0;&#125; eb 的调试信息为: 123456789(gdb) p eb$4 = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557d60 &lt;vtable for Derived+16&gt; &#125;, &lt;No data fields&gt;&#125;(gdb) p sizeof(eb)$5 = 8(gdb) p &amp;eb$6 = (Derived *) 0x7fffffffd7f8 很简单, 似乎 eb 直接把派生类的部分放进自己的内存. 打印 _vptr 指向的虚表: 1234(gdb) p /a *(void **)(0x555555557d60)$2 = 0x555555555258 &lt;Derived::foo()&gt;(gdb) p /a *(void **)(0x555555557d60 + 8)$3 = 0x55555555522a &lt;EmptyBase::bar()&gt; 可以看到对于派生类中没有重写的函数 EmptyBase::bar, 虚表中保存的仍然是派生类的实现的地址; 而对于重写的函数 foo, 虚表中的条目被修改为了派生类的实现地址. 注意这一过程是编译时发生的, 因此对继承的这种形式的使用还称不上「多态」. 派生类的非 override 虚函数 需要注意的是, override 是一种语义而非机制, 并非声明为 override 的函数才是重写函数. 实际上, 只要是派生类中的函数定义 (virtual or not) 在基类中存在同名虚函数, 那么这个派生类的函数就被称为 override 了基类中的虚函数版本. 那么对于派生类中的非 override 虚函数, C++ 是如何处理的呢? 12345678910111213141516class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class Derived : public EmptyBase &#123; public: void foo() override &#123; endl(cout &lt;&lt; &quot;Derived foo&quot;); &#125; virtual void foobar () &#123;&#125;&#125;;int main() &#123; Derived d; return 0;&#125; 上面的实现中, 函数 Derived::foobar 就是一个非 override 虚函数, 我们通过对象 d 打印出它的内存布局: 1234567891011(gdb) p d$1 = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557d50 &lt;vtable for Derived+16&gt; &#125;, &lt;No data fields&gt;&#125;(gdb) p /a *(void **)(0x555555557d50)$2 = 0x55555555529e &lt;Derived::foo()&gt;(gdb) p /a *(void **)(0x555555557d50 + 8)$3 = 0x555555555270 &lt;EmptyBase::bar()&gt;(gdb) p /a *(void **)(0x555555557d50 + 16)$5 = 0x5555555552cc &lt;Derived::foobar()&gt; 可以看出, 内存中仍然是只有基类的虚表指针存在, 并且需表中存在 3 个函数指针, 分别是: 派生类 override 的 Derived::foo 基类定义的 EmptyBase::Bar 派生类特有的 Derived::foobar 可以得出结论: 对于派生类中的非 override 虚函数, C++ 的处理方式是将它的指针放在虚表中基类函数条目的后面. 多态的实现 在继承这一语境中, 「多态」这一术语通常指运行时多态, 即指向基类的指针在运行时动态地决定调用函数的哪一份实现. 下面模拟类似的场景: 1234567891011121314151617181920212223242526class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class Derived : public EmptyBase &#123; public: void foo() override &#123; endl(cout &lt;&lt; &quot;Derived foo&quot;); &#125;&#125;;class Derived2nd : public EmptyBase &#123; public: void bar() override &#123; endl(cout &lt;&lt; &quot;Derived2nd bar&quot;); &#125;&#125;;int main() &#123; EmptyBase* b = new Derived; b-&gt;foo(); b-&gt;bar(); b = new Derived2nd; b-&gt;foo(); b-&gt;bar(); return 0;&#125; 上面的实现中, 派生类 Derived 和 Derived2nd 分别重写了基类 EmptyBase 的 foo 和 bar 函数. 在主函数中, 通过一个基类的指针 b 分别调用 b-&gt;foo(), b-&gt;bar(), 输出的结果是: 1234Derived fooEmptyBase barEmptyBase fooDerived2nd bar 可以看出, 当基类指针 b 指向的是类型 Derived 的对象时, b-&gt;foo() 调用的是 Derived::foo, b-&gt;bar() 调用的是 EmptyBase::bar(). 当运行时将指针 b 指向 Derived2nd 的对象后, 再次对同样的指针调用 foo 和 bar 函数, 实际调用的函数变为了 EmptyBase::foo(), EmptyBase::bar(). 这样在运行时可以动态改变被调用函数的行为即运行时多态. 下面观察 Derived 对象和 Derived2nd 对象的内存布局: 首先在第 19 行位置设置断点, 然后打印 b 指针 (此时指向 Derived 对象) 的信息 12345678(gdb) p *b$2 = &#123; _vptr.EmptyBase = 0x555555557d20 &lt;vtable for Derived+16&gt;&#125;(gdb) p /a *(void **)(0x555555557d20)$4 = 0x55555555532a &lt;Derived::foo()&gt;(gdb) p /a *(void **)(0x555555557d20 + 8)$5 = 0x5555555552fc &lt;EmptyBase::bar()&gt; 然后在第 23 行位置设置断点, 继续执行程序, 打印 b 指针 (此时指向 Derived2nd 对象) 的信息 1234567$6 = &#123; _vptr.EmptyBase = 0x555555557d00 &lt;vtable for Derived2nd+16&gt;&#125;(gdb) p /a *(void **)(0x555555557d00)$7 = 0x5555555552ce &lt;EmptyBase::foo()&gt;(gdb) p /a *(void **)(0x555555557d00 + 8)$8 = 0x555555555358 &lt;Derived2nd::bar()&gt; 可以看出上述对象内存和虚表信息的模式与直接使用对象时没有区别. 运行时多态的实现得益于每个对象内存中的虚表: 调用虚函数时, 只需要查询虚表然后决定要调用哪个版本的实现就可以实现多态. 使用基类指针时, 随着基类指针所指向的对象的不同, 调用虚函数时所查询的虚表自然也不同. 按道理说, 虚表存在于对象的内存中, 那么只要及时更新对象, 使用基类的对象而非指针应该也可以是实现运行时多态. 即下面的代码: 1234567891011int main() &#123; EmptyBase b; b = Derived(); b.foo(); b.bar(); b = Derived2nd(); b.foo(); b.bar(); return 0;&#125; 可实际上上面的代码输出的结果是: 1234EmptyBase fooEmptyBase barEmptyBase fooEmptyBase bar 这是因为复制对象时, C++ 不会复制对象的虚表指针, 因此只能将指针指向不同的派生类对象, 而不能将派生类对象赋值给基类对象实现多态. 多重继承的情况 当一个派生类同时继承多个基类时, 派生类的虚表中会同时存在所有基类的虚表条目. 12345678910111213141516171819202122class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class EmptyBase2nd &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd bar&quot;); &#125;&#125;;class Derived : public EmptyBase, public EmptyBase2nd &#123; public: void foo() override &#123; endl(cout &lt;&lt; &quot;Derived foo&quot;); &#125; virtual void foobar() &#123;&#125;&#125;;int main() &#123; Derived d; return 0;&#125; 上面的实现具有下面的特点: 派生类同时继承 EmptyBase 和 EmptyBase2nd EmptyBase 和 EmptyBase2nd 中都定义有虚函数 foo 和 bar 派生类 Derived 中 override 了虚函数 foo, 定义了新的虚函数 foobar 下面通过对象 d 观察其内存布局: 首先是 d 对象本身, 可以看到它内部有两个虚表指针 12345678(gdb) p d$1 = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557d00 &lt;vtable for Derived+16&gt; &#125;, &lt;EmptyBase2nd&gt; = &#123; _vptr.EmptyBase2nd = 0x555555557d28 &lt;vtable for Derived+56&gt; &#125;, &lt;No data fields&gt;&#125; 观察第一个基类 EmptyBase 的虚表: 123456(gdb) p /a *(void**)(0x555555557d00)$2 = 0x5555555552d8 &lt;Derived::foo()&gt;(gdb) p /a *(void**)(0x555555557d00 + 8)$3 = 0x55555555527c &lt;EmptyBase::bar()&gt;(gdb) p /a *(void**)(0x555555557d00 + 16)$4 = 0x555555555310 &lt;Derived::foobar()&gt; 其中的内容是: 虚函数 foo (地址被派生类的 override 实现覆盖) 虚函数 bar (基类的实现) 派生类特有的虚函数 foobar (派生类的虚函数条目存储在第一基类的虚表中) 观察第二个基类 EmptyBase2nd 的虚表: 1234(gdb) p /a *(void**)(0x555555557d28)$6 = 0x555555555306 &lt;_ZThn8_N7Derived3fooEv&gt;(gdb) p /a *(void**)(0x555555557d28 + 8)$7 = 0x5555555552aa &lt;EmptyBase2nd::bar()&gt; 注意到其中第一个条目是一个特殊信息 &lt;_ZThn8_N7Derived3fooEv&gt;, 根据上面的观察得到的经验, 这个条目本应该用来存储虚函数 foo. 但是考虑到虚函数 foo 被派生类 override, 并且在 EmptyBase 的虚表中已经存了 Derived::foo 的地址, 那么我猜想这里的特殊信息是指导通过 EmptyBase2nd 的指针调用 foo 函数时重定位到 Derived::foo 用的. 虚表的第二个条目就比较符合常规, 是 EmptyBase2nd::bar 的地址. 总的来说, 上面的多重继承内存布局可以简述为下图: 多重继承的歧义性 上面对派生类的内存模型的分析看起来很美好, 但实际上存在一些问题. 例如上面的多重继承例子中, 如果派生类 Derived 的对象 d 调用了基类 EmptyBase 和 EmptyBase2nd 中都存在定义的 bar 函数: 1234567891011121314151617181920212223class EmptyBase &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class EmptyBase2nd &#123; public: virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd bar&quot;); &#125;&#125;;class Derived : public EmptyBase, public EmptyBase2nd &#123; public: void foo() override &#123; endl(cout &lt;&lt; &quot;Derived foo&quot;); &#125; virtual void foobar() &#123;&#125;&#125;;int main() &#123; Derived d; d.bar(); // Error! return 0;&#125; 那么编译器会抱怨它无法确定具体要调用哪一个 bar 的实现: 1234567891011121314151617181920212223# error from gcc../polymorphism/polymorphism.cpp: In function ‘int main()’:../polymorphism/polymorphism.cpp:23:5: error: request for member ‘bar’ is ambiguous 23 | d.bar(); | ^~~../polymorphism/polymorphism.cpp:12:16: note: candidates are: ‘virtual void EmptyBase2nd::bar()’ 12 | virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd bar&quot;); &#125; | ^~~../polymorphism/polymorphism.cpp:6:16: note: ‘virtual void EmptyBase::bar()’ 6 | virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125; | ^~~# error from clang../polymorphism/polymorphism.cpp:23:5: error: member &#x27;bar&#x27; found in multiple base classes of different types d.bar(); ^../polymorphism/polymorphism.cpp:6:16: note: member found by ambiguous name lookup virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125; ^../polymorphism/polymorphism.cpp:12:16: note: member found by ambiguous name lookup virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase2nd bar&quot;); &#125; ^1 error generated. 根据上面对多重继承下 Derived 类型对象内存的分析, Derived 类型对象的虚表中同时存在 EmptyBase::bar 和 EmptyBase2nd::bar. 再根据 public 继承的语义, Derived 对象也是 EmptyBase 对象和 EmptyBase2nd 对象. 因此 Derived 对象对函数 bar 的调用是具有歧义的: 调用任何一个而放弃另一个都是没有道理的. 菱形继承 上面的例子是我们非要在两个基类中定义同一个虚函数, 然后还要同时继承, 最过分的是还要不加区分地调用, 是一种庸人自扰. 那么再看另一个场景: 类型 EmptyBase 是基类, 类型 MiddleA 和 MiddleB 都继承 EmptyBase, 类型 Derived 则多重继承 MiddleA 和 MiddleB. 四个类型的继承关系形成了一个菱形: 假设 MiddleA 和 MiddleB 都是空类, 那么就不存在一个虚函数在两个基类中定义的行为, 但是下面的代码的行为仍然存在问题: 123456789101112131415161718192021222324252627class EmptyBase &#123; public: EmptyBase() &#123; endl(cout &lt;&lt; &quot;Construct EmptyBase&quot;); &#125; ~EmptyBase() &#123; endl(cout &lt;&lt; &quot;Destruct EmptyBase&quot;); &#125; virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class MiddleA : public EmptyBase &#123; public: MiddleA() &#123; endl(cout &lt;&lt; &quot;Construct MiddleA&quot;); &#125; ~MiddleA() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleA&quot;); &#125;&#125;;class MiddleB : public EmptyBase &#123; public: MiddleB() &#123; endl(cout &lt;&lt; &quot;Construct MiddleB&quot;); &#125; ~MiddleB() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleB&quot;); &#125;&#125;;class Derived : public MiddleA, public MiddleB &#123;&#125;;int main() &#123; Derived d; // d.foo(); return 0;&#125; 上面的代码输出为: 12345678Construct EmptyBaseConstruct MiddleAConstruct EmptyBaseConstruct MiddleBDestruct MiddleBDestruct EmptyBaseDestruct MiddleADestruct EmptyBase 可以看出: EmptyBase 被构造和析构了两次, 分别是在构造和析构基类 MiddleA 的部分和 MiddleB 的部分时. 这个行为是不正确的, 因为对于一个派生类的对象 d, EmptyBase 在语义上只是它的基类, d 中不应该有两份 EmptyBase 的成分. 然而如果我们深入观察 d 的内存: 12345678910(gdb) p d$1 = &#123; &lt;MiddleA&gt; = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557c70 &lt;vtable for Derived+16&gt; &#125;, &lt;No data fields&gt;&#125;, &lt;MiddleB&gt; = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557c90 &lt;vtable for Derived+48&gt; &#125;, &lt;No data fields&gt;&#125;, &lt;No data fields&gt;&#125; 然后分别观察两个虚表的条目: 1234567891011# MiddleA::EmptyBase::_vptr.EmptyBase(gdb) p /a *(void **)(0x555555557c70)$2 = 0x55555555531e &lt;EmptyBase::foo()&gt;(gdb) p /a *(void **)(0x555555557c70 + 8)$3 = 0x55555555534c &lt;EmptyBase::bar()&gt;# MiddleB::EmptyBase::_vptr.EmptyBase(gdb) p /a *(void **)(0x555555557c90)$8 = 0x55555555531e &lt;EmptyBase::foo()&gt;(gdb) p /a *(void **)(0x555555557c90 + 8)$9 = 0x55555555534c &lt;EmptyBase::bar()&gt; 就会发现 d 中确实是存了两份关于 EmptyBase 的信息, 这无疑是愚蠢的. 同样的, 如果调用 EmptyBase 的 foo 和 bar 中任意函数, 都会导致歧义性编译错误. 虚继承 对于菱形继承中的歧义性问题, C++ 提供的解决方案是 虚继承, 虚继承体系中的基类被称为虚基类: 虚继承实现的语义是: 虚继承的派生类声明它愿意和当前虚基类的其他派生类共享虚基类, 即在菱形继承中两个中间基类可以共享根部的虚基类, 因此菱形继承中 MiddleA 和 MiddleB 使用虚继承的结果是 Derived 对象中只会存在一份 EmptyBase 的成分. 123456789101112131415161718192021222324252627class EmptyBase &#123; public: EmptyBase() &#123; endl(cout &lt;&lt; &quot;Construct EmptyBase&quot;); &#125; ~EmptyBase() &#123; endl(cout &lt;&lt; &quot;Destruct EmptyBase&quot;); &#125; virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class MiddleA : virtual public EmptyBase &#123; public: MiddleA() &#123; endl(cout &lt;&lt; &quot;Construct MiddleA&quot;); &#125; ~MiddleA() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleA&quot;); &#125;&#125;;class MiddleB : public virtual EmptyBase &#123; public: MiddleB() &#123; endl(cout &lt;&lt; &quot;Construct MiddleB&quot;); &#125; ~MiddleB() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleB&quot;); &#125;&#125;;class Derived : public MiddleA, public MiddleB &#123;&#125;;int main() &#123; Derived d; d.foo(); // OK return 0;&#125; 此时再查看 d 的内存: 1234567(gdb) p d$1 = &#123; &lt;MiddleA&gt; = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557ba0 &lt;vtable for Derived+40&gt; &#125;, &lt;No data fields&gt;&#125;, &lt;MiddleB&gt; = &#123;&lt;No data fields&gt;&#125;, &lt;No data fields&gt;&#125; 非常容易理解: 现在 d 中只有一份 EmptyBase 的成分, 存储在 MiddleA 的部分中. 虚继承的一般模型 我们已经知道了在菱形继承的情况下, 虚继承提供了一种共享根部虚基类的机制. 上面给出的例子中, 只有虚基类中存在虚函数定义, 中间基类和派生类中都没有数据成员. 那么对于一般的虚继承, C++ 会如何构建对象的内存呢? 单虚继承 首先模拟一种简单的单继承情况: 123456789101112131415161718192021class EmptyBase &#123; int i&#123;&#125;; public: EmptyBase() &#123; endl(cout &lt;&lt; &quot;Construct EmptyBase&quot;); &#125; ~EmptyBase() &#123; endl(cout &lt;&lt; &quot;Destruct EmptyBase&quot;); &#125; virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125;&#125;;class Derived : virtual public EmptyBase &#123; int l&#123;&#125;; virtual void bar() &#123;&#125; virtual void foobar() &#123;&#125;&#125;;int main() &#123; Derived d; d.foo(); // OK return 0;&#125; 上面定义了一个有虚函数和成员变量的基类, 派生类通过虚继承继承这个基类, 并且派生类中定义了: 成员变量 override 的虚函数 特有的虚函数 下面观察派生类对象 d 的内存: 12345678910(gdb) p d$1 = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557d00 &lt;vtable for Derived+72&gt;, i = 0 &#125;, members of Derived: _vptr.Derived = 0x555555557cd0 &lt;vtable for Derived+24&gt;, l = 0&#125; 可以看到 Derived 对象 d 的内存中有这么几个条目: 基类 EmptyBase 的部分 虚表指针 _vptr 成员变量 i 派生类的部分 虚表指针 _vptr 成员变量 l 与一般的派生类 (非虚继承; [[#派生类的内存模型]]) 不同的地方在于内存中有两个虚表指针. 下面分别观察这两个虚表指针指向的虚表的内容: EmptyBase::_vptr.EmptyBase: 12(gdb) p /a *(void **)(0x555555557d00)@2$2 = &#123;0x55555555535a &lt;EmptyBase::foo()&gt;, 0x5555555553c5 &lt;_ZTv0_n32_N7Derived3barEv&gt;&#125; Derived::_vptr.Derived: 12(gdb) p /a *(void **)(0x555555557cd0)@2$3 = &#123;0x5555555553b6 &lt;Derived::bar()&gt;, 0x5555555553d2 &lt;Derived::foobar()&gt;&#125; 派生类的虚表中存有派生类定义的两个虚函数的地址, 而基类的虚表中只有一个虚函数的地址. 这是因为虚基类的另一个虚函数已经在派生类中 override, 因此这个虚函数已经被存在了派生类的虚表中, 不需要在基类的虚表中重复. 与 非虚继承 的派生类对象内存做一个对比: 一个上面的例子无法揭示的事实: 实际上, 虚继承体系中, 即使派生类中没有定义虚函数, 只要其中有成员变量, 编译器就会为其创建虚表指针. 此时该虚表指针指向的虚表是空的. 菱形继承 上面讨论了单虚继承下更为一般的内存模型, 下面观察更具一般性的菱形虚继承对象. 首先定义基类和派生类都含有成员变量和一个 override 虚函数和一个特有虚函数的菱形虚继承模型: 123456789101112131415161718192021222324252627282930313233343536373839404142class EmptyBase &#123; int i&#123;&#125;; public: EmptyBase() &#123; endl(cout &lt;&lt; &quot;Construct EmptyBase&quot;); &#125; ~EmptyBase() &#123; endl(cout &lt;&lt; &quot;Destruct EmptyBase&quot;); &#125; virtual void foo() &#123; endl(cout &lt;&lt; &quot;EmptyBase foo&quot;); &#125; virtual void bar() &#123; endl(cout &lt;&lt; &quot;EmptyBase bar&quot;); &#125; virtual void func() &#123;&#125; virtual void foobarfunc() &#123;&#125;&#125;;class MiddleA : virtual public EmptyBase &#123; int a&#123;&#125;; public: MiddleA() &#123; endl(cout &lt;&lt; &quot;Construct MiddleA&quot;); &#125; ~MiddleA() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleA&quot;); &#125; virtual void foo() override &#123;&#125; virtual void foobarA() &#123;&#125;&#125;;class MiddleB : public virtual EmptyBase &#123; int b&#123;&#125;; public: MiddleB() &#123; endl(cout &lt;&lt; &quot;Construct MiddleB&quot;); &#125; ~MiddleB() &#123; endl(cout &lt;&lt; &quot;Destruct MiddleB&quot;); &#125; virtual void bar() override &#123;&#125; virtual void foobarB() &#123;&#125;&#125;;class Derived : public MiddleA, public MiddleB &#123; int l&#123;&#125;; virtual void func() &#123;&#125; virtual void foobar() &#123;&#125;&#125;;int main() &#123; Derived d; return 0;&#125; 注意到上面的继承体系有以下特点: 基类 EmptyBase 有 4 个虚函数, 其中三个分别被派生类 override 每个类都有一个成员变量 每个派生类都有一个 override 虚函数和一个特有的虚函数 可以画出继承关系如下图所示: 对象 d 的内存: 12345678910111213141516171819(gdb) p d$1 = &#123; &lt;MiddleA&gt; = &#123; &lt;EmptyBase&gt; = &#123; _vptr.EmptyBase = 0x555555557b68 &lt;vtable for Derived+144&gt;, i = 0 &#125;, members of MiddleA: _vptr.MiddleA = 0x555555557af0 &lt;vtable for Derived+24&gt;, a = 0 &#125;, &lt;MiddleB&gt; = &#123; members of MiddleB: _vptr.MiddleB = 0x555555557b28 &lt;vtable for Derived+80&gt;, b = 0 &#125;, members of Derived: l = 0&#125; 三个虚表指针, 分别是: 虚基类的虚表指针 MiddleA::EmptyBase._vptr.EmptyBase (存在于 MiddleA 的部分) MiddleA 的虚表指针 MiddleA::_vptr.MiddleA MiddleB 的虚表指针 MiddleB::_vptr.MiddleB 可以看出虚继承加持下的菱形继承实现了虚基类 EmptyBase 的共享, 并且每一个虚继承的派生类都持有自己的虚表指针 (Derived 不是虚继承, 因此它没有自己的虚表指针). 下面分别观察三个虚表的内容: MiddleA::EmptyBase._vptr.EmptyBase: 从基址 0x555555557b68 出发打印 12 个地址 (8字节对其) 的内容, 发现其中含有大量的特殊信息 1234567(gdb) p /a *(void **)(0x555555557b68)@12$12 = &#123;0x555555555493 &lt;_ZTv0_n24_N7MiddleA3fooEv&gt;, 0x555555555591 &lt;_ZTv0_n32_N7MiddleB3barEv&gt;, 0x5555555555bd &lt;_ZTv0_n40_N7Derived4funcEv&gt;, 0x5555555553a2 &lt;EmptyBase::foobarfunc()&gt;, 0x555555557af0 &lt;_ZTV7Derived+24&gt;, 0x555555557bd8 &lt;_ZTC7Derived0_7MiddleA+24&gt;, 0x555555557c18 &lt;_ZTC7Derived0_7MiddleA+88&gt;, 0x555555557c50 &lt;_ZTC7Derived16_7MiddleB+24&gt;, 0x555555557c90 &lt;_ZTC7Derived16_7MiddleB+88&gt;, 0x555555557b68 &lt;_ZTV7Derived+144&gt;, 0x555555557b28 &lt;_ZTV7Derived+80&gt;, 0x20&#125; 其中的 0x5555555553a2 &lt;EmptyBase::foobarfunc()&gt; 是可以解读的函数地址. 这里的特殊信息在本文中一直避免对它进行讨论, 主要因为: 笔者水平和见识有限, 对它们没有认识 这类信息很大程度上与编译器实现对象内存的方式有关, 笔者希望避免对这些特殊实现细节的讨论 但笔者做一个大胆的猜测就是这些信息可能跟类型的运行时类型信息 (Run time type infomation, RTTI) 有关. 可见关于虚函数, 虚基类 EmptyBase 的虚表中只存储了它特有的一个. MiddleA::_vptr.MiddleA: 打印发现这个虚表中有 4 个有效的函数地址, 并且中间还有其他信息 123(gdb) p /a *(void **)(0x555555557af0)@4$17 = &#123;0x555555555484 &lt;MiddleA::foo()&gt;, 0x5555555554a0 &lt;MiddleA::foobarA()&gt;, 0x5555555555ae &lt;Derived::func()&gt;, 0x5555555555ca &lt;Derived::foobar()&gt;&#125; 分别是 0x555555555484 &lt;MiddleA::foo()&gt; 0x5555555554a0 &lt;MiddleA::foobarA()&gt; 0x5555555555ae &lt;Derived::func()&gt; 0x5555555555ca &lt;Derived::foobar()&gt; 这说明编译器将派生类的虚函数也都放进了它的 MiddleA 虚表. MiddleB::_vptr.MiddleB: MiddleB 的虚表中只有它自己的两个虚函数地址 12(gdb) p /a *(void **)(0x555555557b28)@2$20 = &#123;0x555555555582 &lt;MiddleB::bar()&gt;, 0x55555555559e &lt;MiddleB::foobarB()&gt;&#125; 根据以上观察可以画出下图: 需要注意的是, 除了上图中 EmptyBase 的虚表部分, 本文中所有虚表图示都省略了除虚函数以外的其他信息, 但实际的虚表中是含有除虚函数地址以外的如 RTTI 信息的. 总结 本文中笔者利用 GDB 调试器近距离观察了 C++ 对象在涉及虚函数和继承时的内存布局, 主要关注了编译器对虚函数的处理, 以及基于虚表和虚表指针实现的多态机制. 最后, 笔者分析了较为复杂的多重继承和菱形继承模型. 对于菱形继承, C++ 提供了虚继承语义解决歧义性问题, 笔者对这一语义的实现和菱形继承内存布局进行了观察和分析. 本文主要的观察有: 对于没有虚函数的类型, 它的对象内存中只有自己的成员变量 对于没有虚函数的派生类, 它的对象内存布局相当于嵌套的结构体 对于有虚函数的类型, 它的对象内存中存在一个虚表指针 _vptr, 指向一个虚表. 虚表中的条目是该类型虚函数的地址 对于涉及虚函数的 非虚继承 的派生类, 它的内存模型和不涉及虚函数时一致, 都是包含有基类的部分. 只不过在 基类 中有虚函数时, 派生类中的虚函数会被放进虚函数部分的虚表: 对于派生类中的 override 虚函数, 编译器的做法是直接使用该 override 虚函数的地址覆盖基类部分虚表中对应的条目 对于派生类中的非 override 虚函数, 编译器的做法是直接将该虚函数的地址放在基类部分虚表的后面 对于多重 非虚 继承, 派生类的内存布局没有特殊机制: 仍然是派生类中包含有基类的部分. 因此当多个基类中含有相同函数签名的虚函数时, 派生类对该函数的调用会产生歧义. 对于菱形 非虚 继承, 它的歧义性产生原因和多重继承一样: 根部基类中的虚函数地址同时存在于中间层基类的虚表中, 因此派生类对根部基类的虚函数的调用存在多条路径. 对于虚继承语义, 在菱形继承时 C++ 只会在派生类中存放一份虚基类的虚函数地址, 一般存在派生类的第一顺位基类中. Reference Zero-overhead principle - cppreference.com c++头脑风暴-多态、虚继承、多重继承内存布局 - 掘金 (juejin.cn) C++中虚函数、虚继承内存模型 - 知乎 (zhihu.com) Derived classes - cppreference.com","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"多态","slug":"多态","permalink":"https://blog.zhuwenq.cc/tags/%E5%A4%9A%E6%80%81/"},{"name":"内存模型","slug":"内存模型","permalink":"https://blog.zhuwenq.cc/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"}]},{"title":"内存屏障和 C++ 内存模型","slug":"内存屏障和-C-内存模型","date":"2023-04-08T14:47:44.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"内存屏障和-C-内存模型/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%86%85%E5%AD%98%E5%B1%8F%E9%9A%9C%E5%92%8C-C-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/","excerpt":"Memory Barrier 是 CPU 提供的一种机制, 它的作用是在程序中设置一个内存屏障, CPU 会保证所有在 Memory Barrier 之前的代码对内存的访问 (改动) 在 Memory Barrier 之后都是可见的. 这一保证据有两方面的效应: 所有 Memory Barrier 之后的代码都不能被重排 (Reordering) 到 Memory Barrier 之前执行 当执行到 Memory Barrier 时, CPU 必须在所有 Core 之间同步 Cache, 保证此刻所有 Core 看到的内存是一致的. Memory Order 是 C++ 11 提供的一种并发编程机制, 它允许显式地限制编译器/CPU 对指令的重排 (Reordering), 以及显式地控制多核 CPU Cache 之间的同步. 其效果是使得不同线程/不同 CPU Core 所观察到的原子操作附近的内存操作具有可预测的顺序.","text":"Memory Barrier 是 CPU 提供的一种机制, 它的作用是在程序中设置一个内存屏障, CPU 会保证所有在 Memory Barrier 之前的代码对内存的访问 (改动) 在 Memory Barrier 之后都是可见的. 这一保证据有两方面的效应: 所有 Memory Barrier 之后的代码都不能被重排 (Reordering) 到 Memory Barrier 之前执行 当执行到 Memory Barrier 时, CPU 必须在所有 Core 之间同步 Cache, 保证此刻所有 Core 看到的内存是一致的. Memory Order 是 C++ 11 提供的一种并发编程机制, 它允许显式地限制编译器/CPU 对指令的重排 (Reordering), 以及显式地控制多核 CPU Cache 之间的同步. 其效果是使得不同线程/不同 CPU Core 所观察到的原子操作附近的内存操作具有可预测的顺序. 从编译器优化说起 众所周知, 编译器不会忠实地将程序代码翻译为机器码, 而是在遵守 As-if 原则的前提下使用一定程度的指令重排以提高程序效率. As-if 原则是指优化前后的程序在单线程执行时应该具有相同的行为. 因此, 在多线程并发情况下, 无法完全假定程序中位于前面的代码会先于后面的代码执行. 例如如下的简单程序: 123456int a&#123;&#125;;int b&#123;&#125;;int main()&#123; ++a;++b; a = a + b;&#125; 在 g++ -S p1.cpp -o p1.s -O3 命令下编译得到的汇编代码 (部分) 为: 123456789main: movl b(%rip), %eax ; move b to %eax movl a(%rip), %edx ; move a to %edx addl $1, %eax ; ++b movl %eax, b(%rip) ; move (b + 1) back to b leal 1(%rax,%rdx), %eax ; move (1 + b + a) to %eax movl %eax, a(%rip) ; move (1 + b + a) to a xorl %eax, %eax ret 可以看出: 汇编代码首先计算了排在 ++a 后面的 ++b 汇编代码没有计算 ++a, 而是直接计算出了 a 的最终值 a = 1 + b + 1 + a 如果在多线程环境下, 一个线程读到 b 自增了 1 就错误地假设 a 已经完成了自增, 那么程序的行为就无法定义. CPU 重排 另一个会重排指令的地方是 CPU 指令执行模型. 在现代 CPU 设计中, 设计者利用很多机制如乱序发射, 分治预测等提高指令执行的并行性. 这种设计加速程序执行的同时也使得在并发情况下程序的指令执行顺序不可预测. 内存可见性 CPU Cache 的存在使得指令执行的结果 (内存的变动) 可能不会立即被其他 CPU Core 观察到. 对这个现象的理解需要了解 CPU Cache 的架构和缓存一致性协议. 大致地说, CPU 直接能够访问的 Cache 是存在于每个 CPU Core 上的, 即, 每个内核独占地使用一份 Cache. 而 Cache 实际上是主存的缓存, 即一份副本, 那么为了保持每个 CPU Core 中的内存副本是一致的, 就需要一套 cache coherence protocol. Cache coherence Protocol 定义了 Cache line (Cache 操作的最小单位) 的几种状态, 以及 Core 之间沟通的几种消息: Cache line 状态: Modified(M): 即当前的 core 修改了该 cache line, 在当前的 core 将这个 cache line 同步到 memory 之前, 其他的 core 不能再修改这个 cache line 对应的内存位置 Exclusive(E): 即当前 core 缓存的这个 cache line 是最新的, 当前的 core 可以直接 modify 这个 cache line 而不需要等待其他 core 的同步. Share(S): 当前 core 缓存的这个 cache line 在其他的 core 的 cache 中也存在副本, 如果当前 core 需要修改这个 cache line 则需要与其他 core 同步 Invalid(I) 表示这个 cache line 是空的 Protocol Message: Read: 当一个 core 需要读某个 cache line 时发起 read 消息 Read Response: 对 Read 的回应, 可以来自其他 core 的 cache 也可以来自 memory. 如果其他 core 中的该 cache line 是 Modified(M) 状态, 则由该 core 发起 Read Response. Invalidate: 该消息包含一个 memory location, 收到此消息的 core cache 需要删除包含该 memory location 的 cache line, 并响应. Invalidate Acknowledge: 收到 Invalidate 的 Core 删除 cache line 中的数据, 然后向发起者响应 Invalidate Acknowledge 消息. Read Invalidate: 即 Read 和 Invalidate 的结合. Write back: 消息中包含数据和地址, 将数据写入内存中给定的地址位置. 通过上述的一致性协议, 可以保证所有 CPU Core 对 cache line 的访问总是一致的: 如果某个 Core A 需要写入一个 memory location, 而这个 memory location 存在于另一个 Core B 的一个 Modified cache line 中. 那么 Core A 就必须发起 Invalidate 消息, 然后等待来自 Core B 的 Invalidate Ack 后才能写入. Core B 收到 Invalidate 消息后需要先将 cache line invalidate 掉再响应 Invalidate Ack. 但是 CPU 设计者不满足于这样的等待, 于是他们又引入了两层缓存, 即 Store Buffer 和 Invalidate Queue. Store Buffer 位于 Core 和 Cache 之间. 这一层 buffer 提供了这样的功能: 当 Core A 执行写入操作时, 不需要等待来自 Core B 的 Invalidate Ack 消息, 而是直接写入在 Store Buffer 中, 然后执行后续的操作. 等收到 Invalidate Ack 消息后再将 Store Buffer 同步到 Cache 中. 值得注意的是, 由于 Store Buffer 的存在, Core 需要 load 数据时需要先在 Store Buffer 中查找以保证 load 的数据是最新的. 这一过程叫做 Store Forwarding Invalidate Queue 位于 Core Cache 和 memory 之间. 当 Core A 向 Core B 发送 Invalidate 消息时, Core B 可以直接将该消息放入自己的 Invalidate Queue, 并立即返回 Invalidate Ack. 需要注意的是, 如果 Core A 的 Invalidate Queue 中已经存在了该 cache line 的 Invalidate 消息, 那么它必须处理该 cache line 的 Invalidate 事件. 总的来说, 带有 Invalidate Queue 的 Core 提供这样一种保证: 在传输任何关于该 cache line 的 MESI 消息之前处理该条目. 这样的策略进一步压栈了 CPU 性能, 但是其带来一些问题, 例如如下的程序: 123456789101112131415#include &lt;assert.h&gt;#include &lt;thread&gt;int a&#123;&#125;;int b&#123;&#125;;void foo() &#123; a = 1; b = 1;&#125;void bar() &#123; while (b == 0) continue; assert(a == 1);&#125; 假设 Core A 上运行的线程 1 和 Core B 上的线程 2 分别运行 foo() 和 bar(). 那么理想情况下, 当线程 2 从 while 循环退出时, 全局变量 a 的值应该已经变为了 1, 即断言会成功. 但实际上, 由于内存可见性以及可见性顺序的不确定性, 断言可能会失败: 假设此时 a 不存在于 Core A 的 Cache 中, b 存在于 Core A 的 Cache 中且状态为 M. 那么: Core A 执行 a = 1 之后将 a 的值放进 Store Buffer, 然后继续执行 b = 1, 因为 b 在 Core A 上是 M 状态, 因此对 b 的修改不需要和其他 core 同步. 此时 Core B 执行 while(b == 0) 时发现 b == 1, 那么就会退出循环执行 assert. 但此时如果 Core A 的 Store Buffer 还没有将 a 的值同步到 Cache 的话, Core B 获取到的 a 值将为 0. 此时结果不符合预期. 假设 Core A 及时将 Store Buffer 中的值刷到 Cache 中. 那么 Cache 会向 Core B 发送 Invalidate 消息, 提醒其关于 a 的值已经失效. 这个消息会 push 到 Core B 的 Invalidate Queue 中 当 Core B 发现 b == 1 退出循环时, 如果它还没有处理 Invalidate Queue 中的消息, 即 Core B 中 a 的 Cache Line 状态不为 l, 那么 Core B 此时获取到的 a 的取值仍然是 0. 结果不符合预期. 上面两种情况下不符合预期的行为是无法在 CPU 层面解决的: 读取变量的 CPU Core 并不知道读取的变量与其他 Core 有什么关系. Memory Barrier 为了保证程序的正确性, CPU 设计者提供了 Memory Barrier 机制. 它具体的形式是一种指令, 它的执行效果是将 Store Buffer 和 Invalidate Queue 与 Cache 同步, 即要求 CPU Core 立即处理这些缓存. 更具体地说, CPU 设计者为读写操作分别引入了不同的 Memory Barrier 指令: 对于读操作, Read Memory Barrier 会将 Invalidate Queue 与 Cache 同步 对于写操作, Write Memory Barrier 会将 Store Buffer 与 Cache 同步 Memory Order 由于以上的原因, 并发编程在没有设置 Memory Barrier 时难以获取跨线程一致的内存顺序. C++ 对这些现代多核 CPU 并行运行时的内存可见性提供的抽象是 Memory Order. 具体地说, C++ 抽象了 3 种内存顺序模型: Sequential Consistent 即顺序一致性模型, 这种模型下编译器和 CPU 严格按照代码顺序生成和执行指令. 同时所有的内核观察到的内存顺序都一致, 即内存修改的顺序以及可见的顺序是全局一致的 (global memory order) Happens Before 即按照代码顺序, 如果语句 A 在语句 B 的前面, 那么 语句 A 的执行所产生的效果需能够被语句 B 的执行观察到, 则称语句 A Happens Before 语句 B. 值得注意的是 Happens Before 强调内存修改的可见性顺序, 并不强调 「实际的执行顺序」. Synchronizes With 如果线程 A 中的一个 atomic store 操作是 「Release 操作」, 线程 B 种对同一个变量的 atomic load 操作是 「Acquire 操作」, 并且线程 B 中的 load 操作读取到了线程 A 中的 store 操作所写入的值, 那么称线程 A 中的 Store 操作 synchronizes with 线程 B 中的 Load 操作. std::atomic 和 std::memory_order 对于上面三种抽象内存顺序模型, C++ 标准提供了 6 个 flag 用于控制对 atomic 变量 load 和 store 操作, 以使得原子变量周围的内存可见性顺序能够满足以上顺序模型. memory_order_relaxed(Relaxed Ordering) 松散模型, 用该 flag 标记的原子变量操作只保证原子变量本身的内存一致性, 对其他内存的读写操作没有任何的同步和重排限制. memory_order_acquire 和 memory_order_release (Acquire-Release Ordering) C++ 借鉴 lock 的 lock 和 unlock 操作, 引入了两个概念: Acquire 和 Release. Acquire 操作是一种 load 操作, 它保证该 load 操作之后的操作不会被重排到该 load 操作之前. 如果对同一个原子变量的一个 Release 操作 synchronizes with 该 Acquire 操作, 那么这个 Release 操作之前所有的写操作对内存的影响对于该 Acquire 操作以及之后的操作都可见. Release 操作是一种 store 操作, 它保证该 store 操作之前的操作不会重排到该 store 操作之后. Release 操作和 Acquire 操作一般配合使用. memory_order_consume Consume 操作类似于 Acquire 操作, 也是一种 load 操作, 但是其要求更加宽松. 一个某原子变量上的 Consume 操作仅能看到对于 该原子变量依赖于的内存 的修改, 而不是所有对于该原子变量的 Release 操作之前的内存修改. memory_order_acq_rel 该操作用于 read-modify-write 操作, 即一个即读取, 又修改, 又写入的原子变量操作. 它保证该 rmw 操作前后的语句都不会跨越该操作而重排. 该操作兼容 Acquire 和 Store 操作, 且整体上是原子的. memory_order_seq_cst(Sequentially Consistent Ordering) 即 Sequential Consistent 操作, 可以用于 load/store/read-modify-write 操作. 且所有操作都保证有双向屏障 (读写 memory barrier), 操作的前后语句都保证不会重排, 所有变量的内存可见顺序对于所有的线程都是一致的. Conclusion 本文从编译器优化会重排程序代码的执行顺序说起, 对于会产生代码执行顺序变化的 编译器优化 和 CPU reordering 进行了讨论. 对于一个确定的指令执行顺序, 内存可见性顺序 也会影响程序在 并发 时的行为. 针对这一现象, 本文从 CPU Cache 的结构和缓存一致性协议说起, 介绍了引起内存可见性顺序不定的原因: store buffer 和 invalidate queue. 针对这些原因, 本文介绍了 CPU 设计者提供的 Memory Barrier 指令. 基于上述基础, 本文介绍了 C++ 对内存可见性顺序的抽象: Memory Order, 并分别对 C++ 提供的 6 个 memory order flag 和它们能够组合成的 3 种 memory order model 进行了介绍. References std::memory_order - cppreference.com Paul E McKenney. Is Parallel Programming Hard, And, If So, What Can You Do About It?. 现代C++的内存模型 - 知乎 (zhihu.com) 高并发编程–多处理器编程中的一致性问题(上) - 知乎 (zhihu.com) 谈谈 C++ 中的内存顺序 (Memory Order) - Luyu Huang’s Blog","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"CPU Cache","slug":"CPU-Cache","permalink":"https://blog.zhuwenq.cc/tags/CPU-Cache/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://blog.zhuwenq.cc/tags/Concurrency/"},{"name":"Memory Order","slug":"Memory-Order","permalink":"https://blog.zhuwenq.cc/tags/Memory-Order/"}]},{"title":"共享目标文件/共享库","slug":"共享目标文件-共享库","date":"2023-04-02T12:08:44.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"共享目标文件-共享库/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%85%B1%E4%BA%AB%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6-%E5%85%B1%E4%BA%AB%E5%BA%93/","excerpt":"对于被广泛使用的标准库或其他库, 如果一个系统中多个程序都在引用相同的符号, 此时使用静态库将需要为每个程序都加载一份符号定义的代码和数据, 这对内存是极大的浪费. 共享库 (shared library) 是解决静态库这一问题的产物. 共享库是一个目标模块, 在运行或加载时可以被加载到任意内存地址, 并和一个在内存中的程序链接起来. 这一过程称为动态链接, 是由一个叫做 动态链接器 (dynamic linker) 的程序执行的.","text":"对于被广泛使用的标准库或其他库, 如果一个系统中多个程序都在引用相同的符号, 此时使用静态库将需要为每个程序都加载一份符号定义的代码和数据, 这对内存是极大的浪费. 共享库 (shared library) 是解决静态库这一问题的产物. 共享库是一个目标模块, 在运行或加载时可以被加载到任意内存地址, 并和一个在内存中的程序链接起来. 这一过程称为动态链接, 是由一个叫做 动态链接器 (dynamic linker) 的程序执行的. 动态链接 在链接时, 文件系统中对于一个共享库只有一份 .so 文件, 所有引用该库的可执行目标文件共享这个 .so 文件中的代码和数据, 而不是像引用静态库一样将静态库的内容嵌入进可执行目标文件. 在执行时, 一个共享库的 .text 节在内存中的副本可以被不同的进程共享 下图是动态库的链接过程: 链接器在构造可执行目标文件时, 没有将任何动态库的代码和数据被复制进可执行目标. 但链接器需要复制关于重定位和符号表的信息进入可执行目标, 这些信息指引加载器解析程序对动态库的引用. 在加载和运行时: 加载器加载部分链接的可执行目标文件 加载器会注意到部分链接的可执行目标文件中存在一个 .interp 节, 这一节中包含动态链接器的路径名 动态链接器也是一个共享库 (在 Linux 系统上是 ld-linux.so). 加载器加载并运行动态链接器 动态链接器执行以下重定位以完成链接: 分别重定位 被引用的共享库 的文本和数据 重定位可执行目标中对共享库中符号的引用 动态链接器将控制权传递给可执行目标程序 动态加载 除了在链接时由链接器提供重定位信息, 并在运行前由加载器加载, 动态库还可以在运行时动态地加载, 不需要链接器的重定位信息 具体地说, 动态加载是在程序运行时动态地调用动态链接器提供的接口实现的, 通过这些接口可以动态地调用动态链接器在运行时解析共享库. 下面是 Linux 系统提供的简单接口: 1234567891011121314151617181920212223242526#include &lt;dlfcn.h&gt;/** @filename: 动态链接库路径* @flag: 打开的模式* 打开和链接一个本地动态链接库, 返回指向该库的句柄. 打开失败则返回 NULL*/void* dlopen(const char* filename, int flag);/** @handle: 动态链接库句柄* @symbol: 符号名称* 在给定动态链接库句柄中查找给定名称的符号的句柄. 查找失败返回 NULL*/void* dlsym(void* handle, char* symbol);/** @handle: 要关闭的动态链接库句柄* 卸载一个共享库*/int dlclose(void* handle);/** 返回最近一次调用 dlopen, dlsym 或 dlclose 的错误*/const char* dlerror(void); 位置无关代码 (Position-Independent Code, PIC) 实现共享库的一项关键技术是位置无关代码, 这类代码可以被加载到内存中的任意位置而无需链接器修改. GCC 编译器可以使用 -fpic 选项生成 PIC 代码. 对于共享库中模块内部定义的符号, 编译器可以使用 PC 相对偏移地址生成 PIC 代码. 但是对于全局符号的处理则需要特殊技巧. PIC 数据引用 编译器利用一个有趣的事实生成对全局变量的 PIC 引用: 即无论我们在内存中的何处加载一个目标模块 (包括共享目标模块), 数据段和代码段之间的距离总是保持不变. 因此, 代码段中的任何指令和数据段中的任何变量之间的距离都是一个运行时常量. 编译器利用了上述事实, 它在数据段开始的地方创建一个 全局偏移量表 (Global Offset Table, GOT). 在 GOT 中, 每个被这个目标模块引用的全局符号都有一个 8 字节条目. 编译器还为每个条目生成一个重定位记录. 在加载时, 动态链接器会重定位 GOT 中的每个条目, 使它包含正确的绝对地址. 如下图, 代码段中对 addcnt 的地址的访问被设置为了 0x2008b9(%rip), 即 PC 指针偏移 0x2008b9 的位置, 即 GOT[3]. GOT 在编译时不会填充有效的值, 但在加载时, 动态链接器会将其填充为加载到内存中的动态连接库中定义的符号的地址. PIC 函数调用 对于对共享库中定义的函数的调用, PIC 代码编译系统使用 延迟绑定 (lazy binding) 技术解析其地址. 具体地说, 就是将对函数地址的解析延迟到第一次调用该函数时. 其动机是对于定义了很多个函数的标准共享库, 典型的程序只会使用其中的一小部分, 延迟绑定可以避免对没有调用过的函数地址的解析. 延迟绑定是通过 GOT 和 过程连接表 (Procedure Linkage Table, PLT) 之间的交互实现的. PLT 是一个数组, 其中每个条目是一个 16 字节的代码: 每个对共享库函数的调用都有一个条目, 每个条目负责解析一个函数调用. PLT[0] 是一个特殊的条目, 它跳转到动态链接器. PLT[1] 调用系统启动函数 __libc_start_main, 它初始化执行环境, 调用 main 函数并处理其返回值. PLT[2] 及以后的条目调用用户代码中调用的函数 和 PLT 联合使用时: GOT[0] 和 GOT[1] 包含动态链接器在解析函数地址时会用到的信息 GOT[2] 是动态链接器在 ld-linux.so 模块中的入口点. 其余的每个条目对应一个被调用的函数, 其地址在运行时被解析. 每个条目都有一个相匹配的 PLT 条目. 初始时, 每个 GOT 条目都指向对应于 PLT 条目的第二条指令 当第一次调用一个共享库中定义的函数时, 延迟绑定的流程如下图: Instead of directly calling addvec, the program calls into PLT[2], which is the PLT entry for addvec. The first PLT instruction does an indirect jump through GOT[4]. Since each GOT entry initially points to the second instruction in its corresponding PLT entry, the indirect jump simply transfers control back to the next instruction in PLT[2] After pushing an ID for addvec (0x1) onto the stack, PLT[2] jumps to PLT[0]. PLT[0] pushes an argument for the dynamic linker indirectly through GOT[1] and then jumps into the dynamic linker indirectly through GOT[2]. The dynamic linker uses the two stack entries to determine the runtime location of addvec, overwrites GOT[4] with this address, and passes control to addvec.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Computer System","slug":"Computer-System","permalink":"https://blog.zhuwenq.cc/tags/Computer-System/"}]},{"title":"可执行目标文件","slug":"可执行目标文件","date":"2023-04-02T12:03:27.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"可执行目标文件/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%8F%AF%E6%89%A7%E8%A1%8C%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6/","excerpt":"链接器 链接 的结果是生成一个可执行目标文件, 这个目标文件中包含将程序加载到内存并运行的所有信息. 下图是一个 ELF 可执行目标文件所包含的信息:","text":"链接器 链接 的结果是生成一个可执行目标文件, 这个目标文件中包含将程序加载到内存并运行的所有信息. 下图是一个 ELF 可执行目标文件所包含的信息: ELF header 描述文件的总体格式, 以及程序的入口点 (entry point), 即程序运行时执行的第一条指令的地址 .init 节定义了一个 _init 函数, 程序的初始化代码会调用该函数 program header table 描述了文件中连续的片 (chunk) 与连续的内存段之间的映射关系 加载 可执行目标文件的运行由加载器 (loader) 完成. 加载器将可执行文件中的代码和数据从磁盘复制到内存, 然后通过跳转到程序的第一条指令或入口点运行该程序. 每个 Linux 程序都有一个如下图所示的运行时内存映像: 在 Linux x86-64 中, 代码段总是从 0x400000 处开始, 后面是数据段 (已初始化数据 .data 和未初始化数据 .bss). 运行时堆在数据段之后, 通过 malloc 函数调用向上增长. 堆之后有一块为 动态库 保留的内存, 共享库将被加载到该区域. 其后是用户栈区, 其总是从最大的合法用户地址 (248−12^{48} - 1248−1) 开始, 随程序的栈帧分配和释放向下增长或向上回收. 地址 2482^{48}248 开始是为系统内核的代码和数据保留的区域 上面对程序内存映像的描述和图示在逻辑上是正确的, 但实际上由于内存对其, 内存保护等的要求在实现上略有不同. 当运行一个程序时, 具体发生了什么: 加载器为程序创建上图所示的内存映像, 并在程序头部表 (program header table) 的引导下完成片 (chunk) 的复制. 随后, 加载器跳转到程序的入口点, 即 _start 函数的地址, 这个函数在系统目标文件 ctrl.o 中定义, _start 函数会调用系统启动函数 __libc_start_main, 该函数定义在 libc.so 中, 它会初始化执行环境, 调用用户层的 main 函数, 处理 main 函数的返回值, 并在需要的时候将控制还给内核.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Computer System","slug":"Computer-System","permalink":"https://blog.zhuwenq.cc/tags/Computer-System/"}]},{"title":"可重定位目标文件","slug":"可重定位目标文件","date":"2023-04-02T11:19:23.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"可重定位目标文件/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%8F%AF%E9%87%8D%E5%AE%9A%E4%BD%8D%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6/","excerpt":"现代 x86-64 系统使用可执行可链接格式文件 (Executable and Linkable Format, ELF) ELF 文件的结构:","text":"现代 x86-64 系统使用可执行可链接格式文件 (Executable and Linkable Format, ELF) ELF 文件的结构: ELF header: 一个 16 字节的序列描述生成该文件的系统的 字长 和 字节顺序 (大小端) 剩下的部分包含帮助链接器语法分析和解释目标文件的信息, 如 ELF 头的大小, 目标文件的类型 (e.g. 可重定位目标文件/可执行目标文件/共享目标文件), 机器类型 (e.g. x86-64), 节头部表 (section header table) 的偏移地址, 以及节头部表的条目大小和数量 section: .text: 已编译程序的机器代码 (指令) .rodata: 只读数据 .data: 已初始化的全局和静态变量 .bss: 未初始化的全局和静态变量, 以及所有被初始化为 0 的全局或静态变量 .symtab: 符号表, 存放程序中定义和引用的函数和全局变量的信息 .rel.text: .text 节中位置的列表, 链接器需要修改这些位置 .rel.data: 被模块引用或定义的所有全局变量的重定位信息 .debug: 调试符号表, 其条目是程序中定义的局部变量和类型定义, 程序中定义和引用的全局变量, 以及原始的 C 文件. 只有以 -g 选项调用编译器才会得到这张表 .line: 原始 C 文件中的行号和 .text 节中机器指令之间的映射, 只有以 -g 选项调用编译器才会得到这张表 .strtab: 一个字符串表, 其内容包括 .symtab 和 .debug 节中的符号表, 以及节头部中的节名字. 节头部表 (section header table) 符号和符号表 每个 ELF 文件都有一个符号表, 包含该模块 (文件) 定义和引用的符号的信息. 链接器考虑三种类型的符号: 由当前模块定义的并能被其他模块引用的 全局符号, 对应于非静态的函数和全局变量 由其他模块定义并被当前模块引用的 全局符号, 称为 外部符号. 对应于其他模块中定义的非静态函数和全局变量 只被当前模块引用和定义的 局部符号, 对应于带 static 属性的函数和全局变量 重定位条目 (relocation entry) 汇编器生成可重定位目标模块时, 它并不知道该模块的数据和代码在最终程序中的内存位置, 也不知道这个模块所引用的外部符号 (函数, 全局变量) 的位置. 因此, 当编译器遇到一个对未知位置的目标引用时, 它就会生成一个 重定位条目, 告诉链接器在链接时如何修改这个引用. 代码的重定位条目存在于 .rel.text 节, 数据重定位条目存放在 .rel.data 节. 下面是重定位条目的结构: 123456typedef struct &#123; long offset; // Offset of the reference to relocate long type: 32, // Relocation type symbol: 32; // Symbol table index long addend; // Constant part of relocation expression&#125; Elf64_Rela; offset: 需要被修改的引用的节偏移 symbol: 需要被修改的引用应该指向的符号 type: 告知链接器应该怎么修改新的引用. 两种基本的类型: R_X86_64_PC32: 相对地址, 使用一个 32 位的相对 PC 的偏移量重定位目标引用, 如 call 指令 R_X86_64_32: 绝对地址, 使用一个 32 位的绝对地址重定位目标引用 addend: 一些类型的重定位需要该字段对被修改引用的值做偏移调整","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Compute System","slug":"Compute-System","permalink":"https://blog.zhuwenq.cc/tags/Compute-System/"}]},{"title":"Linkage","slug":"Linkage","date":"2023-04-02T11:13:52.000Z","updated":"2023-06-21T06:51:28.828Z","comments":true,"path":"Linkage/","link":"","permalink":"https://blog.zhuwenq.cc/Linkage/","excerpt":"程序从源代码到可执行文件需要经过 4 个步骤, 链接就是其中重要的一步. 链接器的主要工作是将具备引用关系的不同编译单元的编译结果 (i.e. 可重定位目标文件) 相互连接, 组成一个可执行文件. 静态链接器以一组可重定位目标文件和命令行参数作为输入, 生成一个完全链接的, 可以加载运行的可执行目标文件.","text":"程序从源代码到可执行文件需要经过 4 个步骤, 链接就是其中重要的一步. 链接器的主要工作是将具备引用关系的不同编译单元的编译结果 (i.e. 可重定位目标文件) 相互连接, 组成一个可执行文件. 静态链接器以一组可重定位目标文件和命令行参数作为输入, 生成一个完全链接的, 可以加载运行的可执行目标文件. 链接器必须实现两个主要功能以实现链接: 符号解析 (symbol resolution): 目标文件中会定义和引用符号, 每个符号对应于一个函数, 全局变量或静态变量. 符号解析的目的是将符号引用和符号定义关联起来. 重定位 (relocation): 编译器和汇编器生成从 0 开始的代码和数据节. 链接器需要通过将每个符号定义与一个内存地址关联起来实现 重定位 . 随后链接器需要修改所有对该符号定义的引用, 使其指向该地址. 链接器使用汇编器所产生的 重定位条目(relocation entry) 进行这样的重定位. 目标文件 目标文件有三种形式: 可重定位目标文件 可执行目标文件 共享目标文件 编译器和汇编器可以生成可重定位目标文件和共享目标文件, 链接器根据这些文件生成可执行目标文件 符号解析 链接器进行符号解析的方法是将每个引用和它输入的可重定位目标文件符号表中定义的一个确定符号定义关联起来. 全局符号解析 全局符号分为 强 和 弱 两种: 强 符号: 函数和已初始化的全局变量 弱 符号: 未初始化的全局变量 Linux 链接器对于多重定义的全局符号处理规则: 不允许有多个同名强符号 如果有一个强符号和多个弱符号同名, 选择强符号 如果多个弱符号同名, 从中任选一个 对[[静态库]]的解析 链接器按照文件在命令行参数中出现的顺序依次解析符号, 当涉及静态库时, 具体地说, 链接器维护三个集合: 可重定位目标文件集合 EEE: EEE 中的文件会被组合成可执行目标文件 未解析的符号集合 UUU: 即引用但尚未发现定义的符号集合 已发现定义的符号集合 DDD: 即在之前的输入文件中已经找到定义的符号集合 初始时, 三个集合均为空: 对于命令行参数中每个文件 fff, 链接器首先判断 fff 是 目标文件 还是 存档文件 如果是 目标文件: 链接器将 fff 添加到 EEE, 同时修改 DDD 和 UUU 反映 fff 中符号的定义和引用 如果是 存档文件: 链接器尝试匹配 UUU 中未解析的符号和存档文件 fff 中定义的符号. 对于 匹配到的模块 mmm (mmm 在 fff 中被定义且其引用存在于 UUU 中) , 链接器将 mmm 添加到 EEE, 并修改 UUU 和 DDD 以反映 mmm. 对于 所有未匹配到的模块 , 链接器简单地丢弃这些模块 链接器处理完参数中所有的文件后, 如果 UUU 是非空的, 那么说明存在被引用但未定义的符号, 链接器会输出错误并终止. 否则链接器会合并和重定位 EEE 中的目标文件, 构建可执行目标文件 重定位 符号解析之后, 代码中每个符号的引用和一个符号定义关联起来. 链接器此时就知道了他的输入目标模块中代码节 (.text) 以及数据节 (.data .rodata) 的确切大小, 即具备了为每个符号分配运行时地址的信息. 重定位由两个步骤组成: 重定位节和符号定义: 链接器将所有相同类型的节合并为同一个聚合的节. 即, 把来自所有输入模块的 .text, .data 等分别聚合为一个. 随后, 链接器为所有的节以及所有的符号赋予运行时内存地址, 这一步完成时, 程序中所有的指令和全局变量都被分配了唯一的运行时地址. 重定位节中的符号引用: 链接器修改代码节和数据节中对每个符号的引用, 使得它们指向正确的运行时地址. 这一步的进行依赖于可重定位目标模块中的 重定位条目 (relocation entry) 数据结构","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Compute System","slug":"Compute-System","permalink":"https://blog.zhuwenq.cc/tags/Compute-System/"},{"name":"Linkage","slug":"Linkage","permalink":"https://blog.zhuwenq.cc/tags/Linkage/"}]},{"title":"Cache","slug":"Cache","date":"2023-03-15T14:45:25.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"Cache/","link":"","permalink":"https://blog.zhuwenq.cc/Cache/","excerpt":"计算机存储器具有金字塔式结构, 即: 较高层的存储器容量小速度快, 低层的存储器容量大速度慢, 计算机通过缓存的方式提高储存系统的整体性能.","text":"计算机存储器具有金字塔式结构, 即: 较高层的存储器容量小速度快, 低层的存储器容量大速度慢, 计算机通过缓存的方式提高储存系统的整体性能. 缓存命中: 当程序需要第 k+1k+1k+1 层的某个数据对象 ddd 时, 它首先在当前存储在第 kkk 层的某个块中查找 ddd. 如果 ddd 刚好在第 kkk 层中, 那么就称之为缓存命中 (cache hit) 缓存不命中: 如果程序在第 kkk 层没有找到数据对象 ddd, 就称之为 缓存不命中 (cache miss). 缓存不命中有以下几种: 强制性不命中 (compulsory miss)/冷不命中 (cold miss): 当系统刚启动时, 缓存一般是空的, 即任何数据对象的访问都会不命中. 这种情况称为冷不命中 冲突不命中(conflict miss): 由于高层的缓存容量通常远小于低层的储存器容量, 为了完整映射较大容量的低层存储器, 通常数据对象在缓存中放置的位置需要符合某种放置策略. 因放置策略的原因导致缓存中存有另一个数据对象导致的不命中称为冲突不命中. 容量不命中 (capacity miss): 由于程序在某个阶段访问的数据量过大超过缓存的容量导致的不命中称为容量不命中 cache 组织结构 对于一个地址为 mmm 位的计算机, 总共有 M=2mM=2^mM=2m 个不同的地址, 其 cache 一般符合下图的组织结构: 其中包含由 S=2sS=2^sS=2s 个缓存组 (cache set), 每个缓存组包含 EEE 个缓存行 (cache line), 每个缓存行是由一个 B=2bB=2^bB=2b 字节的数据块 (block), 一个有效位 (valid bit) 和 t=m−(b+s)t=m-(b+s)t=m−(b+s) 个 标记位 (tag bit) 组成的. 其中有效位标记着这个行是否存有有意义的信息, 标记位时当前块地址的一个子集. cache 块的定位过程 对于 mmm 位的地址, 其被分为三个部分解读: 位于高位部分的标记位, 位于中间部位的组索引 (set index) 和位于低位部位的块偏移 (block offset). 组选择: 首先对于位于中间部位的组索引 (set index), 通过它可以直接索引到 cache 块所在的 cache set 行匹配: 对于一个 cache set 中 EEE 个 cache line 的每一个, 首先检查 valid bit 是否设置, 只有对于设置了 valid bit 的 cache line, 如果其 tag bit 与地址高位部分的 tag bit 相符, 则说明这个 cache line 中存有要定位的块. 字选择: 对于确定了的 cache line, 地址低位部分的 bbb 位 block offset 可以直接定位到要查找的块 当发生 cache miss 时, 必须从更低一层的存储器中取出一行数据并放在缓存中: 行替换: 如果对应的组中存在一个空行, 那么可以直接将新的行写入空行中. 如果不存在空行, 则必须替换一个有效的行. 常见的替换策略有随机替换, LRU, LFU 等. 写入内存时与 cache 有关的若干问题 写命中 (write hit), 即要写入的数据对象存在于 cache 中时, 有两种数据更新策略: 直写 (write through): 即立即将修改后的 cache 块写到低一层的存储器中. 这种策略较为简单, 但是其会引起大量的总写流量 写回 (write back): 即尽可能迟地推迟更新, 只在替换策略即将驱逐这个更新过了的块时才将其写入到低一层的存储器中. 这种策略能够显著减少总线流量, 但它需要每个 cache line 维护额外的状态位 (dirty bit) 标记这个 cache 块是否被修改过 写不命中 (write miss) 时, 对应地也有两种数据更新策略: 写分配 (write allocate): 加载相应的数据到 cache 中, 然后更新 cache 块 非写分配 (non-write-allocate): 绕开 cache, 直接写入到低一层的存储器 通常写回策略与写分配策略会搭配使用 真实的 cache 通常每个 CPU 会有两个 L1 cache, 一个用于缓存数据, 一个用于缓存指令; 同一个 CPU 的两个 L1 cache 共享一个 L2 cache; 所有 CPU 的 L2 cache 共享一个 L3 cache.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Compute System","slug":"Compute-System","permalink":"https://blog.zhuwenq.cc/tags/Compute-System/"},{"name":"CPU Cache","slug":"CPU-Cache","permalink":"https://blog.zhuwenq.cc/tags/CPU-Cache/"}]},{"title":"Xv6 Operating system 03-page table","slug":"Xv6-Operating-system-03-page-table","date":"2023-03-09T11:13:11.000Z","updated":"2023-06-21T06:51:28.932Z","comments":true,"path":"Xv6-Operating-system-03-page-table/","link":"","permalink":"https://blog.zhuwenq.cc/Xv6-Operating-system-03-page-table/","excerpt":"页表是操作系统之所以能够为进程提供独立的地址空间和内存所依赖的机制。","text":"页表是操作系统之所以能够为进程提供独立的地址空间和内存所依赖的机制。 操作系统使用页表管理物理内存，由页表决定内存地址的意义，以及物理内存的访问。 通过页表提供的中间层，操作系统还可以实现一些额外的 tricks, 例如将同一块内存映射到多个进程的地址空间 (如所有进程地址空间中都一样的内核代码和数据区)，或者通过永远不映射内核所在的内存实现内核和进程之间的隔离。 换页硬件 xv6 操作系统是为 RISK-V 处理器设计的，因此这里描述的换页硬件可能和其他系统的有所不同。 在 xv6 所运行在的 Sv39 RISK-V 处理器上，64 位地址的中只有低 39 位被使用，其余 25 位留空。 逻辑上 一个 RISK-V 页表是一个 2272^{27}227 长的 page table entries(PTE) 数组. 而每个 PTE 由一个 44 位宽的物理页编号 (physical page number, PPN) 和一些标志位组成。 换页硬件翻译虚拟地址时，利用虚拟地址低 39 位中的高 27 位在页表中定位 PTE， 然后将找到的 PTE 的 44 位物理页编号和虚拟地址低 39 位中的剩余 12 位组合成一个 56 位的物理地址。 实际上 xv6 的页表在实现上并不是一个长度为 2272^27227 的数组，而是一个三级的树状结构. 数的根部是一个 4096 字节大小的页表页 (储存着页表的页), 其内部保存了 512 个 PTE，其中每个 PTE 保存了下一级页表页的物理地址。 换页硬件使用虚拟地址的 27 位页表地址的高 9 位定位根页表中的 PTE，然后根据该 PTE 定位二级页表，然后使用中间 9 位定位二级页表中的 PTE，并根据二级页表中的 PTE 定位三级页表，最后使用低 9 位定位最终的 PTE。该最终 PTE 的 44 位地址会用来访问实际的页，而虚拟地址中剩余的 12 位会用来在该页中定位字节。 如果三级 PTE 中任意一个不存在 ( 根据虚拟地址检索不到 )，换页硬件会产生页错误异常 (page-fault exception), 将由内核负责处理该异常 PTE 标志位 每个 PTE 都包含一系列标志位 (flags)， 标识着该 PTE 中的地址被允许的用法： PTE_V (valid): 该 PTE 是否可见，如果没有设置 (not set), 对该 PTE 指向的页的引用会导致异常 PTE_R (readable): 该 PTE 指向的页是否可读 PTE_W (writable): 该 PTE 指向的页是否可写 PTE_X (executable): CPU 是否可以将该 PTE 指向的页的内容解释为指令并执行 PTE_U (user): 该 PTE 指向的页是否可以由用户态指令访问 内核地址空间 xv6 为每个进程维护一个页表，该页表描述了进程的用户地址空间。此外，xv6 还维护一个页表用于描述内核的地址空间。 内核将配置自己的地址空间使其能够在可预测的地址上访问物理内存和其他硬件设备。即，内核使用直接映射 (direct mapping) 的方式获取 RAM 和硬件设备寄存器的访问地址。 直接映射即将硬件设备的地址映射为跟它们的物理地址一样的虚拟地址。如内核本身将自己加载到虚拟内存 KERNBASE=0x80000000 的位置，同时该地址数值也是其在物理内存中的位置。 内核地址空间中也有一些不是直接映射的虚拟地址: 顶部的 trampoline 页被映射到内核地址空间的顶部，同时它也被映射到用户地址空间的顶部。 ==值得注意的是这个页在物理内存中的位置同时也会被内核的直接映射映射在内核中的某个位置。 内核栈页. 每个进程都有一个自己的内核栈，所有的内核栈都会被映射在内核地址空间的高处，并且每个内核栈下方会防止一个页的隔离区。该隔离页的 PTE_V 没有设置，因此当内核栈溢出到隔离页时，对 invalid 内存的访问会导致异常，并造成 kernel panic. 内核没有全部使用直接映射而是将内核栈以及 trampoline 页映射到顶部主要目的是提供保护机制 (隔离区) 物理内存分配 xv6 使用链表管理物理内存, 管理的最小单位是 4096 字节大小的页. 具体地说, xv6 通过链表串起物理内存中所有的页 (从内核地址的结尾到物理内存的结尾) , 分配内存即从链表中移除对应的页节点, 回收内存即将待回收的内存加回到链表 进程空间(用户地址空间) 进程的地址空间从虚拟地址 0 一直到 MAXVA. 当进程向 xv6 要求更多内存时, xv6 首先调用 kalloc 分配物理页, 随后 xv6 向进程的页表添加指向新分配的物理页的 PTE, 同时为这些 PTE 设置标志位. 大多数进程用不到全部的地址空间, xv6 将没有使用的 PTE 设置成不可用状态. 页表管理用户地址空间的几点好处: 不同进程维护不同的页表, 可以将用户地址映射为物理内存上不同的页, 使得每个进程拥有自己的私有内存. 每个进程都认为自己的地址空间是从 0 开始且连续的, 但实际的物理内存可以是不连续的 内核可以将同一块在所有进程中共享的物理内存映射到所有进程的地址空间, 即同一块物理内存可以出现在所有进程的地址空间中","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"https://blog.zhuwenq.cc/tags/Operating-System/"},{"name":"xv6","slug":"xv6","permalink":"https://blog.zhuwenq.cc/tags/xv6/"}]},{"title":"C++ 模板03--基本技巧","slug":"C-模板03-基本技巧","date":"2023-03-06T12:04:48.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"C-模板03-基本技巧/","link":"","permalink":"https://blog.zhuwenq.cc/C-%E6%A8%A1%E6%9D%BF03-%E5%9F%BA%E6%9C%AC%E6%8A%80%E5%B7%A7/","excerpt":"C++ template 中关于实践的一些技巧和基础机制","text":"C++ template 中关于实践的一些技巧和基础机制 typename 关键字 当一个类型名依赖于一个模板参数时，该类型名之前必须使用 typename: 12345678910template&lt;typename T&gt;class C &#123; void fun() &#123; // without the typename keywork, // the compiler might think // some_type is a member of T // instead of a type. typename T::some_type* ptr; &#125;&#125; zero-initialization 对于 C++ 中的基本类型如 int, double, 和 pointer 等类型，它们没有默认构造函数因此不会自动将自己初始化为默认值。在非全局作用域下，未显式初始化的局部变量具有未定义的值: 1234void func () &#123; int x; // x has undefined value int* ptr; // ptr might point to anywhere instead of nowhere&#125; 因此在模板中，如果需要对模板参数提供的类型构造一个对象，必须显式指定为其默认初始化( 大括号初始化 ): 12345template&lt;typename T&gt;void func () &#123; T t1; // not good, t1 has undefined value T t2&#123;&#125;; // good, t2 is default initialized&#125; 大括号初始化意味着: 如果对象类型定义有默认构造函数，则调用该默认构造函数，如果没有默认构造函数，则会尝试使用初始化列表构造函数 对于内置类型，执行零初始化( zero initialization )。即: 对象的值被初始化为 0 (数值类型，指针) 或 false (bool 类型) 使用 this-&gt; 对于继承体系中的类模板，如果基类也是类模板且基类模板依赖于派生类模板的模板参数，那么对于在基类中定义的符号，应该总是使用 this-&gt; 或 Base&lt;T&gt;:: 标识: 123456789101112131415template &lt;typename T&gt;class Base &#123;public: void fun();&#125;;template &lt;typename T&gt;class Derived : Base&lt;T&gt; &#123; // Base relies on T of Derivedpublic: void foo()&#123; fun(); // Error, Base&lt;T&gt;::fun() is never considered in symbol resolving process this-&gt;func(); // Ok Base&lt;T&gt;::func(); // Ok &#125;&#125; 原始数组或字符串字面量作参数的模板 当原始数组或字符串字面量作参数时，如果模板的参数类型被声明为引用类型，那么原始数组或字符串字面量不会在传参过程中发生类型退化: 1234567891011121314template&lt;typename T, size_t N, size_t M&gt;bool less(T(&amp;a)[N], T(&amp;b)[M]) &#123; for(size_t i = 0; i &lt; N &amp;&amp; i &lt; M; ++i)&#123; if(a[i] &lt; b[i])return true; if(a[i] &gt; b[i])return false; &#125; return N &lt; M;&#125;int x[] = &#123;1, 2, 3&#125;;int y[] = &#123;1, 2, 3, 4, 5&#125;;// both type and length are reserved when passing argumentsless(x, y); 数组也可以以不完整类型的形式出现，即一个数组的声明符号也可以用作模板参数。由于数组声明符号可以不提供长度信息，因此一个支持所有数组类型的模板必须为不完整数组(无边界数组)提供特化: 1234567891011121314151617181920212223242526272829303132333435extern int x[]; // an incomplete type of int []// undefined templatetemplate &lt;typename T&gt;class TemplateForArray;// template 1: for array typetemplate &lt;typename T, std::size_t N&gt;class TemplateForArray&lt;T[N]&gt; &#123;public: static void func() &#123; std::cout &lt;&lt; &quot;for array T[&quot; &lt;&lt; N &lt;&lt; &quot;]&quot;; &#125;&#125;;// template 2: for reference to array typetemplate &lt;typename T, std::size_t N&gt;class TemplateForArray&lt;T (&amp;)[N]&gt; &#123;public: static void func() &#123; std::cout &lt;&lt; &quot;for array T(&amp;)[&quot; &lt;&lt; N &lt;&lt; &quot;]&quot;; &#125;&#125;;// template 3: for incomplete arraytemplate &lt;typename T&gt;class TemplateForArray&lt;T[]&gt; &#123;public: static void func() &#123; std::cout &lt;&lt; &quot;for incomplete array T[]&quot;; &#125;&#125;;int main() &#123; int x[] = &#123;1, 2, 3&#125;; // x is of type int[3] int (&amp;y)[3] = x; // y is of type int(&amp;)[3] extern int z[]; // z in of type int[], which is incomplete TemplateForArray&lt;decltype(x)&gt;::func(); // ok TemplateForArray&lt;decltype(y)&gt;::func(); // ok TemplateForArray&lt;decltype(z)&gt;::func(); // error: incomplete type &#x27;TemplateForArray&lt;int []&gt;&#x27; used in nested name specifier if without template 3&#125; 成员模板 类/类模板的成员也可以被定义成模板 对于类模板，只有被用到的成员函数才会被实例化 类/类模板的成员模板也可以被特化(全特化, 偏特化) 类/类模板的特殊成员函数也可以被定义成模板，但是需要注意: 构造函数模板或赋值运算符模板不会替换预定义的构造函数或赋值运算符 成员模板不算做复制或移动对象的特殊成员函数 .template 关键字 当调用成员模板的对象其自身依赖于模板参数时，编译器无法将 &lt; 解析成模板参数列表的开始，因此需要显式地使用 .template 表示模板构造。例如： 1234template&lt;unsigned long N&gt;void printBitset ((std::bitset&lt;N&gt; const&amp; bs)) &#123; std::cout &lt;&lt; bs.template to_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt;&gt;();&#125; 上例中由于 bs 的类型依赖于模板参数 N, 因此必须用 .template 才可以调用成员模板。 类似的操作还有 -&gt;template 和 ::template, 应该仅在模板中使用 泛型 lambda 表达式是成员模板的 wrapper C++ 14 中引入的泛型 lambda 表达式例如: 123[] (auto a, auto b) &#123; return a + b;&#125;; 实际上相当于: 12345678class [InnerCompilerID] &#123; // compiler given namepublic: [InnerCompilerID](); template &lt;typename T1, typename T2&gt; auto operator()(T1 a, T2 b) const &#123; return a + b; &#125;&#125;;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"}]},{"title":"Xv6 Operating system 02-organization","slug":"xv6-Operating-system-02-organization","date":"2023-03-05T12:15:06.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"xv6-Operating-system-02-organization/","link":"","permalink":"https://blog.zhuwenq.cc/xv6-Operating-system-02-organization/","excerpt":"The core functionality of an OS 操作系统的核心任务在于同时支持多个活动，即，操作系统必须将硬件资源分时共享给所有的进程使用。 为了保证进程的活动质量，操作系统还必须保证进程的执行是 独立/孤立(isolation) 的，即进程不会因意外或恶意目的破坏其他进程的执行，以及不会被其他进程破坏。 保证独立性的前提下，进程之间合理的交互和通信也应该被支持，因此操作系统应该提供 进程间通信 的方式，例如 pipe 总的来说，操作系统必须为进程提供以下服务： 时分复用 底层硬件 独立 的执行环境(内存，内核状态等) 进程之间的合法 交互","text":"The core functionality of an OS 操作系统的核心任务在于同时支持多个活动，即，操作系统必须将硬件资源分时共享给所有的进程使用。 为了保证进程的活动质量，操作系统还必须保证进程的执行是 独立/孤立(isolation) 的，即进程不会因意外或恶意目的破坏其他进程的执行，以及不会被其他进程破坏。 保证独立性的前提下，进程之间合理的交互和通信也应该被支持，因此操作系统应该提供 进程间通信 的方式，例如 pipe 总的来说，操作系统必须为进程提供以下服务： 时分复用 底层硬件 独立 的执行环境(内存，内核状态等) 进程之间的合法 交互 Abstracting physical resources 为实现进程之间的独立性, 操作系统需要完全接管对系统硬件的管理，在进程和硬件之间提供一层抽象中间层，即将硬件抽象成服务。同时防止进程直接操作系统硬件。 这是因为如果进程可以直接操作硬件，那么则需要进程之间实现很强的协调或同步机制才能实现硬件在多个进程之间的共享。 恶意进程长时间占用硬件或破坏硬件会破坏其他进程的运行 非恶意进程不协调的硬件操作 (例如所有进程都在主动让出硬件) 也会导致进程无法正常运行 Storage 进程与存储硬件的交互只能通过操作系统提供的 open, read, write 和 close 等系统调用进行，由操作系统进行存储硬件(磁盘，光盘，磁带，远程存储服务)的管理和实际的硬件操作。 CPU 操作系统会透明地在进程之间切换 CPU。即，将进程切出 CPU 是保存寄存器等进程上下文，并在切回时重新载入上下文，如此 上下文切换 使得进程对于 CPU 时分复用没有感知，不会干扰进程的执行状态。 Memory 进程的内存由 exec 系统调用交由操作系统内核分配，进程的创建者不需要关心进程在物理内存中的位置。 User mode, supervisor mode, and system calls 为实现进程的孤立性，即保证进程无法操作其他进程的内存，以及保证进程无法操作系统内核的内存，必须在进程之间设置硬件隔离。 类似目的的硬件隔离可以由 CPU 提供，以 RISC-V 为例，CPU 提供三种指令执行模式：machine mode, supervisor mode, user mode. machine mode 在 machine mode 下执行的指令具备全部的特权，即可以操作任意设备。CPU 启动时默认处在 machine mode, 在操作系统的声明周期中，machine mode 一般用来进行系统启动过程的配置，之后操作系统将切换到 supervisor mode supervisor mode 在 supervisor mode 下，CPU 可以执行特权指令，例如：使能/失能中断，读取/写入持有页表地址的寄存器等。 在 supervisor mode 下运行的程序也可以称为是在 kernel space 中运行 运行在 kernel space 中的程序被称为 kernel kernel is running in kernel space, and there is nothing but the kernel in kernel space. user mode user mode 下 CPU 只能执行如算数运算等用户态指令。 如果 user mode 下的程序试图执行特权指令，那么 CPU 会切换到 supervisor mode 并终止该程序。 在 user mode 下执行的程序也被称为在 user space 中执行 Kernel organization 有很多种组织方式可以实现 OS 的核心要求，两种大的分类是宏内核(monolithic kernel) 和 微内核(micro kernel). 它们的主要区别在于哪些组件运行在 supervisor mode (kernel space) 其中宏内核是目前大多数 Unix 操作系统内核所采用的组织方式。 Monolithic Kernel 宏内核即整个操作系统都运行在 kernel space, 也即整个操作系统都属于内核的一部分。 这样的组织方式下所有的操作系统部件都具备硬件特权，其具备如下优缺点: 优点： 设计方便，操作系统设计者不需要考虑不同操作系统部件对特权指令的要求不同 利于操作系统组件之间的协作，例如虚拟内存部件和文件系统部件之间可以共享缓存 缺点： 操作系统部件之间的接口较复杂，因此系统中存在 bug 的可能性更高 一旦某个部件运行错误，由于其运行在 supervisor mode, 错误通常会导致整个内核 fail, 进而导致计算机停止运行。 Micro Kernel 考虑宏内核的缺点，即复杂性带来的工程问题，OS 设计者考虑将内核最小化，即将大部分的操作系统组件移到 user space. 这种内核组织方式即 微内核 在微内核操作系统中，部分操作系统功能以进程的形式运行在 user mode 下，这种进程称为服务 Process overview 进程是 Unix 提供的孤立 (isolation) 执行单元，进程对执行程序的抽象保证了程序无法破坏或监视其他程序的内存，CPU，文件描述符等运行时上下文，无法破环内核的运行状态。 操作系统内核通过进程这一抽象为执行程序提供了一种幻觉：程序会幻觉自己在独占机器硬件 实现这一幻觉所使用的机制包括 user/supervisor mode flag, address spaces, time-slicing of threads Address space 操作系统通过为每个进程维护独立的地址空间为程序提供其在独占内存的幻觉。 具体地说，操作系统使用页表 (page tables) 将进程地址空间中的 虚拟地址 (virtual address) 映射为硬件上的 物理地址(physical address) . 其中虚拟地址即程序指令操作的地址，物理地址则是 CPU 在访存时发给主存的地址。 操作系统内核为每个进程维护一个独立的页表，该页表定义了进程的地址空间。地址空间通常组织如下(一个典型 Linux 进程的地址空间，与 xv6 实现的地址空间存在差异)： Thread 进程的指令是由 thread 执行的，thread 可以被暂停和恢复。当 CPU 在进程间切换时，关于指令执行部分的切换工作实际上是暂停上个进程的 thread 然后恢复下个进程的 thread。 thread 的状态(local variables, function call return addresses) 主要存储在 thread 的 stack 中。而进程维护两个 stack: user stack 和 kernel stack. 当 process 的 thread 在执行用户指令时，使用的是 user stack, 此时 kernel stack 是空的。当进程进入内核时(通过系统调用或中断)，内核中的代码会在进程的 kernel stack 中执行，此时进程的 user stack 没有被占用，但其中仍储存着数据。 进程的整个生命周期中，user stack 和 kernel stack 一直在交替被占用。同时 kernel stack 不能由用户代码操作，以防用户代码破坏内核代码的执行。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"https://blog.zhuwenq.cc/tags/Operating-System/"},{"name":"xv6","slug":"xv6","permalink":"https://blog.zhuwenq.cc/tags/xv6/"}]},{"title":"Xv6 Operating system 01-interface","slug":"xv6-Operating-system-01-interface","date":"2023-03-02T14:36:49.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"xv6-Operating-system-01-interface/","link":"","permalink":"https://blog.zhuwenq.cc/xv6-Operating-system-01-interface/","excerpt":"操作系统的职责是在多个程序(进程)之间共享计算机提供的物理资源，并提供一系列比直接操作硬件更有用的服务。具体地说，操作系统为计算机硬件提供一层抽象，使软件程序不需要关心硬件的具体实现。操作系统还必须通过某种时分复用的机制使得多个程序可以共享底层硬件。最后，操作系统也必须为不同程序进程提供某种通信机制。 操作系统中，运行的程序被抽象成进程，一个进程的内容即它所占用的内存及其他资源。其中，在内存中存有进程的指令，数据，堆栈等运行时上下文。操作系统接口即操作系统内核为其他进程提供的一系列服务的接口，称为系统调用 (system call) 。当进程需要调用系统服务时，其只能通过调用操作系统接口实现.","text":"操作系统的职责是在多个程序(进程)之间共享计算机提供的物理资源，并提供一系列比直接操作硬件更有用的服务。具体地说，操作系统为计算机硬件提供一层抽象，使软件程序不需要关心硬件的具体实现。操作系统还必须通过某种时分复用的机制使得多个程序可以共享底层硬件。最后，操作系统也必须为不同程序进程提供某种通信机制。 操作系统中，运行的程序被抽象成进程，一个进程的内容即它所占用的内存及其他资源。其中，在内存中存有进程的指令，数据，堆栈等运行时上下文。操作系统接口即操作系统内核为其他进程提供的一系列服务的接口，称为系统调用 (system call) 。当进程需要调用系统服务时，其只能通过调用操作系统接口实现. xv6 提供的系统调用有: 系统调用 描述 int fork() 创建一个进程，返回子进程的PID int exit(int status) 终止当前进程，并将状态报告给wait()函数。无返回 int wait(int *status) 等待一个子进程退出; 将退出状态存入*status; 返回子进程PID。 int kill(int pid) 终止对应PID的进程，返回0，或返回-1表示错误 int getpid() 返回当前进程的PID int sleep(int n) 暂停n个时钟节拍 int exec(char *file, char *argv[]) 加载一个文件并使用参数执行它; 只有在出错时才返回 char *sbrk(int n) 按n 字节增长进程的内存。返回新内存的开始 int open(char *file, int flags) 打开一个文件；flags表示read/write；返回一个fd（文件描述符） int write(int fd, char *buf, int n) 从buf 写n 个字节到文件描述符fd; 返回n int read(int fd, char *buf, int n) 将n 个字节读入buf；返回读取的字节数；如果文件结束，返回0 int close(int fd) 释放打开的文件fd int dup(int fd) 返回一个新的文件描述符，指向与fd 相同的文件 int pipe(int p[]) 创建一个管道，把read/write文件描述符放在p[0]和p[1]中 int chdir(char *dir) 改变当前的工作目录 int mkdir(char *dir) 创建一个新目录 int mknod(char *file, int, int) 创建一个设备文件 int fstat(int fd, struct stat *st) 将打开文件fd的信息放入*st int stat(char *file, struct stat *st) 将指定名称的文件信息放入*st int link(char *file1, char *file2) 为文件file1创建另一个名称(file2) int unlink(char *file) 删除一个文件 进程和内存 xv6 的进程拥有用户态内存(指令，数据，堆栈)和由内核管理的进程状态。xv6 在进程之间时分复用处理器，即它透明地切换待执行的进程进入处理器执行。当进程被切换出处理器不再执行时，xv6 会为其保存 CPU 寄存器(保存上下文)，并在该进程下次执行时将保存的 CPU 寄存器重新加载 (恢复上下文)。操作系统内核为每个进程分配一个标识符，即 PID, 作为该进程的索引。 fork 系统调用 一个进程可以使用 fork 系统调用创建一个新进程 新创建的进程具有和 caller process 相同的内存内容，称为子进程 fork 函数创建子进程成功后立刻返回到 caller process ，返回值是子进程的 PID。在子进程中，fork 返回值是 0， 可以利用这一点为父进程和子进程设计不同的执行分支 exit 系统调用 一个进程可以使用 exit 系统调用终止自己的执行，并释放自己占用的所有资源 exit 接受一个参数用于指示 caller process 的退出状态，按照惯例，状态 0 表示正常退出，状态 1 表示进程因错误退出。 wait 系统调用 一个进程可以使用 wait 系统调用等待其子进程执行完成，同时阻塞自己的执行 如果 caller process 的子进程之一退出，wait 返回退出子进程的 PID，如果 caller process 没有子进程， wait 立即返回 -1 wait 接受一个指针作为参数，子进程的退出状态将被拷贝到该指针位置. 如果 caller process 不关心子进程的退出状态，可以传递空指针 (0) exec 系统调用 一个进程可以使用 exec 系统调用将自己的进程内存替换为从文件系统中新加载的内存镜像，并保留原进程的文件描述符表。 exec 函数执行成功后并不返回 caller process ，它会直接执行其内存中的指令(即从文件系统加载的指令). exec 只有在错误时 (加载文件或替换进程内存时的错误，而非新的程序执行时的错误) 才返回, 因此 caller process 中 exec 函数之后的程序应该只有错误处理过程。 IO 和文件描述符(file descriptors) 文件描述符是一个整数值，其代表一个指向内核管理的对象的指针。进程可以通过 open 文件、文件夹、或设备，创建 pipe, 或 dup 一个已有的文件描述符来获取文件描述符。 文件描述符旨在提供一种抽象的接口，它使得操作系统管理的 IO 设备在接口上没有差异，都是字节流的形式。 值得注意的是文件描述符的管理是逐 process 的，操作系统会为每个 process 维护一张文件描述符表。其中三个特殊的文件描述符是: 0 代表标准输入 (standard input) 1 代表标准输出 (standard output) 2 代表标准错误 (standard error) 文件描述符的数值会递增地分配，即: 新获取的文件描述符将总是所有未占用文件描述符中数值最小的。 read 和 write 系统调用 read 和 write 从文件描述符指向的已打开的文件中读取或向其写入字节。 对于指向文件的文件描述符，其总是具备一个偏移量 offset，该 offset 指示该从该文件读取或向该文件写入的字节起点。 对于 read， read(fd, buf, n) 表示从文件描述符 fd 指向的文件中读取最多 n 个字节，并将读取的数据复制到指针 buf 所指向的内存中，同时返回本次读取到的字节长度。值得注意的是 read 会从文件描述符 fd 所具备的 offset 位置开始读取，并在读取后将 offset 的值递增到本次读取的终点，即：下次对 fd 的读取将从本次读取的终点之后开始。当 offset 到达文件的终点时，新的 read 调用将返回 0， 代表文件已读取完毕。 对于 write, write(fd, buf, n) 表示向文件描述符 fd 指向的文件中写入 n 个来自指针 buf 指向的内存位置的字节。同样地，write 会返回本次写入的字节长度，当返回值小于 n 时，代表本次写入有错误发生。对于 offset 的处理也与 read 类似：写入总是从 offset 的位置开始，并在结束后将 offset 递增到本次写入的终点。 关于文件描述符的 offset: 如果一个文件描述符是从另一个已经存在的文件描述符得到的，即通过 fork 复制文件描述符表或 dup 复制文件描述符的方法，那么该文件描述符与原文件描述共享同一个 offset。否则，新的文件描述符(如通过 open 同一个文件)即使指向相同的 IO 设备，也不会共享 offset open 系统调用 open 系统调用打开一个文件并为其分配文件描述符，open(filename, flag) 表示以 flag 模式打开文件 filename. 可选的 flag 有： O_RDONLY: 只读模式 O_WRONLY: 只写模式 O_RDWR: 读写模式 O_CREATE: 文件不存在则创建 O_TRUNC: 截断模式，打开时将文件内容清空 close 系统调用 close 系统调用释放一个文件描述符，使其在后续的分配中可用。 操作系统会保证新分配的文件描述符总是所有可用文件描述符中数值最小的(否则随着系统的运行，文件描述符的数值终将导致溢出) dup 系统调用 dup 系统调用复制一个已有的文件描述符并返回，两个文件描述符将指向同一个 IO 设备，并共享一个 offset 管道 (pipe) pipe 是一个小型内核缓冲区，其以一对文件描述符的形式暴露给用户进程：一个用于读取，另一个用于写入。读取端文件描述符可以读取向写入端文件描述符写入的数据，内核以这种形式向用户态提供了一种跨进程通信的方式。 pipe 以如下方式调用: 1234int pipefds[2];pipe(pipefds);// pipefds[0] is for reading// pipefds[1] is for writing 值得注意的是 pipe 以阻塞式语义进行读写，即: 对读取端文件描述符的读取操作 (read) 会等待写入端的写入。 这一特性还可以重述为：对读取端文件操作符的读取会一直阻塞到写入端文件描述符有数据写入。另一种结束读取端阻塞的情况是: 所有指向 pipe 写入端的文件描述符都被关闭，此时对 pipe 读取端的 read 操作会返回 0，与对非 pipe IO 设备 read 时遇到 EOF 相似。 跨进程通信也可以通过向临时文件读写数据实现，但 pipe 相比这种方式至少有以下四点优势: pipe 会自动清理其缓冲区，临时文件则需要在通信结束后手动处理 pipe 可以传递任意长度的数据，临时文件则受到文件系统的限制 pipe 允许读写阶段的并行执行，临时文件方案则需要两个阶段逐个执行 当使用 pipe 实现进程间通信时，其阻塞式读写语义将比文件的非阻塞式语义更高效 对于 3, 4，一个理解是: pipe 的阻塞式读写使得读取端对写入端有感知，因此可以在写入端写入一段数据后立刻读取，而不用等待其全部写入后再读取，因此是并行的。而文件方案的读取端对文件的写入没有感知，因此必须等待写入端写完数据并显式通知读取端才可以读取，否则很可能读到缺少的或重复的数据。 还是因为 pipe 的阻塞读写语义，在构建跨进程通信程序时不需要在如何同步读写操作上花费功夫，因此更加高效 文件系统(File system) 文件系统中，文件名 (file name / file path) 只是对文件系统中文件本体的一个具名引用。文件系统中的文件本体叫做 inode. 一个 inode 可以有多个文件名，称为 links. 一个 link 即目录中的一个条目，包含一个文件名和一个指向 inode 的引用。 inode 描述了文件的元数据 metadata, 包括文件的类型 (文件/目录/设备)，长度，文件在磁盘上的地址，该文件的 link 数目。 fstat 系统调用 fstat 系统调用通过一个文件描述符查询该文件描述符所指向的 inode 的信息，其结果以结构体 stat 的形式返回： 1234567891011#define T_DIR 1#define T_FILE 2#define T_DEVICE 3struct stat &#123; int dev; // disk device of the file system uint ino; // inode number short type; // type of file short nlink; // number of links uint64 size; // size of the file in bytes&#125; chdir, open, mkdir, mknod, link, unlink 系统调用 chdir 系统调用可以改变当前进程所在的当前文件夹。当计算相对路径时，进程的当前文件夹被用作参照 open 系统调用当使用 O_CREATE flag 时可以创建新的文件 mkdir 系统调用用于创建新的文件夹 mknod 系统调用于创建新的其他设备文件。具体地说，mknod 为一个已有的设备创建一个文件，这个文件的路径指向该设备。 该设备的主设备号和次设备号需要作为参数传入 mknod, 并于新创建的设备文件关联。这样一来，当用于进程访问该设备文件时，内核会将对该设备文件的 read 和 write 系统调用转发给内核中的设备实现，而不是调用文件系统中的实现。 link 系统调用为一个已有文件创建一个新的 link unlink 系统调用 unlink 系统调用从文件系统删除一个 link 在典型 unix 系统中，一个文件的 inode 以及文件在磁盘中的本体只有在文件系统中没有 link 指向该 inode, 并且所有进程中都不存在指向该文件的文件描述符时才会被释放和删除。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"Operating System","slug":"Operating-System","permalink":"https://blog.zhuwenq.cc/tags/Operating-System/"},{"name":"xv6","slug":"xv6","permalink":"https://blog.zhuwenq.cc/tags/xv6/"}]},{"title":"C++ 模板022-可变模板","slug":"C-模板022-可变模板","date":"2023-02-15T08:49:38.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"C-模板022-可变模板/","link":"","permalink":"https://blog.zhuwenq.cc/C-%E6%A8%A1%E6%9D%BF022-%E5%8F%AF%E5%8F%98%E6%A8%A1%E6%9D%BF/","excerpt":"自 C++ 11， 模板可以接受可变数量的参数，这一特性允许向模板传入任意数目的任意类型参数","text":"自 C++ 11， 模板可以接受可变数量的参数，这一特性允许向模板传入任意数目的任意类型参数 可变函数模板 例如，可以定义如下函数以实现任意类型任意参数的输出: 123456void print() &#123;&#125;template&lt;typename T, Typename... Args&gt;void print(T t, Args... args)&#123; std::cout &lt;&lt; t &lt;&lt; std::endl; print(args...);&#125; 当传入一个或多个参数时，参数列表中的第一个参数将匹配到 T t, 其余参数则被聚合在 Args... args 中，称为模板参数包. 在 print 模板内, 函数会递归地解包模板参数包并执行实例化的函数 当所有参数都解包完成时, 空的参数列表会匹配到 print 的非模板实现, 以终止递归 重载可变参数模板 要实现可变参数模板的递归终止, 也可以为其重载一个非可变参数模板: 123456789template&lt;typename T&gt;void print(T t)&#123; std::cout &lt;&lt; t &lt;&lt; std::endl;&#125;template&lt;typename T, typename... Args&gt;void print(T t, Args... args)&#123; print(t); print(args...);&#125; 这是因为, 当两个函数模板仅区别于可变参数包时, 非可变参数模板将优先被决议. 因此当参数解包到只剩下一个参数时, 非可变参数模板将优先被匹配 可变参数模板的 sizeof 运算符 C++ 11 为模板参数包引入了新的 sizeof 运算符: 12345template&lt;typename T, typename... Ts&gt;void size(T t, Ts... ts)&#123; sizeof...(Ts); // number of types in Ts sizeof...(ts); // number of values in ts&#125; fold expressions fold expressions 是 C++ 17 引入的一项新特性，它允许使用二元运算符在模板参数包中的所有参数上计算结果 例如如下的函数模板： 1234template&lt;typename... Ts&gt;auto foldSum(Ts... ts)&#123; return (... + ts);&#125; 其效果是从 ts 中的第一个参数加到其最后一个参数，即: 1(((ts1 + ts2) + ... )+ tsn) 注意如果传入的参数包为空，那么 fold expression 通常不会通过编译(ill-formed)。但对于以下操作符，其可以返回确定的结果: operator result &amp;&amp; true ` , void() fold expression 的所有形式: fold expression evaluation (... op pack) (((pack1 op pack2) op pack3) ... op packN) (pack op ...) (pack1 op (... op (packN-1 op packN))) (init op ... op pack) (((init op pack1) op pack2) ... op packN) (pack op ... op init) (pack1 op (... op (packN op init))) 可变类模板和其他可变表达式 可变参数包可以出现在: 表达式 类模板 using 声明 deduction guides 可变表达式 一些例子: 打印翻倍后的参数： 1234template&lt;typename... T&gt;void printDouble(T... args)&#123; print(args + args...);&#125; 效果相当于: 12345print( arg1 + arg1, arg2 + arg2, ...); 注意到此时 print 也应该是一个可变参数模板 打印加 1 后的参数： 12345template&lt;typename... T&gt;void printPlusOne(T... args)&#123; print(args + 1 ...); // or: print((args + 1)...);&#125; 相当于: 12345print( arg1 + 1, arg2 + 1, ...); 注意到此时 ... 符号不能直接跟在数字 1 后面，需要空开一个空格 编译时表达式: 1234template&lt;typename T1, typename... TN&gt;constexpr bool isHoMogeneous(T1, TN...)&#123; return (std::is_same_v&lt;T1, TN&gt; &amp;&amp; ...);&#125; 可变索引 利用可变表达式，可以实现对任意数量的数组索引访问的函数: 1234template&lt;typename Container, typename... Idx&gt;void printElems(Container const&amp; c, Idx... idx)&#123; print(c[idx]...);&#125; 注意到此时作为数组索引的参数 idx 可以是任意数量和任意类型，也可以将其声明为非类型模板参数以限制其类型: 1234template&lt;std::size_t... idx, typename Container&gt;void printElems(Container const&amp; c)&#123; print(c[idx]...);&#125; 可以以下面的形式调用该函数: 1printElems&lt;1, 2, 3, 5&gt;(c); // print(c[1], c[2], c[3], c[5]); 可变类模板 元组类 元组类需要能够持有任意数量任意类型的值: 12template&lt;typename... Elems&gt;class Turple; Variant Variant 对象可以持有预先声明的任意类型的值: 12template&lt;typename... Elems&gt;class Variant; 表示任意下标列表的类型 12template&lt;std::size_t... idx&gt;struct Index &#123;&#125;; 此类型可以将传入的下标表示为列表, 但是其并未实际(内存上)持有这些下标值, 毕竟该类型定义为一个空的 struct。 它的表示功能实际上发生在编译期, 当该类型的对象作为参数传入函数时, 实际上是某种类型萃取功能实现了 Idx 的传递: 1234template&lt;typename Container, std::size_t... Idx&gt;void printByIdx(Container const&amp; c, Index&lt;Idx...&gt;)&#123; // trait Idx out print(c[Idx]...);&#125; variadic deduction guides 对于接受可变参数的函数模板，当其需要返回值deduction guide 时，deduction guide 也可以是可变表达式。 例如 STL 中对 std::array 构造函数的声明: 123456namespace std &#123;template&lt;typename T,typename ... U&gt;array(T, U...) -&gt; array&lt; enable_if_t&lt;(is_same_v&lt;T, U&gt; &amp;&amp; ...), T&gt;, (1 + sizeof...(U))&gt;;&#125; 此声明中为 std::array 定义了返回值 deduction guide, 并在其中使用了 enable_if, 以实现当构造函数传入的初始值类型不同时禁用构造函数. 在构造函数传入的可变参数包的类型判断上，该 deduction guide 使用了可变表达式。 同时在参数包数量的计算上，使用了用于可变参数包的 sizeof 操作符 可变基类和 using 声明 在继承体系中基类也可以是可变的，即，可以通过参数包实现对任意数量任意类型的继承。 例如： 123456789101112131415161718192021222324252627282930313233#include &lt;string&gt;#include &lt;unordered_map&gt;class Customer &#123;private: std::string name;public: Customer(std::string const&amp; n) : name(n) &#123; &#125; std::string getName() const &#123; return name; &#125;&#125;;struct CustomerEq &#123; bool operator() (Customer const&amp; c1, Customer const&amp; c2) const &#123; return c1.getName() == c2.getName(); &#125;&#125;;struct CustomerHash &#123; bool operator() (Customer const&amp; c) const &#123; return std::hash&lt;std::string&gt;(c.getName()); &#125;&#125;;template&lt;typename... Bases&gt;struct Overloader : Bases... &#123; using Bases::operator()...;&#125;;int main() &#123; using CustomOps = Overloader&lt;CustomerEq, CustomerHash&gt;; std::unordered_set&lt;Customer, CustomerHash, CustomerEq&gt; s1; std::unordered_set&lt;Customer, CustomOps, CustomOps&gt; s2;&#125; 该例中，CustomOps 通过可变基类同时继承了 CustomerEq 和 CustomerHash 这两个函数对象, 并且在定义中使用可变 using 声明同时启用了两个基类的 () 操作符 此时的 CustomOps 同时具备了 CustomerEq 和 CustomerHash 这两个函数对象的功能. 即通过 CustomOps 调用 () 操作符时，其会根据参数自动匹配基类的定义。 因此，在 main 函数中定义的两个自定义相等和哈希操作的 unordered_set 能够实现相同的功能。 可变参数模板一些场景 可变参数模板在标准模板库中应用广泛, 主要用途体现在参数转发上 在一些资源管理类中其可用于转发任意数量任意类型的参数到被管理对象的构造函数。 如: 1auto share_ptr = std::make_shared&lt;std::complex&lt;float&gt;&gt;(1.1, 2.2); 将可调用对象的参数传递进 thread 对象: 1std::thread t(f, 1, &quot;hello&quot;); // call f(1, &quot;hello&quot;) in thread t 在容器中接收构造对象所需的参数并直接构造对象(emplace): 12std::vector&lt;std::pair&lt;int, int&gt;&gt; vec;vec.emplace_back(1, 2); // construct std::pair&lt;int, int&gt; &#123;1, 2&#125; in vec 对于传递参数的函数, 其参数通常被声明为万能引用的形式.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"}]},{"title":"C++模板021-非类型模板参数","slug":"C-模板021-非类型模板参数","date":"2023-02-14T12:42:15.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"C-模板021-非类型模板参数/","link":"","permalink":"https://blog.zhuwenq.cc/C-%E6%A8%A1%E6%9D%BF021-%E9%9D%9E%E7%B1%BB%E5%9E%8B%E6%A8%A1%E6%9D%BF%E5%8F%82%E6%95%B0/","excerpt":"C++ 模板的参数不必是类型，也可以是编译时整型常量。例如标准模板库中的 std::array 就接受非类型模板参数用于描述数组的长度: std::array&lt;int, 10&gt;","text":"C++ 模板的参数不必是类型，也可以是编译时整型常量。例如标准模板库中的 std::array 就接受非类型模板参数用于描述数组的长度: std::array&lt;int, 10&gt; 非类型函数模板参数 例如： 1234template&lt;int val, typename T&gt;T addVal(T x)&#123; return x + val;&#125; 该函数模板的实例可以作为可调用对象传入标准模板库中的算法： 123transform(nums.begin(), nums.end() dest.begin(), addVal&lt;5, int&gt;); 非类型模板参数的限制 只有 常量整数值(包括枚举), 指针(指向对象/函数/成员), 左值引用(指向对象/函数), std::nullptr_t 浮点数和类类型对象不可以做为非类型模板参数 当指针或引用作为模板参数时，其所指向的参数不能是字符串字面量, 临时变量或数据成员以及其他子对像 auto 作为非类型模板参数的类型 自 C++ 17 ，非类型模板参数的类型可以被声明为 auto ，以实现可适用于所有合法类型的模板 例如可以定义: 123456template&lt;typename T, auto MaxSize&gt;class Stack &#123;public: using size_type = decltype&lt;MaxSize&gt;; ...&#125; 定义中 Stack 的大小以非类型模板参数的形式传入，且其类型被声明为 auto. 同时，在定义中可以通过 decltype 取出其类型 值得注意的是，也可以将非类型模板参数的类型声明为 decltype(auto), 这样，其类型将允许被推导为引用类型： 1234567int i&#123;0&#125;;template&lt;auto I&gt;class A &#123; &#125;;A&lt;i&gt; a; // I is of type inttemplate&lt;decltype(auto) J&gt;class B &#123; &#125;;B&lt;i&gt; b; // J is of type int&amp;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"}]},{"title":"C++ 模板01--模板基础","slug":"C-模板01-模板基础","date":"2022-11-17T13:08:23.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"C-模板01-模板基础/","link":"","permalink":"https://blog.zhuwenq.cc/C-%E6%A8%A1%E6%9D%BF01-%E6%A8%A1%E6%9D%BF%E5%9F%BA%E7%A1%80/","excerpt":"包括函数模板, 类模板, 别名模板, 变量模板基本的用法, 偏特化和全特化的基本知识, 以及类型推导的基本用法.","text":"包括函数模板, 类模板, 别名模板, 变量模板基本的用法, 偏特化和全特化的基本知识, 以及类型推导的基本用法. 基本用法 函数模板 对于常用的 max 函数, 其返回输入参数中最大的那个. 这一操作对许多种类型都适用, 如果不想对每种参数都实现相同的行为, 就需要使用模板技术定义一个函数模板: 12345// return the max value of the argstemplate&lt;typename T&gt;T max(T a, T b) &#123; return a &lt; b ? b : a;&#125; 对于上述定义, 所有支持 &lt; 运算符的类型 T 都可以实例化一个 max 函数的特殊版本: 1234567891011121314#include&lt;string&gt;int main () &#123; int i = 42; ::max(7, i); // int max(int ,int ); 42 double f1 = 3.4; double f2 = -6.7; ::max(f1, f2); // double max(double, double); 3.4 std::string s1 = &quot;abc&quot;; std::string s2 = &quot;abcdefg&quot;; ::max(s1, s2); // std::string max(std::string, std::string); abcdefg&#125; 模板也可以有多个不同的参数, 例如: 1234template&lt;typename T1, typename T2, typename RT&gt;RT max(T1 a, T2 b)&#123; return a &lt; b ? b : a;&#125; 此例中可以为函数的参数和返回值分别设置一个类型, 例如: 1::max&lt;int, double, double&gt;(1, 1.1); 重载 函数模板可以被重载, 即, 另外定义一个非模板函数或不同的函数模板. 当调用是的参数类型完美匹配时, 重载决议会优先决议非模板的版本: 1234567891011121314151617181920int max(int a, int b) &#123; return a &lt; b ? b : a;&#125;template&lt;typename T&gt;T max(T a, T b) &#123; return a &lt; b ? b : a;&#125;template&lt;typename RT, typename T1, typename T2&gt;RT max(T1 a, T2 b) &#123; return a &lt; b ? b : a;&#125;int main() &#123; ::max(1, 2); // non-template ::max&lt;&gt;(1, 2); // template; int max&lt;int&gt;(int, int); ::max(1, 2.1); // non-template; ::max&lt;float&gt;(1, 2.1); // template; float max&lt;float, int, float&gt;(int, float);&#125; 类模板 类型的定义也可以使用一个或多个类型参数化, 例如标准模板库中的容器类型就是类模板. 作为一个例子, 下面定义一个简单的容器适配器 Stack: 12345678910111213141516171819202122232425262728293031#include &lt;vector&gt;#include &lt;cassert&gt;template&lt;typename T, typename Cont = std::vector&lt;T&gt;&gt;class Stack &#123;private: Cont elems; // elementspublic: void push(T const&amp; elem); // push element void pop(); // pop element T const&amp; top() const; // return top element bool empty() const &#123; // return whether the stack is empty return elems.empty(); &#125;&#125;;template&lt;typename T, typename Cont&gt;void Stack&lt;T,Cont&gt;::push (T const&amp; elem) &#123; elems.push_back(elem); // append copy of passed elem&#125;template&lt;typename T, typename Cont&gt;void Stack&lt;T,Cont&gt;::pop () &#123; assert(!elems.empty()); elems.pop_back(); // remove last element&#125;template&lt;typename T, typename Cont&gt;T const&amp; Stack&lt;T,Cont&gt;::top () const &#123; assert(!elems.empty()); return elems.back(); // return copy of last element&#125; 别名模板 C++ 中的 using 关键字也可以模板化: 1234template&lt;typename T&gt;using DequeStack = Stack&lt;T, std::deque&lt;T&gt;&gt;;auto stack = DequeStack&lt;int&gt;; // a stack of int with the base container being a std::deque&lt;int&gt; 别名模板还可以用来定义模板化的成员类型, 例如对于迭代器类型, 假如存在定义: 12345template&lt;typename T, typename Cont = std::vector&lt;int&gt;&gt;class Stack &#123;public: using iterator = ...;&#125; 则可以通过: 12template&lt;typename T&gt;using StackIterator = typename Stack&lt;T&gt;::iterator; 来定义任意元素类型的 Stack 迭代器类型. 注意 Stack&lt;T&gt;::iterator 前面的 typename 不可缺少, 否则编译器并不敢假设后面的东西是个类型. C++14 中 type traits 的 _t 后缀 标准模板库中有一种 type traits 基础设施用于对类型进行操作, 其结果也是一种类型. 在 C14 之前, 这类设施只能通过 std::some_traits&lt;T&gt;::type 使用. 在 C14 中, 这类设施都使用 _t 后缀进行了定义: 1234namespace std &#123; template&lt;typename T&gt; using some_traits_t = typename some_trait&lt;T&gt;::type;&#125; 这样就可以通过简单直观的方式使用这类设施: 1std::some_traits_t&lt;T&gt; 变量模板 自 C++14, 变量也可以被参数化定义, 例如: 123456template&lt;typename T = long double&gt;constexpr T pi&#123;3.1415926&#125;;std::cout &lt;&lt; pi&lt;&gt; &lt;&lt; std::endl; // outputs a long doublestd::cout &lt;&lt; pi&lt;float&gt; &lt;&lt; std::endl; // outputs a floatstd::cout &lt;&lt; pi &lt;&lt; std::endl;; // Error, you always need to specify the angle brackets 模板参数也可以使用非类型参数: 12345678910111213141516#include&lt;iostream&gt;#include&lt;array&gt;template&lt;int N&gt;std::array&lt;int, N&gt; arr&#123;&#125;;template&lt;auto N&gt;constexpr decltype(N) dval = N;int main() &#123; std::cout &lt;&lt; dval&lt;&#x27;a&#x27;&gt; &lt;&lt; std::endl; arr&lt;10&gt;[0] = 1; for (auto i = 0; i &lt; arr&lt;10&gt;.size(); ++i)&#123; cout &lt;&lt; arr&lt;10&gt;[i] &lt;&lt; std::endl; &#125;&#125; 变量模板的一大用处是可以用来定义类的数据成员. 例如, 对于标准模板库中的另一种 type traits 基础设施: 123456789namespace std &#123; template&lt;typename T&gt; class numeric_limits &#123; public: ... static constexpr bool is_signed = false; ... &#125;;&#125; 对于类型 T, 只能通过以下方式使用该设施: 1std::numeric_limits&lt;T&gt;::is_signed 如果定义: 12template&lt;typename T&gt;constexpr bool isSigned = std::numeric_limits&lt;T&gt;::is_signed; 就可以通过 isSigned&lt;T&gt; 使用该设施。 C++17 中 type taits 的 _v 后缀 基于变量模板技术, C++17 为结果为值的 type traits 基础设施定义了 _v 后缀的版本: 1234namespace std &#123; template&lt;typename T&gt; constexpr bool is_some_traits_v = is_some_traits&lt;T&gt;::value;&#125; 偏特化和全特化 特化 (Specialization) 是针对类模板的一种用法, 与函数模板可以重载类似, 类模板的特化提供了一种为特殊的模板参数或模板参数组合实现不同行为的类模板定义方法. 通常类模板的特化可以为特殊的模板参数定义更优化的类实现, 或者修正在某些模板参数下模型的定义有误的情况. 例如, 可以针对 POD 类型提供特殊的容器定义以实现更加紧凑的内存布局和更加快速的数据拷贝. 需要注意的是, 在类模板的特化中, 其所有的成员都需要有对应的特化实现. 全特化 (Specialization) 类模板的全特化声明由一个空的模板参数列表开头: 12template&lt;&gt;class Stack&lt;std::string&gt;; 在全特化模板的实现中, 所有的成员签名类似于非类模板成员的签名, 即不以 template&lt;&gt; 开头: 1void Stack&lt;std::string&gt;::push(std::string const&amp; s); 以上就是类模板全特化的基本用法. 在实际使用中, 可以通过全特化为特定类型提供特定的实现. 例如在一个例子中, 可以为 std::string 类型的 Stack 提供一个以 std::deque 为底层容器的版本: 123456789101112#include&lt;deque&gt;#include&lt;string&gt;template&lt;&gt;class Stack&lt;std::string&gt; &#123;private: std::deque&lt;std::string&gt; elems;public: void push(std::string const&amp;); void pop(); std::string const&amp; top() const; bool empty() const;&#125; 偏特化 (Partial Specialization) 偏特化主要用在多模板参数的类模板中, 可以利用偏特化技术为多个模板参数中的部分提供具体的特化. 在单参数模板中也可以将类型参数特华为特殊的形式 (如指针类型) 将模板参数特化为特殊形式 1234template&lt;typename T&gt;class Stack&lt;T*&gt;; // specialization for raw pointerstemplate&lt;typename T1, typename T2&gt;class Widget&lt;T1*, T2*&gt;; // specialization for raw pointers 特化部分模板参数 123456template&lt;typename T1&gt;class Widget&lt;T1, std::string&gt;; // specialization for std::string as T2template&lt;typename T1&gt;class Widget&lt;T1, T1&gt;; // specialization for T1 as T2template&lt;typename T1, typename T2&gt;class Widget&lt;T1*, T2*&gt;; 类型推导 (Deduction) C17 允许对类型模板的实例化语句省略显示的模板类型, C 编译器会自动推导实例化的模板参数. 例如: 123Stack&lt;int&gt; istack; // stack of intsStack&lt;int&gt; istack2 = istack; // stack of intsStack istack3 = istack; // OK since C++17, stack of ints istack3 的定义语句中不需要显示声明模板参数 &lt;int&gt;, 编译器会自动根据拷贝对象的类型推导出模板参数为 int 这种级别的类型推导是由构造函数的定义决定的, 因此, 只要构造函数提供相应的支持, 类似上面示例中的参数推导可以推广到任意的形式: 1234567891011template&lt;typename T&gt;class Stack &#123;private: vector&lt;T&gt; elems; // vector as base containerpublic: Stack() = default; Stack(T const&amp; elem) : elems(&#123;elem&#125;) // &#123;&#125; around to initialize vector elems, otherwise wrong constructor will be invoked &#123;&#125;; // support for construction from one element of type T&#125;;Stack s = 0; // stack of int, with one element of value 0 对字符串字面量的处理 上面的 Stack 定义支持从任意单个值实例化模板并初始化容器, 并且不需要显示传递模板参数. 但是当以字符串字面量实例化上述模板时, 由于其特殊的类型, 会造成一些特殊结果: 1Stack s = &quot;string literal&quot;; // &quot;string literal&quot; is of type `char const[15]`, thus s is of type Stack&lt;char const[15]&gt; 这是由于在上面的构造函数定义中, elem 被声明为了引用类型, 而对于字符串字面量的引用类型, 其不会像值类型一样在参数传递的过程中 decay, 因此参数 T 就如实地保留了字符数组的原始类型 如果上面对 Stack 的定义中构造函数定义为参数按值传递, s 的推导类型就会变成 Stack&lt;char const*&gt;. Deduction Guide 上面对于字符串字面量的处理最多可以将其引导为指针类型, 而且需要改变构造函数的定义. 通过后置类型声明的语法, 则可以为构造函数显示声明实例化的结果. 例如添加以下声明: 1Stack(char const*) -&gt; Stack&lt;std::string&gt;; 字符串字面量作为初始化物时得到的对象类型就会是 Stack&lt;std::string&gt;. 但需要注意此时无法再使用拷贝操作符初始化 Stack, 这是因为: 1Stack s = &quot;string literal&quot;; 将 s 实例化为 Stack&lt;std::string&gt;, 那么 s 的定义中构造函数就变为: 1Stack&lt;std::string&gt;::Stack(std::string const&amp;); 此构造函数无法接纳字符串字面量为参数. 也就是说, 此时拷贝操作符所在的语句对类模板的实例化是成功的, 但调用构造函数的过程失败了. 此失败是由于并不存在从字符串字面量类型即 char const[] 向 std::string 的直接转换. 因此通过以下语句可以实现需要的实例化和初始化: 1Stack s&#123;&quot;string literal&quot;&#125;; 这是因为大括号初始化语句会自动以传入的量为参数调用目标类型的构造函数, 而 std::string 显然是存在以字符串字面量为参数的构造函数的.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"}]},{"title":"C++ template","slug":"C-template","date":"2022-11-14T08:38:26.000Z","updated":"2023-06-21T06:51:28.780Z","comments":true,"path":"C-template/","link":"","permalink":"https://blog.zhuwenq.cc/C-template/","excerpt":"大概半个月前在看 cppcon 2022 一个 talk 时候偶然发现 Nicolai M. Josuttis 这么一个技术作家, 打算把他的 the complete guide 系列, 包括 C++ templates: the complete guide, C++ 17: the complete guide 和 C++ 20: the complete guide 都看一看。最近刚读完 C++ templates: the complete guide 的第一部分也是基础部分的 11 章。 书中许多概念也在别的书例如 Effective Modern C++ 出现过, 这里打算完整地写几篇文章作为我对 C++ template 的学习总结, 主要记录下那些我阅读本书之前未知的新知识和虽然是已知的知识但尚未考虑过的地方。希望能够顺利完成这一系列。","text":"大概半个月前在看 cppcon 2022 一个 talk 时候偶然发现 Nicolai M. Josuttis 这么一个技术作家, 打算把他的 the complete guide 系列, 包括 C++ templates: the complete guide, C++ 17: the complete guide 和 C++ 20: the complete guide 都看一看。最近刚读完 C++ templates: the complete guide 的第一部分也是基础部分的 11 章。 书中许多概念也在别的书例如 Effective Modern C++ 出现过, 这里打算完整地写几篇文章作为我对 C++ template 的学习总结, 主要记录下那些我阅读本书之前未知的新知识和虽然是已知的知识但尚未考虑过的地方。希望能够顺利完成这一系列。 打算分 9 部分记录我所学到的 C++ template 的知识, 分别是: [x] 模板基础包括函数模板, 类模板, 别名模板, 变量模板基本的用法, 偏特化和全特化的基本知识, 以及类型推导的基本用法 [x] 非类型模板参数和可变参数模板包括将数值作为模板参数的技术以及可变参数模板技术, 涉及到基于可变参数模板的编译时递归和它的应用 [x] 基本的模板 Tricks包括本书介绍的几种奇技淫巧 [ ] 移动语义包括右值引用, 万能引用, 完美转发的基本知识和应用 [ ] enable_if&lt;&gt; 和 SFiNAE包括通过 disable 某些参数组合的模板实例化来指导编译器更好地进行重载决议的 enable_if&lt;&gt; 和它背后的 SFiNAE 技术 [ ] 其他值得讨论的地方包括按值传递参数和按引用传递参数的讨论, 对字符串字面量参数和原始数组参数的处理 [ ] 模板元编程基础包括使用非类型模板参数和编译时递归技术的模板元编程示例, constexpr 关键字的使用, 利用偏特化技术实现的执行路径选择, 编译时 if 等 [ ] 模板实践知识包括模板定义和实现分离带来的链接器错误, 解决编译时长问题的预编译头技术, 解读编译器为模板生成的冗长的 error novel 报错信息等 [ ] 泛型库包括标准模板库提供的一些泛型方法和包括 type traits 在内的泛型基础设施","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"}]},{"title":"A random afternoon","slug":"A-random-afternoon","date":"2022-11-05T12:56:06.000Z","updated":"2023-06-21T06:51:28.724Z","comments":true,"path":"A-random-afternoon/","link":"","permalink":"https://blog.zhuwenq.cc/A-random-afternoon/","excerpt":"Shot with Nikon D610.","text":"Shot with Nikon D610.","categories":[{"name":"Photograph","slug":"Photograph","permalink":"https://blog.zhuwenq.cc/categories/Photograph/"}],"tags":[]},{"title":"Somewhere in the campus","slug":"Somewhere-in-the-campus","date":"2022-11-05T12:45:30.000Z","updated":"2023-06-21T06:51:28.884Z","comments":true,"path":"Somewhere-in-the-campus/","link":"","permalink":"https://blog.zhuwenq.cc/Somewhere-in-the-campus/","excerpt":"Shot with Nikon D610.","text":"Shot with Nikon D610.","categories":[{"name":"Photograph","slug":"Photograph","permalink":"https://blog.zhuwenq.cc/categories/Photograph/"}],"tags":[]},{"title":"回答","slug":"回答","date":"2022-11-05T09:52:54.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"回答/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%9B%9E%E7%AD%94/","excerpt":"","text":"卑鄙是卑鄙者的通行证, 高尚是高尚者的墓志铭, 看吧, 在那镀金的天空中, 飘满了死者弯曲的倒影. 冰川纪过去了, 为什么到处都是冰凌? 好望角发现了, 为什么死海里千帆相竞? 我来到这个世界上, 只带着纸、绳索和身影, 为了在审判之前, 宣读那些被判决的声音. 告诉你吧，世界 我——不——相——信! 纵使你脚下有一千名挑战者, 那就把我算作第一千零一名. 我不相信天是蓝的, 我不相信雷的回声, 我不相信梦是假的, 我不相信死无报应. 如果海洋注定要决堤, 就让所有的苦水都注入我心中, 如果陆地注定要上升, 就让人类重新选择生存的峰顶. 新的转机和闪闪星斗, 正在缀满没有遮拦的天空. 那是五千年的象形文字, 那是未来人们凝视的眼睛. 北岛 『回答 』","categories":[{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/categories/poem/"}],"tags":[{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/tags/poem/"}]},{"title":"Red","slug":"Red","date":"2022-11-05T09:10:33.000Z","updated":"2023-06-21T06:51:28.836Z","comments":true,"path":"Red/","link":"","permalink":"https://blog.zhuwenq.cc/Red/","excerpt":"Shot with Nikon D610.","text":"Shot with Nikon D610.","categories":[{"name":"Photograph","slug":"Photograph","permalink":"https://blog.zhuwenq.cc/categories/Photograph/"}],"tags":[]},{"title":"宣告","slug":"宣告","date":"2022-10-28T06:23:51.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"宣告/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%AE%A3%E5%91%8A/","excerpt":"","text":"也许最后的时刻到了 我没有留下遗嘱 只留下笔, 给我的母亲 我并不是英雄 在没有英雄的年代里, 我只想做一个人。 宁静的地平线 分开了生者和死者的行列 我只能选择天空 绝不跪在地上 以显出刽子手的高大 好阻挡那自由的风 从星星的弹孔里 将流出血红的黎明 北岛 『献给遇罗克 』","categories":[{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/categories/poem/"}],"tags":[{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/tags/poem/"}]},{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","slug":"BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding","date":"2022-04-23T16:00:00.000Z","updated":"2023-06-21T06:51:28.780Z","comments":true,"path":"BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/","link":"","permalink":"https://blog.zhuwenq.cc/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding/","excerpt":"Introduction BERT 是一种新的语言表征模型，它是一种双向编码器表征的Transformers模型(Bidirectional Encoder Representions from Transformers) BERT 利用了无标签预训练文本数据的左右上下文信息，训练出一个深度双向表征模型。预训练后的模型可以通过一个额外的输出层并微调到多个下游任务，并取得 state-of-the-art 的结果。 本文中，BERT通过使用“掩码语言模型(Masked Language Model, MLM)”作为预训练目标函数，缓解了单向性约束，改进了基于微调的方法。MLM即随机地将输入中的一些token替换为掩码，并且目标是基于上下文预测被掩码位置token的原始词典id。这种目标函数使表征有能力融合左右上下文的信息，从而使训练一个深度双向Transformer成为可能。 BERT是第一个在大量句级和词级任务上取得最优结果的基于微调的表征模型，优于许多特定于任务的体系结构。","text":"Introduction BERT 是一种新的语言表征模型，它是一种双向编码器表征的Transformers模型(Bidirectional Encoder Representions from Transformers) BERT 利用了无标签预训练文本数据的左右上下文信息，训练出一个深度双向表征模型。预训练后的模型可以通过一个额外的输出层并微调到多个下游任务，并取得 state-of-the-art 的结果。 本文中，BERT通过使用“掩码语言模型(Masked Language Model, MLM)”作为预训练目标函数，缓解了单向性约束，改进了基于微调的方法。MLM即随机地将输入中的一些token替换为掩码，并且目标是基于上下文预测被掩码位置token的原始词典id。这种目标函数使表征有能力融合左右上下文的信息，从而使训练一个深度双向Transformer成为可能。 BERT是第一个在大量句级和词级任务上取得最优结果的基于微调的表征模型，优于许多特定于任务的体系结构。 Background 语言模型预训练在提升NLP任务上十分有效。在把预训练模型应用在下游任务上，目前有两种策略： feature-based 和 fine-tuning 。基于特征的方法，如 ELMo ，使用特定于任务的架构作为额外的特征，其中包括预训练表征。基于微调的方法，如 GPT 引入了最小特定于任务的参数，并且在下游任务上通过微调所有的预训练参数来进行训练。这两类方法在预训练时使用同样的目标函数：利用单向语言模型来学习通用语言表征。 我们认为现有的技术限制了预训练表征的能力，特别是对于微调的方法。主要表现在单向的标准语言模型限制了预训练时模型的架构选择。这样的限制对于句级(sentence-level)的任务通常是次优的，对于词级(token-level)的任务通常十分有害。 无监督的微调方法中，用于生成下文token表征的文档或句子编码器从无标签的文本中预训练，并针对有监督的下游任务进行微调。这样做的优势在于只有很少的参数需要从零开始学习。在这种模型的预训练过程中，通常使用自左向右的语言建模和自动编码器目标函数。 BERT BERT提出的框架分为两步：预训练和微调。在预训练中，模型通过不同的预训练任务在无标签的数据集上进行训练。在微调时，模型首先使用预训练得到的参数进行初始化，然后所有的参数都使用有标签的下游任务数据进行微调。每一种下游任务都使用相同的预训练模型进行初始化，然后分别微调。 模型架构 BERT的模型架构是一个多层双向Transformer编码器。 假设层数(即Transformer blocks)为 LLL ，隐藏层尺寸为 HHH，self-attention heads数量为 AAA。初步设计了两种尺寸的模型： BERTBASE(L=12, H=768, A=12, Total Parameters=110M)\\text{BERT}_\\text{BASE}(\\text{L=12, H=768, A=12, Total Parameters=110M})BERTBASE​(L=12, H=768, A=12, Total Parameters=110M)和 BERTLARGE(L=24, H=1024, A=16, Total Parameters=340M)\\text{BERT}_\\text{LARGE}(\\text{L=24, H=1024, A=16, Total Parameters=340M})BERTLARGE​(L=24, H=1024, A=16, Total Parameters=340M) 输入输出表征 针对不同的下游任务需要，BERT可以清晰地将一个单独的句子或一个句子对表示为一个序列。其中，句子可以指一段文本中的任务跨度，而不仅仅是实际的语言句子。一个序列指输入到BERT的token序列，可以是一个单独的句子或两个句子打包在一起。 每一个序列中的第一个token总是一个特殊的分类token：[CLS]，对应的最终隐藏状态用于分类任务中作为序列的总体类型表示。对于打包在一起的句子对，使用两种方法对其进行区分：1. 可以使用一个特殊的token [SEP]作为分段标志。2. 为每个token添加一个可学习的embedding来指示这个token属于句子A或句子B 假设输入embedding是 EEE，特殊token [CLS]的最终隐藏向量是 C∈RHC\\in \\mathbb{R}^HC∈RH，第 iii 个输入token的最终隐藏向量为 Ti∈RHT_i\\in \\mathbb{R}^HTi​∈RH 对于一个输入token，它的表征由对应的token embedding, 分段embedding和位置embedding的加和构成。 预训练 BERT的预训练使用两种无监督任务： Masked LM : 直觉上来说，深度双向模型比单向模型或者两个单向模型连接的效果更好。但是，标准的条件语言模型只能是单向的。这是因为标准语言模型是以预测当前词为目标，而双向模型将允许每个词“看到自己”，这样模型就可以在多层上下文中直接给出目标词。 为了训练双向表征，BERT使用MLM： 掩码token最终的隐藏向量被馈送到词典上的输出softmax中。 这种训练目标带来一个问题：在预训练和微调之间形成了割裂——在微调中不存在所谓的掩码。针对这个问题，我们并不是总是将掩码位置的token替换为[MASK]，而是以下述规则进行掩码操作： 然后， TiT_iTi​ 被用于使用交叉熵预测原始的token Next Sentence Prediction, NSP (后续的工作认为该任务无效)： 具体地说，选择两个句子A，B作为一个预训练样本。50%的时间B是A的下一句(标记为 IsNext\\text{IsNext}IsNext)，另外50%的时间不是(标记为 NotNext\\text{NotNext}NotNext)。上文所述的对应于特殊token [CLS]的最终隐藏向量用于NSP任务，其值经过softmax得到NSP的预测概率。 微调 微调的过程即在预训练后的BERT模型上添加一个简单的分类层，然后在下游任务数据上调整所有的参数。 对于每个下游任务， 只需要简单地插入特定于任务的输入输出数据并端到端微调所有的参数。 Evaluation","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"BERT","slug":"BERT","permalink":"https://blog.zhuwenq.cc/tags/BERT/"}]},{"title":"A Brief Introduction to PLMs","slug":"A-Brief-Introduction-to-PLMs","date":"2022-04-22T08:00:55.000Z","updated":"2023-06-21T06:51:28.712Z","comments":true,"path":"A-Brief-Introduction-to-PLMs/","link":"","permalink":"https://blog.zhuwenq.cc/A-Brief-Introduction-to-PLMs/","excerpt":"Introduction to PLMs","text":"Introduction to PLMs Think Language Modeling as Representation Learning 自 [@Bengio2003] 将前馈神经网络引入自然语言处理以来，表征学习就是语言建模的主要思想。通常认为，好的语言表征应该能够捕捉到文本的表层语法信息和隐式的语义知识。基于 [[Attention-is-all-you-need|Transformer]] 的 PLMs 实际上就是利用 self-attention 的序列建模能力学习上下文表征。 对于长度为 LLL 的自然语言语句 s={x1,x2,...,xL}\\mathbf{s}=\\{x_1, x_2, ..., x_L\\}s={x1​,x2​,...,xL​}, 其中 xix_ixi​ 是自然语言 token，表征计算即如下的过程： [h1,h2,...,hL]=Encoder(x1,x2,...,xL)[\\mathbf{h}_1, \\mathbf{h}_2, ..., \\mathbf{h}_L] = \\text{Encoder}(\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_L) [h1​,h2​,...,hL​]=Encoder(x1​,x2​,...,xL​) 其中 xi\\mathbf{x}_ixi​ 是 token xix_ixi​ 的嵌入向量，hi\\mathbf{h}_ihi​ 是 xix_ixi​ 的表征，Encoder\\text{Encoder}Encoder 是一个神经网络编码器。 通常，Encoder\\text{Encoder}Encoder 是多层堆叠结构，根据每层中的基本计算单元可以将其分为三类： Convolutional Language Encoder 卷积编码器通常用来捕捉局部上下文信息，其计算过程为： hi=CNN(xi−k:i+k)\\mathbf{h}_i = \\text{CNN}(\\mathbf{x}_{i-k : i+k}) hi​=CNN(xi−k:i+k​) 其中，xi−k:i+k\\mathbf{x}_{i-k:i+k}xi−k:i+k​ 是 token xix_ixi​ 的 2k2k2k 邻域内 tokens 嵌入向量的拼接矩阵，2k2k2k 是卷积核的尺寸。不难看出，每层的卷积操作只能捕捉一个有限的邻域内的信息，长程信息只能通过多层卷积间接获得 Recurrent Language Encoder RNN 是在 [[Attention-is-all-you-need|Transformer]] 网络之前 NLP 的主流网络结构，由于 RNN 存在的梯度消失/爆炸问题，通常使用的计算单元是其变体 LSTM([@hochreiter1997]) 和 GRU([@chung2014]). 其计算过程为： hi=RNN(hi−1,xi)\\mathbf{h}_i = \\text{RNN}(\\mathbf{h}_{i-1}, \\mathbf{x}_i) hi​=RNN(hi−1​,xi​) 可以看出，对于输入序列, RNN 必须逐 token 的顺序计算：第 iii 个 token 的表征计算依赖于第 i−1i - 1i−1 个 token 的表征, 因此无法有效并行。除此之外，RNN 只能建模单向(L2R)依赖，除非同时对逆序的输入序列再使用一次 RNN 计算(e.g., [@Peters2018]). Self-Attention Lanugage Encoder Self-Attention mechanism 可以简述为：在进行序列表征学习(序列建模)时，让模型自动选择基于该序列 (Self) 的哪些部分进行计算，即： hi=∑j=1Lαijxj\\mathbf{h}_i=\\sum_{j=1}^L\\alpha_{ij}\\mathbf{x}_j hi​=j=1∑L​αij​xj​ 其中的 xi\\mathbf{x}_ixi​ 是嵌入向量或模型上一层的表征，hi\\mathbf{h}_ihi​ 可以认为是相对于 x\\mathbf{x}x 的上下文表征，αij\\alpha_{ij}αij​ 则是 attention 参数，被认为建模了 token iii 和 jjj 之间的关系。 [Attention-is-all-you-need|Transformer] Layer 由两个子层：Self-Attention Sublayer 和 Feed-Forward Sublayer 组成，每个子层还使用了 Residual Connection ([@He2016])和 [[LayerNormalization|Layer Normalization]] ([@Ba2016]). Attention Layer Attention Layer 的核心在于How to attention: 即如何计算上文说到的 αij\\alpha_{ij}αij​. Transformer 使用 Query-Key-Value(Q, K, V) 模型实现 αij\\alpha_{ij}αij​ 的计算。 下面首先介绍 Scaled dot-product attention 的计算过程，然后解释其为什么有效，最后介绍 transformer 的 attention sublayer 中使用的 multi-head attention 的计算过程。 Scaled dot-product attention 首先计算 Q,K,V\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}Q,K,V： Q,K,V=XWQ,XWK,XWV\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} = \\mathbf{X}\\mathbf{W}^Q, \\mathbf{X}\\mathbf{W}^K, \\mathbf{X}\\mathbf{W}^V Q,K,V=XWQ,XWK,XWV X=concat(x1,x2,...,xL)\\mathbf{X} = \\text{concat}(\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_L)X=concat(x1​,x2​,...,xL​) 是输入文本嵌入向量的拼接，WQ,WK,WV\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^VWQ,WK,WV 是投影矩阵，也是模型参数 self-attention 通过以下过程计算表征： H=Attn(Q,K,V)=softmax(QK⊤dk)V\\mathbf{H}=\\text{Attn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{QK}^\\top}{\\sqrt{d_k}})\\mathbf{V} H=Attn(Q,K,V)=softmax(dk​​QK⊤​)V 其中 dkd_kdk​ 是模型维度，H∈RL×dk\\mathbf{H}\\in\\mathbb{R}^{L\\times d_k}H∈RL×dk​ 是输出表征的拼接矩阵，即：hi=H[i,:] for i∈[1,L]\\mathbf{h}_i = \\mathbf{H}[i, :]\\text{ for }i\\in[1, L]hi​=H[i,:] for i∈[1,L] 为什么有效？ 回忆上文所说的 attention 过程即选择序列的某些部分的过程，那么 Query-Key-Value 模型则是在模拟该选择过程。 首先明确一个概念：矩阵相乘即相似度计算。 然后考虑 attention 的计算过程的简化版： Attn(X)=softmax(XX⊤)X\\text{Attn}(\\mathbf{X}) = \\text{softmax}(\\mathbf{XX}^\\top)\\mathbf{X} Attn(X)=softmax(XX⊤)X 其中 X=[x1;x2;...;xL]\\mathbf{X} = [\\mathbf{x}_1; \\mathbf{x}_2; ...; \\mathbf{x}_L]X=[x1​;x2​;...;xL​], xi\\mathbf{x}_ixi​ 是 dkd_kdk​ 维行向量，代表 token iii 的嵌入向量。 分析其行为： XX⊤=[x1x2⋮xL]⋅[x1⊤,x2⊤,⋯ ,xL⊤]=[x1x1⊤x1x2⊤⋯x1xL⊤x2x1⊤x2x2⊤⋯x2xL⊤⋮⋮⋱⋮xLx1⊤xLx2⊤⋯xLxL⊤]∈RL×L\\begin{aligned} \\mathbf{XX}^\\top &amp;= \\left[\\begin{matrix} \\mathbf{x}_1\\\\ \\mathbf{x}_2\\\\ \\vdots\\\\ \\mathbf{x}_L \\end{matrix}\\right]\\cdot [\\mathbf{x}_1^\\top, \\mathbf{x}_2^\\top, \\cdots, \\mathbf{x}_L^\\top]\\\\ &amp;=\\left[\\begin{matrix} &amp;\\mathbf{x}_1\\mathbf{x}_1^\\top &amp;\\mathbf{x}_1\\mathbf{x}_2^\\top &amp;\\cdots &amp;\\mathbf{x}_1\\mathbf{x}_L^\\top\\\\ &amp;\\mathbf{x}_2\\mathbf{x}_1^\\top &amp;\\mathbf{x}_2\\mathbf{x}_2^\\top &amp;\\cdots &amp;\\mathbf{x}_2\\mathbf{x}_L^\\top\\\\ &amp;\\vdots &amp;\\vdots &amp;\\ddots &amp;\\vdots\\\\ &amp;\\mathbf{x}_L\\mathbf{x}_1^\\top &amp;\\mathbf{x}_L\\mathbf{x}_2^\\top &amp;\\cdots &amp;\\mathbf{x}_L\\mathbf{x}_L^\\top \\end{matrix}\\right]\\\\ &amp;\\in\\mathbb{R}^{L\\times L} \\end{aligned}XX⊤​=⎣⎢⎢⎢⎢⎡​x1​x2​⋮xL​​⎦⎥⎥⎥⎥⎤​⋅[x1⊤​,x2⊤​,⋯,xL⊤​]=⎣⎢⎢⎢⎢⎡​​x1​x1⊤​x2​x1⊤​⋮xL​x1⊤​​x1​x2⊤​x2​x2⊤​⋮xL​x2⊤​​⋯⋯⋱⋯​x1​xL⊤​x2​xL⊤​⋮xL​xL⊤​​⎦⎥⎥⎥⎥⎤​∈RL×L​ 显然, xi⋅xj⊤\\mathbf{x}_i\\cdot\\mathbf{x}_j^\\topxi​⋅xj⊤​ 的结果表征的是向量 xi\\mathbf{x}_ixi​ 和 xj\\mathbf{x}_jxj​ 之间的相似度。于是 XXij⊤\\mathbf{XX}^\\top_{ij}XXij⊤​ 具备了 αij\\alpha_{ij}αij​ 的作用：描述了第 i,ji,ji,j 个 token 之间的关系 [[Softmax]] 函数可以简单理解为一种归一化手段，它将上述向量内积的结果归一化到 (0,1)(0, 1)(0,1) 的范围内，且同一行中所有值的和为 111. 这样，上述过程中 softmax(XX⊤)\\text{softmax}(\\mathbf{XX}^\\top)softmax(XX⊤) 的结果可以认为是一个归一化的加权矩阵 最后一步的意义是显然的：根据前面计算的加权矩阵为输入序列的嵌入计算加权和，作为输出表征, 即 hi=∑j=1Lαijxj\\mathbf{h}_i = \\sum_{j=1}^L\\alpha_{ij}\\mathbf{x}_jhi​=∑j=1L​αij​xj​ 的过程. 下面是一个模拟该过程的图示： 矩阵相乘计算相似度分数： softmax 归一化: 根据归一化的注意力分数计算表征: 下面解释原始计算过程中的细节问题。对于原始的计算过程，其与简化版的区别在于： 输入表征经过线性投影后参与计算，而不是直接计算 存在一个缩放系数 dk\\sqrt{d_k}dk​​ 下面解释其原因： WQ,WK,WV\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^VWQ,WK,WV三个矩阵都是与输入相乘而起作用，其本质是对输入表征进行线性变换，主要的信息仍然是 X\\mathbf{X}X，其意义在于 提升模型的拟合能力。三个线性变换相当于线性层，属于神经网络的一种。 如果不使用这三个线性变换，XX⊤\\mathbf{XX}^\\topXX⊤ 的结果中，XXii⊤\\mathbf{XX}^\\top_{ii}XXii⊤​ 的值，即每个 token 对自己的注意力将会大大超过对其他 token 的注意力，经过 [[Softmax|Softmax]] 之后其他 token 的 attention score 会被严重挤压 缩放系数 dk\\sqrt{d_k}dk​​ 对矩阵的内积进行缩放 如果不进行缩放，在 dkd_kdk​ 即向量维度很大时向量之间的内积也会很大，这样经过 [[Softmax|Softmax]] 之后的梯度会非常小[1]。 如上就是 self-attention 中 scaled dot-product attention 的主要内容，实际上 transformer 模型中使用的是它的简单复合：Multi-Head Attention Multi-Head Attention 在 Scaled dot-product attention 中，输入表征直接被投影到 dkd_kdk​ 维度的 Query, Key, Value 向量，然后进行 self-attention 的加权平均过程。[@Vaswani2017a] 认为这个过程抑制了模型从多个方面提取文本序列的特征，于是提出进一步在多个子空间 (Multi-Head) 中分别进行 self-attention 计算。 具体地说，就是将 Q,K,V\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}Q,K,V 三个矩阵进一步投影到 hhh 个子空间, 然后进行 self-attention 计算： headi=Attn(QWiQ,KWiK,VWiV)\\text{head}_i = \\text{Attn}(\\mathbf{QW}_i^Q, \\mathbf{KW}_i^K, \\mathbf{VW}_i^V) headi​=Attn(QWiQ​,KWiK​,VWiV​) 其中 i∈[1,h]i\\in[1,h]i∈[1,h], WiQ,WiK∈Rdmodel×dk\\mathbf{W}_i^Q, \\mathbf{W}_i^K\\in\\mathbb{R}^{d_\\text{model}\\times d_k}WiQ​,WiK​∈Rdmodel​×dk​, WiV∈Rdmodel×dv\\mathbf{W}_i^V\\in\\mathbb{R}^{d_\\text{model}\\times d_v}WiV​∈Rdmodel​×dv​ Multi-Head Attention 中 dkd_kdk​, dvd_vdv​ 是一个比 dmodeld_\\text{model}dmodel​ 小的值，在 Attention is all you need([@Vaswani2017a])中, 作者取 h=8,dk=dv=dmodel/hh=8, d_k=d_v=d_\\text{model}/hh=8,dk​=dv​=dmodel​/h 在 hhh 个子空间进行 self-attention 计算将对每个词获得 hhh 个 dvd_vdv​ 维度的表征，Multi-Head Attention 将它们拼接起来并投影回 dmodeld_\\text{model}dmodel​ 维度。 MultiHead(Q,K,V)=concat(head1,...,headh)WO\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{concat}(\\text{head}_1, ..., \\text{head}_h)\\mathbf{W}_O MultiHead(Q,K,V)=concat(head1​,...,headh​)WO​ 其中 WO∈Rhdv×dmodel\\mathbf{W}_O\\in\\mathbb{R}^{hd_v\\times d_\\text{model}}WO​∈Rhdv​×dmodel​ 是投影矩阵 至此就是 Transformer 模型中关于 self-attention sublayer 的全部内容。 Feed-Forward Layer Feed-Forward sublayer 是一个全连接层，由两个线性层和 [[ReLU|ReLU]] 激活函数组成，它对所有 token 的表征分别进行同等的变换: FFN(x)=ReLU(xW1+b1)W2+b2\\text{FFN}(\\mathbf{x}) = \\text{ReLU}(\\mathbf{xW}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2 FFN(x)=ReLU(xW1​+b1​)W2​+b2​ 虽然 self-attention 是 transformer 成功的关键，但是 FFN 层的参数实际上是 self-attention 层的两倍，以 Transformer 模型为例，[@Vaswani2017a] 使用的参数是 dmodel=512d_\\text{model}=512dmodel​=512, 而 FFN 层的隐藏层维度是 dff=2048d_{ff}=2048dff​=2048。一些工作认为预训练的 transformer 模型中 FFN 层存储着与下游任务相关的信息, 而 self-attention 层中存储的则是文本之间如何进行有效交互的信息 ([@Geva2021], [@He2022]). Residual Connection and Layer Normalization Residual Connection ([@He2016]) 和 Layer Normalization ([@Ba2016]) 实际上都是为解决深度模型训练困难的实践性问题提出的，并无理论解释。对于 Residual Connection，自 ResNet([@He2016]) 以来，就一直是深度模型必备的部分，它主要解决了在过深的模型中训练时梯度(信息)消失的问题。 对于 Layer Normalization，它是针对 Batch Normalization ([@ioffe2015]) 提出的，而 Normalization 操作简而言之就是将神经网络中每层的输出(神经元激活分数)的分布恢复为均值为 000, 方差为 111 的正态分布，解决 [[Interval-Covariate-Shift|Interval Covariate Shift]] 问题。区别在于，BatchNormal 操作是对每个隐藏单元在 batch 内归一化，达到每个隐藏单元在同一 batch 内均匀分布的效果, 需要在一个有足够样本数量的 batch 中才能进行；而 LayerNormal 操作是对同一层的所有隐藏单元进行归一化，达到同一个样本的所有隐藏单元均匀分布的效果, 可以对单个样本进行。而主流的 NLP 模型因序列长度不确定，无法稳定获得足够的统计样本只能使用 Layer Normalization 总而言之，结合了 Residual Connection 和 Layer Normalization 的 transformer 层计算过程为： Hattn=LayerNorm(MultiHead(X)+X)Hffn=LayerNorm(FFN(Hattn)+Hattn)\\begin{aligned} \\mathbf{H}_\\text{attn} &amp;= \\text{LayerNorm}(\\text{MultiHead}(\\mathbf{X}) + \\mathbf{X})\\\\ \\mathbf{H}_\\text{ffn} &amp;= \\text{LayerNorm}(\\text{FFN}(\\mathbf{H}_\\text{attn}) + \\mathbf{H}_\\text{attn}) \\end{aligned}Hattn​Hffn​​=LayerNorm(MultiHead(X)+X)=LayerNorm(FFN(Hattn​)+Hattn​)​ Hffn\\mathbf{H}_\\text{ffn}Hffn​ 即最终的输出表征。 Why Self-Attention 从三个方面考虑 Self-Attention 替代 CNN 和 RNN 的益处： 每层总的计算复杂度 可以并行化的计算比例 建模长距离依赖时需要的计算路径长度 其中 3 在 NLP 以及其他序列建模任务中非常关键，一般来说，建模长距离依赖需要在网络中经过的计算路径越多，信息损失越严重，也就越难建模长距离依赖。 [@Vaswani2017a] 总结的上述三点的对比如下： PLMs NLP 任务中，优质的数据标注高度依赖领域专家知识，直接在任务的标注数据上进行监督学习通常很困难，而且无法迁移到不同领域。预训练则是一种自监督学习，利用大量存在的原始语料进行通用的语言建模。在足够大规模的语料库上预训练过的语言表征通常被认为有利于下游任务的性能([@Radford2018])，同时能够提升深度神经网络的泛化性([@erhan2010])。 较早的预训练语言模型如 word2vec[@mikolov2013], GloVe[@Pennington2014] 等词嵌入模型在语料库上将词映射到向量空间，训练后的词嵌入可以用在下游任务上以提高性能。这些静态的词嵌入相当于上下文无关的词表征，无法表示词的多义性。而多义性是自然语言中常见且重要的特征之一，因此后来的预训练语言模型主要关注于上下文相关的语言表征。 [@Peters2018] 提出的 ELMo 使用双向 LSTM 网络计算上下文表征，ELMo 生成的表征作为特征在下游任务模型中使用，相当于仅作为文本特征提取器嵌入到下游任务模型。随着 transformer 模型和预训练-微调模式的出现，仅作为特征提取器的预训练模型很快消失，ELMo 基本成为这类预训练模型的绝唱。 结构 在 transformer 时代，PLM 根据模型架构可以分为三种： Encoder based([BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding|BERT], [RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach|RoBERTa], [XLNet|XLNet].) Decoder based(GPT-1([@Radford2018]), GPT-2([@Radford2019]), GPT-3([@Brown2020]).) Encoder-Decoder based([BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension|BART], MASS([@Song2019]).) Encoder based Model Encoder 模型使用多层 transformer 对输入序列进行编码，生成动态的上下文语言表征。与上面讲到的 Self-Attention Encoding 过程一致 Decoder based Model Decoder 模型与 Encoder 只有轻微不同：Encoder 模型在计算 token 表征时在整个输入序列的上下文范围内进行注意力计算，而 Decoder 模型只在当前 token 的上文进行注意力过程，即 Autoregressive 过程。 Encoder-Decoder based Model Encoder-Decoder 模型由一个 Encoder 和一个 Decoder 组成，首先利用 Encoder 为输入文本生成表征，然后在 Decoder 中对该表征进行注意力计算。与 Decoder 模型相似，Encoder-Decoder 模型中的 Decoder 也会对当前 token 的上文进行注意力计算。即：Decoder 同时以输入文本的表征和生成结果的上文为条件进行生成，这一模式与翻译，摘要生成等 seq2seq 任务十分契合。 预训练目标 语言模型的训练目标基本可以简单归为以某种方式计算文本 xxx 的概率 P(x)P(x)P(x)。在 PLM 中主要有三种预训练目标，基本与上述三种模型结构相对应。 Autoencoding Modeling Autoencoding Modeling 常用于 Encoder based Model，如 BERT([@Devlin2019]) 中使用的 MLM。它的过程可以简述为：首先向原始文本中插入噪音，将其变为受损的文本，然后训练模型恢复受损的部分，训练的损失仅根据受损部分的文本计算，损失函数可以表示为： \\mathbfcal{L}_\\text{AE}(\\hat{\\mathbf{X}})=\\sum_{x_i\\in m(\\mathbf{X})}\\log P(x_i|\\mathbf{X}\\backslash m(\\mathbf{X})) 其中 X\\mathbf{X}X 是输入序列，X^\\hat{\\mathbf{X}}X^ 是原始文本序列经过噪音函数后的文本序列 X^=fnoise(X)\\hat{\\mathbf{X}} = f_\\text{noise}(\\mathbf{X})X^=fnoise​(X)，m(X)m(\\mathbf{X})m(X) 是输入序列中受损的部分。 Autoregressive Modeling Autoregressive Modeling 有时也称 Standard Language Modeling(SLM)，常在 Decoder based Model 中使用，如 GPT 系列模型就是 Autoregressive 模型。Autoregressive 模型是一种单向生成模型，即它根据上文信息训练模型生成下一个词，训练损失为： LAR(X)=∑i=1∣X∣log⁡P(xi∣x1,x2,...,xi−1)\\mathcal{L}_\\text{AR}(\\mathbf{X}) = \\sum_{i=1}^{|\\mathbf{X}|}\\log P(x_i|x_1, x_2, ..., x_{i-1}) LAR​(X)=i=1∑∣X∣​logP(xi​∣x1​,x2​,...,xi−1​) Sequence-2-Sequence Modeling seq2seq 常用于 Encoder-Decoder based Model，它的过程是 Autoencoding 和 Autoregressive 过程的结合：首先向输入文本加入噪音，然后训练模型基于受损的文本使用编码器和解码器恢复出原始文本，其训练损失是恢复文本与原始文本之间的负对数似然： LSS=∑i=1∣X∣log⁡P(xi∣X^,x1,x2,...,xi−1)\\mathcal{L}_{SS} = \\sum_{i=1}^{|\\mathbf{X}|}\\log P(x_i|\\hat{\\mathbf{X}}, x_1, x_2, ..., x_{i-1}) LSS​=i=1∑∣X∣​logP(xi​∣X^,x1​,x2​,...,xi−1​) 一些噪音函数([@Lewis2019])： Token Masking: 随机采样一些 tokens 并使用 [MASK] 替换 Token Deletion: 随机删去一些 tokens ，与 Token Masking 相比，这种方法迫使模型预测被删除的位置 Text Infilling: 随机采样一些文本片段，并用遮罩 [MASK] 替换。对于长度为 0 的文本片段，相当于插入了 [MASK] 。这种噪声迫使模型预测被替换的文本片段的长度 Sentence Permutation: 将文档按照句号分割成不同的句子，然后随机排列句子的顺序。这种噪声迫使模型学习同一文档中句子的顺序 Document Rotation: 随机选择一个 token ，然后将文本旋转到以这个 token 为开头的状态。这种噪声训练模型识别文本开头的能力 例如对于均值为 000, 方差为 111 的 dkd_kdk​ 维向量 q,k\\mathbf{q}, \\mathbf{k}q,k, 他们内积结果 q⋅k⊤\\mathbf{q}\\cdot\\mathbf{k}^\\topq⋅k⊤ 的均值为 000, 方差为 dkd_kdk​. 当 dkd_kdk​ 很大时, 意味着分布 q⋅k⊤\\mathbf{q}\\cdot\\mathbf{k}^\\topq⋅k⊤ 的值集中在绝对值大的区域，即 softmax(q⋅k⊤)\\text{softmax}(\\mathbf{q}\\cdot\\mathbf{k}^\\top)softmax(q⋅k⊤) 的大部分值之间的梯度很小。因此需要以 k\\sqrt{k}k​ 进行缩放，使内积结果的方差为 111 ↩︎","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"}]},{"title":"Transformer Feed-Forward Layers are Key-Value Memories","slug":"Transformer-Feed-Forward-Layers-are-Key-Value-Memories","date":"2022-04-03T06:59:27.000Z","updated":"2023-06-21T06:51:28.928Z","comments":true,"path":"Transformer-Feed-Forward-Layers-are-Key-Value-Memories/","link":"","permalink":"https://blog.zhuwenq.cc/Transformer-Feed-Forward-Layers-are-Key-Value-Memories/","excerpt":"Motivation Transformer 模型中, FFN 层的参数占 3/2，但是其在网络中的作用还没有很好的被研究和理解 作者提出 FFN 层相当于神经记忆系统，以第一个矩阵为 key， 第二个矩阵为 value 记录了键值对信息。其中的 key 指人类可解释的文本特征(表层的文本结构特征和深层的文本语义特征), value 则可以诱导成在词典空间中的概率分布。","text":"Motivation Transformer 模型中, FFN 层的参数占 3/2，但是其在网络中的作用还没有很好的被研究和理解 作者提出 FFN 层相当于神经记忆系统，以第一个矩阵为 key， 第二个矩阵为 value 记录了键值对信息。其中的 key 指人类可解释的文本特征(表层的文本结构特征和深层的文本语义特征), value 则可以诱导成在词典空间中的概率分布。 Introduction FFN 层的计算过程和 Key-value Memories 的计算过程相似： FFN 层中的第一个矩阵是神经键值对记忆的 key 矩阵，第二个矩阵是 value 矩阵。作者在此理论的基础上进行了实验，分析了 FFN 层作为记忆的观点下其究竟记忆了什么信息 实验发现 FFN 中的 key 通常与人类可解释的文本模式相关：即文本结构或语义主题。而 value 可以诱导成词典空间上的概率分布，且该分布与 key 中的文本模式的下一个词有关 作者还发现：Transformer 模型中每一层的 FFN 层都有数百个激活的 memories, 即数百个在词典空间上的概率分布，但是 transformer 层的最终输出的分布与激活的记忆的分布均不相同。这暗示 transformer 层之间的 残差信息 起到主要的预测作用，FFN 层的输出是该残差信息的调整。 一些结论 FFN 层可以看作未经归一化的 Key-Value Memories FFN 中的 key 可以看作是对输入的模式捕捉：部分 key 向量与输入 x 的乘积较大，说明输入 x 符合该 key 中存储的模式 key 中存储的模式是人类可解释的：实验中发现被激活的模式可分为文本结构或语义注意上的模式，均为人类可解释的 浅层的 key 主要捕捉浅层的特征：实验中发现浅层的 key 中主要捕捉的是文本表层特征，深层的 key 则主要捕捉语义特征 values 可以诱导成词典上的概率分布 FFN 层存储的信息主要是关于根据输入直接预测下一词概率分布的 每个 记忆单元的概率分布与该层的最终输出通常都不相同，实验发现每层的最终输出通常偏向残差的信息，而不是 FFN 的信息。即使当最终输出与残差不同时(即残差被 FFN 改变), 最终输出也很少会偏向 ffn 的输出。这暗示 ffn 层实际扮演着对 残差信息 的否决者作用，即它智能否决残差中的 top prediction，使之偏向另一个 candidate，而不是偏向 ffn 自己的 top prediction","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Interpretability","slug":"Interpretability","permalink":"https://blog.zhuwenq.cc/tags/Interpretability/"},{"name":"FFN","slug":"FFN","permalink":"https://blog.zhuwenq.cc/tags/FFN/"},{"name":"Nueral Memory","slug":"Nueral-Memory","permalink":"https://blog.zhuwenq.cc/tags/Nueral-Memory/"}]},{"title":"Towards a Unified View of Parameter Efficient Transfer Learning","slug":"Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning","date":"2022-03-30T07:52:19.000Z","updated":"2023-06-21T06:51:28.924Z","comments":true,"path":"Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning/","link":"","permalink":"https://blog.zhuwenq.cc/Towards-a-Unified-View-of-Parameter-Efficient-Transfer-Learning/","excerpt":"作者认为：现有的 parameter-efficient tuning 方法虽然有效，但是对这些方法中真正有效的设计以及它们之间的联系很少被研究和理解。","text":"作者认为：现有的 parameter-efficient tuning 方法虽然有效，但是对这些方法中真正有效的设计以及它们之间的联系很少被研究和理解。 作者基于此研究了 adapter, prefix-tuning 和 LoRA 等方法的设计重点和它们之间的联系，同时研究了这些方法之间的交换或组合。 作者认为：parameter-efficient tuning 方法保持基模型参数不变，其本质相当于以某种方式修饰基模型的表征。主要的设计在于： 用来计算表征修饰的方法(函数) 进行修饰的位置 修饰与原始表征之间结合的方法 parameter-efficient tuning 计算过程 Recall Transformer transformer 层的计算过程即一个 multi-head attention 接一个 FFN 层 attn 层计算过程为: 首先输入映射为注意力矩阵 Q∈Rn×dk\\mathbf{Q}\\in\\mathbb{R}^{n\\times d_k}Q∈Rn×dk​, K∈Rm×dk\\mathbf{K}\\in\\mathbb{R}^{m\\times d_k}K∈Rm×dk​, V∈Rm×dv\\mathbf{V}\\in\\mathbb{R}^{m\\times d_v}V∈Rm×dv​ Attn(Q,K,V)=softmax(QKTdk)⋅V\\text{Attn}(\\mathbf{Q},\\mathbf{K},\\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}) \\cdot \\mathbf{V}Attn(Q,K,V)=softmax(dk​​QKT​)⋅V 在 multi-head attn 中，实际是在 NhN_hNh​ 个 heads 中并行进行上述 attn 计算。第 iii 个 head 的输入由注意力参数矩阵 Wqi,Wki,Wvi∈Rd×dh\\mathbf{W}_q^i, \\mathbf{W}_k^i, \\mathbf{W}_v^i\\in\\mathbb{R}^{d\\times d_h}Wqi​,Wki​,Wvi​∈Rd×dh​ 分别与输入相乘得到，即： headi=Attn(xWqi,CWki,CWvi)\\text{head}_i = \\text{Attn}(\\mathbf{xW}_q^i, \\mathbf{CW}_k^i, \\mathbf{CW}_v^i)headi​=Attn(xWqi​,CWki​,CWvi​) 其中，x\\mathbf{x}x 是 query vector, C\\mathbf{C}C 是整个输入序列的嵌入。 multi-head attn 将每个 head 的计算结果拼接，并投影到模型维度： MHA(C,x)=Concati=1Nh(headi)Wo\\text{MHA}(\\mathbf{C}, \\mathbf{x}) = \\text{Concat}_{i=1}^{N_h}(\\text{head}_i)\\mathbf{W}_oMHA(C,x)=Concati=1Nh​​(headi​)Wo​ 其中 Wo∈Rd×d\\mathbf{W}_o\\in\\mathbb{R}^{d\\times d}Wo​∈Rd×d, dhd_hdh​ 通常是 dNh\\frac{d}{N_h}Nh​d​, 即：每个 head 通常是在低维空间上进行计算。 FFN 层的计算过程为: FFN(x)=ReLU(xW1+b1)W2+b2\\text{FFN}(\\mathbf{x}) = \\text{ReLU}(\\mathbf{x}W_1 + b_1)W_2 + b_2FFN(x)=ReLU(xW1​+b1​)W2​+b2​ 其中 W1∈Rd×dm\\mathbf{W}_1\\in\\mathbb{R}^{d\\times d_m}W1​∈Rd×dm​, W2∈Rdm×d\\mathbf{W}_2\\in\\mathbb{R}^{d_m\\times d}W2​∈Rdm​×d, b1∈Rdm\\mathbf{b}_1\\in\\mathbb{R}^{d_m}b1​∈Rdm​, b2∈Rd\\mathbf{b}_2\\in\\mathbb{R}^{d}b2​∈Rd. transformer 中通常在 FFN 层使用较大的 dmd_mdm​, 如 dm=4dd_m = 4ddm​=4d。 Adapters(串行式) adapter 通常在 transformer 层之间插入较小的 adapter layer, adapter layer 通常由一个下投影层 Wdown∈Rd×r\\mathbf{W}_\\text{down}\\in\\mathbb{R}^{d\\times r}Wdown​∈Rd×r 将输入表征 h\\mathbf{h}h 投影到低维空间，然后通过一个非线性层 f(⋅)f(\\cdot)f(⋅), 最后通过一个上投影层 Wup∈Rr×d\\mathbf{W}_\\text{up}\\in\\mathbb{R}^{r\\times d}Wup​∈Rr×d 将表征恢复到高维空间。adapters 层通常会应用残差连接，以保证初始状态下的性能。 h←h+f(hWdown)Wup\\mathbf{h}\\leftarrow\\mathbf{h}+f(\\mathbf{hW}_\\text{down})\\mathbf{W}_\\text{up} h←h+f(hWdown​)Wup​ Prefix-tuning prefix-tuning 方法即在每个 transformer 层的 multi-head attn 计算中，向每个 head 的 key 和 value 矩阵的前部添加 lll 个可调的向量，即 prefix 向量。 具体来说，prefix-tuning 将两个矩阵: Pk,Pv∈Rl×d\\mathbf{P}_k, \\mathbf{P}_v\\in\\mathbb{R}^{l\\times d}Pk​,Pv​∈Rl×d 分别与 key, value 矩阵 K,V\\mathbf{K}, \\mathbf{V}K,V 拼接，则拼接后的 attn 计算过程为： headi=Attn(xWqi,concat(Pki,CWki),concat(Pvi,CWvi))\\text{head}_i = \\text{Attn}(\\mathbf{xW}_q^i, \\text{concat}(\\mathbf{P}_k^i, \\mathbf{CW}_k^i), \\text{concat}(\\mathbf{P}_v^i, \\mathbf{CW}_v^i)) headi​=Attn(xWqi​,concat(Pki​,CWki​),concat(Pvi​,CWvi​)) 其中的 Pki,Pvi∈Rl×d/Nh\\mathbf{P}_k^i, \\mathbf{P}_v^i\\in\\mathbb{R}^{l\\times d/N_h}Pki​,Pvi​∈Rl×d/Nh​, 即 Pk,Pv\\mathbf{P}_k, \\mathbf{P}_vPk​,Pv​ 分解到 NhN_hNh​ 个 heads 中第 iii 个 head 的向量。 LoRA LoRA 通过向 transformer layer 中插入低维矩阵，以低秩矩阵相乘的方式近似到高维矩阵，实现对基模型参数的修饰。 具体的说，对于一个高维基模型参数 W∈Rd×k\\mathbf{W}\\in\\mathbb{R}^{d\\times k}W∈Rd×k, 对其进行参数修饰可以表示为：W+ΔW\\mathbf{W} + \\Delta\\mathbf{W}W+ΔW. LoRA 则通过两个低秩矩阵近似地得到 ΔW\\Delta\\mathbf{W}ΔW，即：ΔW=WdownWup\\Delta\\mathbf{W} = \\mathbf{W}_\\text{down}\\mathbf{W}_\\text{up}ΔW=Wdown​Wup​, 其中 Wdown∈Rd×r\\mathbf{W}_\\text{down}\\in\\mathbb{R}^{d\\times r}Wdown​∈Rd×r, Wup∈Rr×k\\mathbf{W}_\\text{up}\\in\\mathbb{R}^{r\\times k}Wup​∈Rr×k。 LoRA 将上述参数修饰方法应用到 query 和 value 的注意力投影参数 Wq\\mathbf{W}_qWq​ 和 Wv\\mathbf{W}_vWv​ 上。LoRA 通过对参数的修饰实现对表征的修饰： h←h+s⋅xWdownWup\\mathbf{h}\\leftarrow\\mathbf{h}+s\\cdot\\mathbf{x}\\mathbf{W}_\\text{down}\\mathbf{W}_\\text{up} h←h+s⋅xWdown​Wup​ 其中 s≥1s\\geq 1s≥1 是可调的缩放超参数。 区别和联系 Prefix-tuning 与 Adapters head=Attn(Q,concat(Pk,K),concat(Pv,V))=softmax(Qconcat(Pk,K)T)concat(Pv,V)=(1−λ(x))softmax(QKT)V+λ(x)softmax(QPkT)Pv=(1−λ(x))Attn(Q,K,V)+λ(x)Attn(Q,Pk,Pv)\\begin{aligned} \\text{head} &amp;= \\text{Attn}(\\mathbf{Q}, \\text{concat}(\\mathbf{P}_k, \\mathbf{K}), \\text{concat}(\\mathbf{P}_v, \\mathbf{V}))\\\\ &amp;= \\text{softmax}(\\mathbf{Q}\\text{concat}(\\mathbf{P}_k, \\mathbf{K})^T)\\text{concat}(\\mathbf{P}_v, \\mathbf{V})\\\\ &amp;= (1-\\lambda(\\mathbf{x}))\\text{softmax}(\\mathbf{Q}\\mathbf{K}^T)\\mathbf{V}+\\lambda(\\mathbf{x})\\text{softmax}(\\mathbf{Q}\\mathbf{P}_k^T)\\mathbf{P}_v\\\\ &amp;= (1-\\lambda(\\mathbf{x}))\\text{Attn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})+\\lambda(\\mathbf{x})\\text{Attn}(\\mathbf{Q}, \\mathbf{P}_k, \\mathbf{P}_v)\\\\ \\end{aligned}head​=Attn(Q,concat(Pk​,K),concat(Pv​,V))=softmax(Qconcat(Pk​,K)T)concat(Pv​,V)=(1−λ(x))softmax(QKT)V+λ(x)softmax(QPkT​)Pv​=(1−λ(x))Attn(Q,K,V)+λ(x)Attn(Q,Pk​,Pv​)​ 其中的 Q=xWq\\mathbf{Q} = \\mathbf{xW}_qQ=xWq​, K=CWk\\mathbf{K} = \\mathbf{CW}_kK=CWk​, V=CWv\\mathbf{V} = \\mathbf{CW}_vV=CWv​, 因此又可以写为： head=Attn(xWq,concat(Pk,CWk),concat(Pv,CWv))=softmax(xWqconcat(Pk,CWk)T)[PvCWvT]=(1−λ(x))softmax(xWqWkTCT)CWv+λ(x)softmax(xWqPkTPv)=(1−λ(x))Attn(xWq,CWk,CWv)⏟standard attention+λ(x)Attn(xWq,Pk,Pv)⏟independent ofC\\begin{aligned} \\text{head} &amp;= \\text{Attn}(\\mathbf{x}\\mathbf{W}_q, \\text{concat}(\\mathbf{P}_k, \\mathbf{CW}_k), \\text{concat}(\\mathbf{P}_v, \\mathbf{CW}_v)) \\\\ &amp;= \\text{softmax}(\\mathbf{xW}_q\\text{concat}(\\mathbf{P}_k, \\mathbf{CW}_k)^T) \\left[ \\begin{matrix} \\mathbf{P}_v \\\\ \\mathbf{CW}_v^T \\end{matrix} \\right]\\\\ &amp;= (1 - \\lambda(\\mathbf{x}))\\text{softmax}(\\mathbf{xW}_q\\mathbf{W}_k^T\\mathbf{C}^T)\\mathbf{CW}_v + \\lambda(\\mathbf{x})\\text{softmax}(\\mathbf{xW}_q\\mathbf{P}_k^T\\mathbf{P}_v)\\\\ &amp;= (1 - \\lambda(\\mathbf{x}))\\underbrace{\\text{Attn}(\\mathbf{xW}_q, \\mathbf{CW}_k, \\mathbf{CW}_v)}_\\text{standard attention} + \\lambda(\\mathbf{x}) \\underbrace{\\text{Attn}(\\mathbf{xW}_q, \\mathbf{P}_k, \\mathbf{P}_v)}_\\text{independent of} \\mathbf{C} \\end{aligned}head​=Attn(xWq​,concat(Pk​,CWk​),concat(Pv​,CWv​))=softmax(xWq​concat(Pk​,CWk​)T)[Pv​CWvT​​]=(1−λ(x))softmax(xWq​WkT​CT)CWv​+λ(x)softmax(xWq​PkT​Pv​)=(1−λ(x))standard attentionAttn(xWq​,CWk​,CWv​)​​+λ(x)independent ofAttn(xWq​,Pk​,Pv​)​​C​ 其中 λ(x)=Σiexp⁡(xWqPkT)iΣiexp⁡(xWqPkT)i+Σjexp⁡(xWqWkTCT)j\\lambda(\\mathbf{x}) = \\frac{\\Sigma_i\\exp(\\mathbf{xW}_q\\mathbf{P}_k^T)_i}{\\Sigma_i\\exp(\\mathbf{xW}_q\\mathbf{P}_k^T)_i + \\Sigma_j\\exp(\\mathbf{xW}_q\\mathbf{W}_k^T\\mathbf{C}^T)_j}λ(x)=Σi​exp(xWq​PkT​)i​+Σj​exp(xWq​WkT​CT)j​Σi​exp(xWq​PkT​)i​​. note: softmax(x)=exp⁡(x)∑iexp⁡(x)\\text{softmax}(\\mathbf{x}) = \\frac{\\exp(\\mathbf{x})}{\\sum_i\\exp(\\mathbf{x})}softmax(x)=∑i​exp(x)exp(x)​ note: x∈Rd,Wq,Wk,Wv∈Rd×dh,C∈Rm×d,Pk,Pv∈Rl×dh\\mathbf{x}\\in\\mathbb{R}^d, \\mathbf{W}_q, \\mathbf{W}_k, \\mathbf{W}_v\\in\\mathbb{R}^{d\\times d_h}, \\mathbf{C}\\in\\mathbb{R}^{m\\times d}, \\mathbf{P}_k, \\mathbf{P}_v\\in\\mathbb{R}^{l\\times d_h}x∈Rd,Wq​,Wk​,Wv​∈Rd×dh​,C∈Rm×d,Pk​,Pv​∈Rl×dh​, hence: concat(Pk,CWk),concat(Pv,CWv)∈R(l+m)×d\\text{concat}(\\mathbf{P}_k, \\mathbf{CW}_k), \\text{concat}(\\mathbf{P}_v, \\mathbf{CW}_v)\\in\\mathbb{R}^{(l+m)\\times d}concat(Pk​,CWk​),concat(Pv​,CWv​)∈R(l+m)×d 注意到上述 head 计算结果中前一项即正常的 attn ， 后一项为独立于输入 C\\mathbf{C}C 的逐 query x\\mathbf{x}x 的 attn。可看作对表征的修饰： h←(1−λ(x))h+λ(x)Δh\\mathbf{h}\\leftarrow(1-\\lambda(\\mathbf{x}))\\mathbf{h} + \\lambda(\\mathbf{x})\\Delta\\mathbf{h} h←(1−λ(x))h+λ(x)Δh 其中: Δh:=softmax(xWqPkT)Pv\\Delta\\mathbf{h} :=\\text{softmax}(\\mathbf{xW}_q\\mathbf{P}_k^T)\\mathbf{P}_vΔh:=softmax(xWq​PkT​)Pv​ 定义 W1=WqPkT\\mathbf{W}_1 = \\mathbf{W}_q\\mathbf{P}_k^TW1​=Wq​PkT​, W2=Pv\\mathbf{W}_2 = \\mathbf{P}_vW2​=Pv​, f=softmaxf = \\text{softmax}f=softmax, 则可以将上式重写为： h←(1−λ(x))h+λ(x)f(xW1)W2\\mathbf{h}\\leftarrow(1-\\lambda(\\mathbf{x}))\\mathbf{h} + \\lambda(\\mathbf{x})f(\\mathbf{xW}_1)\\mathbf{W}_2 h←(1−λ(x))h+λ(x)f(xW1​)W2​ 相同之处 prefix-tuning 计算过程与 adapter 的计算过程结构相似，W1∈Rdh×l\\mathbf{W}_1\\in\\mathbb{R}^{d_h\\times l}W1​∈Rdh​×l, W2∈l×dh\\mathbf{W}_2\\in\\mathbf{l\\times d_h}W2​∈l×dh​ 起到 adapter 中 Wdown\\mathbf{W}_\\text{down}Wdown​ 和 Wup\\mathbf{W}_\\text{up}Wup​ 的作用, lll 则和 adapter 中的 rrr 类似：是计算修饰向量 Δh\\Delta\\mathbf{h}Δh 时的秩的约束，即 Δh\\Delta\\mathbf{h}Δh 是至多 lll 个向量的线性组合 不同之处 在修饰项链结合上，adapter 没有类似 λ\\lambdaλ 的门控过程 prefix-tuning 使用 PLM layer 的输入 x\\mathbf{x}x 来计算 Δh\\Delta\\mathbf{h}Δh, adapter 使用 h\\mathbf{h}h 即 PLM layer 的输出 prefix-tuning 在 multi-head attn 的每个 head 内进行修饰过程，adapter 可以选择在 attn 或 FFN 的输出进行修饰 设计要点 综合三种方法的计算过程的异同，可以总结其设计要点如下： Functional Form: 即用来计算 Δh\\Delta\\mathbf{h}Δh 的函数. 三种方法都使用类似 proj_down→non_linear→proj_up\\text{proj\\_down}\\rightarrow\\text{non\\_linear}\\rightarrow\\text{proj\\_up}proj_down→non_linear→proj_up 的过程，LoRA 中没有 non_linear 过程 Modified Representation: 即使用什么 hidden state 作为输入来计算 Δh\\Delta\\mathbf{h}Δh Insertion Form: 即添加的模型如何插入到基模型中 Composition Function: 即 Δh\\Delta\\mathbf{h}Δh 如何与基模型的表征 h\\mathbf{h}h 组合，如 adapter 通过简单相加，prefix-tuning 通过门控相加, LoRA 通过缩放相加 作者基于这些设计要点，设计了三种他们的组合, 以研究哪个设计维度和设计方式最为有效： Parallel Adapter: 将 prefix-tuning 以 x\\mathbf{x}x 作为输入的并行性引入 adapter Multi-head Parallel Adapter: 将 prefix-tuning 作用在每个 head 的特点引入 adapter Scaled Parallel Adapter: 将 LoRA 的缩放相加组合方式引入 adapter Insertion Form: Sequential or Parallel 作者使用 parallel adapter 和 sequential adaper 实验，结果表明并行设计在所有 bench mark 上都优于串行设计 Where to Modify: Attention or FFN 作者使用并行 adapter 分别作用在 attention 和 FFN 的模型以及 prefix-tuning 和作用在 FFN 层上的 LoRA 进行实验，结果表明 FFN 上的修饰优于 attention 上的修饰。作者认为：不论使用何种修饰函数和组合方式，FFN 上的修饰可以更加有效地利用新添加的参数 作者对这一现象提出猜想：FFN 层学习到的是任务相关的文本模式特征，而 attention 学习到的是文本之间的交互模式(即进行有效自注意力计算的模式) 进一步地，作者减少添加的参数量 (0.1%) 发现在参数量小的情况下 attention 层的修饰更有效。 基于此实验结果，作者提出一条设计原则: 应该在 FFN 层的修饰上使用更大的参数量 (更大的维度或更深的层数) Composition Function 作者在 LoRA 上做消融实验，并于 scaled parallel adapter 比较，发现缩放相加优于直接相加，并且缩放系数的选择对模型性能有影响 作者最终结合所有被认为更优的设计要素: 基于 prefix-tuning (门控相加), 在 attention 层使用小维度的修饰层 l=30l=30l=30, 在 FFN 层使用大维度的修饰层 r=512r=512r=512, 设计出新的模型 MAM Adapter","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://blog.zhuwenq.cc/tags/Transfer-Learning/"},{"name":"Parameter-Efficient","slug":"Parameter-Efficient","permalink":"https://blog.zhuwenq.cc/tags/Parameter-Efficient/"}]},{"title":"Integrating Multimodal Information in Large Pretrained Transformers","slug":"Integrating-Multimodal-Information-in-Large-Pretrained-Transformers","date":"2022-03-23T09:20:49.000Z","updated":"2023-06-21T06:51:28.792Z","comments":true,"path":"Integrating-Multimodal-Information-in-Large-Pretrained-Transformers/","link":"","permalink":"https://blog.zhuwenq.cc/Integrating-Multimodal-Information-in-Large-Pretrained-Transformers/","excerpt":"作者基于一种观点：结合多模态信息的语言表征与不结合多模态信息的语言表征在表征空间中的位置不同，因此，可以将多模态信息视为表征空间中的 移位向量，与不结合多模态信息的语言表征向量相加可以得到向量空间中的最终位置。","text":"作者基于一种观点：结合多模态信息的语言表征与不结合多模态信息的语言表征在表征空间中的位置不同，因此，可以将多模态信息视为表征空间中的 移位向量，与不结合多模态信息的语言表征向量相加可以得到向量空间中的最终位置。 Multimodal Adaptation Gate(MAG) 作者基于上述观点，设计了多模态融合架构 MAG 结合 BERT 和 XLNet 在情感分类任务上进行了验证。 MAG 试图将视觉信息和音频信息融入语言表征。数据集中，作者首先使用机器翻译生成视频字幕，然后人工修正，作为语言信息。随后作者根据时间共现性提取出 &lt;图像，音频，字幕&gt; 的序列，其中每个三元组是同时出现的。 MAG 随后提取图像中的特征信息(任务的表情，手部动作等)和语音中的特征信息(语调等)，并编码为向量与语言信息融合。 记序列中第 iii 个元组的语言嵌入向量，语音嵌入向量和视觉嵌入向量为 (Zi,Ai,Vi)(Z_i, A_i, V_i)(Zi​,Ai​,Vi​), 作者首先将语言向量分别和其他模态向量拼接，分别计算出门控向量 givg_i^vgiv​ 和 giag_i^agia​: giv=R(Wgv[Zi;Vi]+bv)gia=R(Wga[Zi;Ai]+ba)\\begin{aligned} g_i^v &amp;= R(W_{gv}[Z_i; V_i] + b_v)\\\\ g_i^a &amp;= R(W_{ga}[Z_i; A_i] + b_a) \\end{aligned}giv​gia​​=R(Wgv​[Zi​;Vi​]+bv​)=R(Wga​[Zi​;Ai​]+ba​)​ 其中 RRR 是激活函数. 作者认为，上述门控网络可以计算出多模态信息与语言信息的相关系数。作者随后使用上述门控系数与多模态嵌入向量相乘获得多模态信息融合向量，即：表征空间中的位移向量： Hi=gia⋅(WaAi)+giv⋅(WvVi)+bHH_i = g_i^a\\cdot(W_aA_i) + g_i^v\\cdot(W_vV_i) + b_H Hi​=gia​⋅(Wa​Ai​)+giv​⋅(Wv​Vi​)+bH​ 随后，按照位移向量的观点，作者将多模态信息向量和语言向量相加获得多模态信息下的语言表征： Zˉi=Zi+αHi\\bar{Z}_i = Z_i + \\alpha H_i Zˉi​=Zi​+αHi​ 其中 α=min⁡(∥Zi∥2∥Hi∥2β,1)\\alpha = \\min(\\frac{\\|Z_i\\|_2}{\\|H_i\\|_2}\\beta, 1)α=min(∥Hi​∥2​∥Zi​∥2​​β,1) 上述 MAG 模型将多模态信息向量与语言表征向量做了融合，作者随后介绍了 MAG 与 BERT 的结合。 MAG-BERT 作者选择将 MAG 模型夹在 BERT 的某两层之间，不妨记作第 jjj 层和第 j+1j + 1j+1 层之间，则 MAG 的语言表征来自 BERT 第 jjj 层 transformer layer 的输出，且自 j+1j + 1j+1 层起，输入向量中融入了多模态信息： 在情感分类任务中，作者直接取序列开头的 [CLS][CLS][CLS] token 的表征作为整个序列的表征。 MAG-XLNet MAG-XLNet 的结构与 MAG-BERT 相同，不同之处来自 XLNet 本身：在 XLNet 中，[CLS][CLS][CLS] token 加在序列的尾部。 experiment 作者在 CMU-MOSI 数据集上进行了实验，该数据集是多模态情感分类数据集，数据来自 youtube 电影评论视频。 除此之外，作者试验了 MAG 可以放置的位置：从嵌入层之后第一层 transformers 之前到最后一层 transformer 之前，结果显示放在第一层 transformer 之后性能最好。这一结果是符合直觉的：嵌入层的语言表征没有上下文信息，而在太靠后的层加入多模态信息则无法对其进行有效的上下文融合。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Multimodal","slug":"Multimodal","permalink":"https://blog.zhuwenq.cc/tags/Multimodal/"},{"name":"Visual Knowledge","slug":"Visual-Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Visual-Knowledge/"},{"name":"Aoustic Knowledge","slug":"Aoustic-Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Aoustic-Knowledge/"}]},{"title":"莫里斯遍历","slug":"莫里斯遍历","date":"2022-03-20T08:47:29.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"莫里斯遍历/","link":"","permalink":"https://blog.zhuwenq.cc/%E8%8E%AB%E9%87%8C%E6%96%AF%E9%81%8D%E5%8E%86/","excerpt":"本文写于 2020 年 5 月 3 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 5 月 3 日，2022 年 3 月 20 日重新整理 莫里斯遍历 (Morris Traversal) 是一种时间复杂度为 O(n) ，空间复杂度为 O(1) ，且不改变树的结构的二叉树遍历算法。 莫里斯遍历 通常的二叉树遍历算法使用堆栈或者递归完成，它们的空间复杂度通常是 O(n) 的。 要使用 O(1) 的空间实现遍历，最主要的难点是如何在指针到达叶节点的时候回到父节点，在不能使用堆栈的情况下，莫里斯遍历使用线索二叉树 (Threaded Binary Tree) 的思想，莫里斯方法中不需要额外的空间来保存叶子节点的前驱 (predecessor) 节点和后继 (successor) 节点，只需要利用叶子节点的空闲左右子节点按照一定的顺序指向前驱或后继节点就可以。 首先看莫里斯方法的中序遍历，通过中序遍历可以推出其他顺序的遍历。 首先实现一个 LeetCode 风格的二叉树节点原型： 123456struct TreeNode&#123; int val; TreeNode* left; TreeNode* right; TreeNode(int _val):val(_val),left(NULL),right(NULL)&#123; &#125;&#125; 中序遍历 算法流程： 如果当前节点的左子节点为空，则访问当前节点并以当前节点的右子节点作为当前节点（与普通遍历无异） 如果当前节点的左子节点不为空，则在当前节点的左子树中寻找当前节点在中序遍历下的前驱节点 如果前驱节点的右子节点为空，则将它的右子节点设置为当前节点（设置线索）。更新当前节点为当前节点的左子节点 如果前驱节点的右子节点为当前节点，则将其重设为空（断开线索，恢复树的形状）。访问当前节点并更新当前节点为当前节点的右子节点。 重复以上两点直到当前节点为空。 线索设置和遍历过程图示: 代码过程如下： 123456789101112131415161718192021222324252627void visit(TreeNode *root);void inOrderTraversal(TreeNode *root)&#123; TreeNode *cur = root; TreeNode *prev = NULL; while (cur != NULL) &#123; if (cur-&gt;left == NULL) &#123; //最左侧 visit(cur); cur = cur-&gt;right; &#125; else &#123; prev = cur-&gt;left; while (prev-&gt;right != NULL &amp;&amp; prev-&gt;right != cur) //find the predecessor prev = prev-&gt;right; if (prev-&gt;right == NULL) &#123; prev-&gt;right = cur; cur = cur-&gt;left; &#125; else if (prev-&gt;right == cur) &#123; //recover prev-&gt;right = NULL; visit(cur); cur = cur-&gt;right; &#125; &#125; &#125;&#125; 算法的空间复杂度是 O(1) 的，时间复杂度主要体现在寻找前驱节点的操作上： 12while (prev != NULL &amp;&amp; prev-&gt;right != NULL) //find the predecessor prev = prev-&gt;right; 这段代码的时间复杂度直观上是 O(log n) 的，但是由于要对每个左子节点非空的节点都要做一次，因此总体上是 O(n) 的。 前序遍历 前序遍历与中序遍历总体上是相同的，只是访问节点的时机略有不同 算法流程： 如果当前节点的左子节点为空，则访问当前节点并以当前节点的右子节点作为当前节点（与普通遍历无异） 如果当前节点的左子节点不为空，则在当前节点的左子树中寻找当前节点在中序遍历下的前驱节点 如果前驱节点的右子节点为空，则将它的右子节点设置为当前节点（设置线索）之后访问当前节点。更新当前节点为当前节点的左子节点 如果前驱节点的右子节点为当前节点，则将其重设为空（断开线索，恢复树的形状）。更新当前节点为当前节点的右子节点。 重复以上两点直到当前节点为空。 代码过程： 12345678910111213141516171819202122232425void preOrderTraversal(TreeNode *root)&#123; TreeNode *cur = root; TreeNode *prev = NULL; while (cur != NULL) &#123; if (cur != NULL) &#123; visit(cur); cur = cur-&gt;right; &#125; else &#123; prev = cur-&gt;left; while (prev-&gt;right != NULL &amp;&amp; prev-&gt;right != cur) prev = prev-&gt;right; if (prev-&gt;right == NULL) &#123; prev-&gt;right = cur; visit(cur); cur = cur-&gt;left; &#125; else &#123; prev-&gt;right = NULL; cur = cur-&gt;right; &#125; &#125; &#125;&#125; Reference Morris Traversal方法遍历二叉树（非递归，不用栈，O(1)空间）","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"遍历","slug":"遍历","permalink":"https://blog.zhuwenq.cc/tags/%E9%81%8D%E5%8E%86/"},{"name":"算法","slug":"算法","permalink":"https://blog.zhuwenq.cc/tags/%E7%AE%97%E6%B3%95/"},{"name":"线索树","slug":"线索树","permalink":"https://blog.zhuwenq.cc/tags/%E7%BA%BF%E7%B4%A2%E6%A0%91/"}]},{"title":"数据结构-图的最短路径问题","slug":"数据结构-图的最短路径问题","date":"2022-03-20T08:22:04.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-图的最短路径问题/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E9%97%AE%E9%A2%98/","excerpt":"本文写于 2020 年 4 月 10 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 4 月 10 日，2022 年 3 月 20 日重新整理 即：在网络（带权图）中，求两个不同顶点之间的所有路径中，边的权值和最小的那条路径。这条路径就叫最短路径 (Shortest Path) ，这条路径的起点叫做源点 (Source) ，路径的最后一个顶点叫做终点 (Destination) 。 最短路径问题不是一个孤立的问题，它是一系列问题的集合，可以分为单源最短路径问题和多源最短路径问题。 单源最短路径问题 即从某固定源点出发，求其到其他任一点的最短路径。 其过程是按照路径长度递增（非递减）的顺序找到到达各个顶点的最短路径。 如下图所示的图： 假设顶点 0 是源点，则距源点的距离为的顶点是 0 ，距离源点为的顶点有 1,4 ，他们是源点的邻接点。查找距离为 2 的顶点时，不需要从源点开始查找，可以从距离为 1 的顶点查找他们的邻接点（同时不能是源点的邻接点）。这一过程与广度优先搜索相似，都是优先查找顶点的所有邻接点。 对距离从 0 到 1 的顶点列表，可得下表： 距源点的距离 顶点 0 0 1 1, 4 2 2, 3 回忆广度优先搜索的程序过程： 123456789101112131415161718192021int Visited[MaxSize] = 0;void breadthFirstSearch(Graph *graph, Vertex *vertex)&#123; visit(vertex); //visite vertex first Visited[vertex-&gt;id] = 1; //set visited true Queue *queue = createQueue(); add(queue, vertex); //add to queue to find its neighbors while (!isEmpty(queue)) &#123; Vertex *v = delete (queue); for (v 的每个邻居节点 neighbor) &#123; if (!Visited[neighbor-&gt;id]) //if not visted,visit and add to queue &#123; visit(neighbor); Visited[neighbor-&gt;id] = 1; add(queue, neighbor); &#125; &#125; &#125;&#125; 无权单源最短路径算法 对于广度优先搜索，我们需要一个 Visited 数组来记录顶点有没有被访问过，在最短路径问题中，我们还需要记录顶点与源点之间的距离。这里使用一个 dist 数组 dist[v] = 顶点v与源点之间的距离 来记录距离。同时最短路径问题实际上需要求解的不仅是最短距离，还需要求解实际的路径，因此我们还需要一个容器记录从源点到达顶点的路径。可以使用一个 path 数组 path[w] = v //v是从源点去顶点w的必经顶点来记录路径 ，这样，当需要直到源点到顶点 w 的路径时，递归的访问数组 path 就可以得到: 1234567vertex v;//目的顶点wStack stack;while(dist[w]!=0)//Dist[w] == 0时，说明递归到了源点，路径生成完毕&#123; push(stack,w);//路径顶点压栈 w = path[w];&#125; 综上，无权单源最短路径问题的程序过程大致如下： 1234567891011121314151617181920int dist[MaxSize] = &#123;-1&#125;;int path[MaxSize];void shortestPath(Vertex vertex)&#123; Queue *queue = new Queue(); enqueue(queue, vertex); while (!isEmpty(queue)) &#123; Vertex v = dequeue(queue); for (v 的所有邻居顶点 n) &#123; if (dist[n] == -1) &#123; dist[n] = dist[v] + 1; path[n] = v; enqueue(queue, n); &#125; &#125; &#125;&#125; 这个过程的大部分与广度优先搜索无异，只是在判断是否已访问以及标记路径距离和记录路径方面稍有不同。 程序执行完成后，从源点到图中各个顶点的最短路径长度被记录在数组 dist[] 中，路径信息被记录在数组 path[] 中。 带权单源最短路径算法 首先看如下图所示的带权图： 可以看到边 &lt;v2,v5&gt;&lt;v_2, v_5&gt;&lt;v2​,v5​&gt; 的权值是 −10-10−10 ，对这个图而言，只要沿着路径 v4→v2→v5v_4\\rightarrow v_2\\rightarrow v_5v4​→v2​→v5​ 走一圈，总的权值和就是 −5-5−5 ，如果走无数圈，权值和就是 −∞-\\infty−∞ 。这种走一圈权值和为负值的圈被称为负值圈 (nagative-cost cycle) ，显然，只要图中存在负值圈，一般的算法肯定无法正常工作。因此，下面的讨论默认图中不存在负值圈。 无权图可以看作特殊的带权图，因此，带权图的单源最短路径算法实际上与无权图的单源最短路径算法有一定的相似之处，他们都是按照路径长度递增（非递减）的顺序找到最短路径。下面讨论的带权图的单源最短路径算法就是大名鼎鼎的 **迪杰斯特拉 (Dijkstra)**算法。 Dijkstra算法大致有以下过程： 定义一个集合 s ， s 在初始状态下只有源点 source 存在。然后逐步将已经确定了最短路径的节点收进集合 s 。即：s=source+已经确定了最短路径的顶点vis = source + 已经确定了最短路径的顶点 v_is=source+已经确定了最短路径的顶点vi​ 对于图中任何一个尚未收进 s 的顶点 v ，定义 dist[v] = 源点到顶点 v 的最短路径长度 ，同时该路径必须仅仅经过集合 s 中的顶点，即路径 source→vi∈s→vsource\\rightarrow v_i \\in s\\rightarrow vsource→vi​∈s→v 的长度。值得注意的是，在程序运行的初始阶段，集合 s 中的顶点较少，此时路径 source→vi∈s→vsource\\rightarrow v_i \\in s\\rightarrow vsource→vi​∈s→v 多半并不是最短路径，但是随着集合 s 的扩大，该路径也会不断完善，最后当所有顶点都被收入集合 s ，最后的路径就一定是最短路径了。 由于路径是按照递增（非递减）的顺序生成的，因此有以下结论： 真正的最短路径肯定只经过集合 s 中的节点 每次收录进集合 s 的顶点是剩余未收录顶点中 dist 值最小的顶点（贪心算法） 增加一个顶点 v 进入集合 s 时，可能会影响到其他顶点 w 的 dist 值，这是因为源点到顶点 w 的路径可能会经过顶点 v ，并且顶点 v 到顶点 w 之间可能存在一条边。因此需要对顶点 w 的 dist 值进行更新： dist[w]=min(dist[w],dist[v]+&lt;v,w&gt;的权值)dist[w] = min(dist[w],dist[v] + &lt;v,w&gt;的权值)dist[w]=min(dist[w],dist[v]+&lt;v,w&gt;的权值) 。 下面是 Dijkstra 算法的大致过程，首先为了表示集合 s ，我们使用一个数组 collected 来记录顶点是否被收录（类似图的搜索算法中的 Visited 数组） 12345678910111213141516171819202122232425int dist[MaxSize] = &#123;infty&#125;;int path[MaxSize];bool collected[MaxSize] = &#123;false&#125;;void dijkstra(Vertex vertex)&#123; while (true) &#123; 取未被集合收录的顶点中dist值最小的顶点v //这个过程可以直接扫描全部顶点然后取最小或者把顶点存在一个最小堆中 if (v不存在，即顶点完全被收录) break; //跳出循环 collected[v] = true; //收录顶点 for (v 的所有邻居节点 w) &#123; if (!collected[w]) &#123; if (dist[v] + weight(v, w) &lt; dist[w]) &#123; //注意到这个条件要求dist[w]&gt;=dist[v]+weight(v,w)时下面的更新路径长度代码才会运行 //因此dist的初始值应该是正无穷，否则其初始值可能无法被更新 dist[w] = dist[v] + weight(v, w); path[w] = v; &#125; &#125; &#125; &#125;&#125; 这段伪代码不适用于存在负边的情况. 多源最短路径算法 多源最短路径问题求解的是任意两点之间的最短路径，要实现这个问题的求解，可以用图中每一个顶点作为源点对其运行一次单源最短路径算法，这种方法比较 low 而且对于稠密图效率不高。或者还有一种大名鼎鼎的算法专门解决这个问题：佛洛伊得 (Floyd) 算法 Floyd 算法对于稠密图的效果较好，因此下面的图使用邻接矩阵表示。 Floyd 算法的路径是按照顶点的编号递增的顺序生成的，它的大致过程如下： 用矩阵 Dk[i][j]D^k[i][j]Dk[i][j] 表示顶点 iii 到顶点 jjj 之间仅经过编号小于等于 kkk 的顶点的最短路径长度，即路径 i→(l≤k)→ji\\rightarrow (l \\leq k)\\rightarrow ji→(l≤k)→j 的长度。程序运行的初始阶段，kkk 值较小，路径并不一定是最短路径，当 kkk 增长到顶点个数少一时，所得的最短路径就是实际上的最短路径了，即最短路径是逐步生成的 矩阵 Dk[i][j]D^k[i][j]Dk[i][j] 的初始值是 D−1[i][j]D^{-1}[i][j]D−1[i][j] ，它可以直接初始化为图的邻接矩阵，对角元全部是 000 ，对于没有直接边的顶点 kkk 和 mmm ，其初始值 D−1[k][m]D^{-1}[k][m]D−1[k][m] 应该被初始化为无穷大 从矩阵 Dk−1[i][j]D^{k-1}[i][j]Dk−1[i][j] 递推到矩阵 Dk[i][j]D^k[i][j]Dk[i][j] 时，即新读入一个顶点 kkk： kkk 不影响 iii 到 jjj 的最短路径，即 kkk 不在路径上，则不需要更新最短路径长度：Dk[i][j]=Dk−1[i][j]D^k[i][j] = D^{k-1}[i][j]Dk[i][j]=Dk−1[i][j] kkk 影响 iii 到 jjj 的最短路径，即从 kkk 走的路径更短，则新的路径由两部分组成：Dk[i][j]=Dk−1[i][k]+Dk−1[k][j]D^k[i][j] = D^{k-1}[i][k] + D^{k-1}[k][j]Dk[i][j]=Dk−1[i][k]+Dk−1[k][j] 要记录最短路径，同样需要一个矩阵 Path[i][j]Path[i][j]Path[i][j] 储存从 iii 到 jjj 的路径。假如我们给矩阵赋值为 Path[i][j]=kPath[i][j]=kPath[i][j]=k ，这表明从 iii 到 jjj 的路径等于从 iii 到 kkk 的路径加上从 kkk 到 jjj 的路径。因此，路径的取出可以使用一个递归过程实现。 Floyd 算法的程序过程如下： 1234567891011121314151617181920212223242526272829int Dist[MaxSize][MaxSize];int Path[MaxSize][MaxSize];void floyd(Vertex **map, int mapSize)&#123; //初始化 for (int i = 0; i &lt; mapSize; ++i) &#123; for (int j = 0; j &lt; mapSize; ++j) &#123; Dist[i][j] = map[i][j]; Path[i][j] = -1; &#125; &#125; for (int k = 0; k &lt; mapSize; ++k) &#123; for (int i = 0; i &lt; mapSize; ++i) &#123; for (int j = 0; j &lt; mapSize; ++j) &#123; if (Dist[i][k] + Dist[k][j] &lt; Dist[i][j]) &#123; //这里判断k是否影响最短路径 //若无连接的两点未被初始化为无穷大，会影响这里的判断 Dist[i][j] = Dist[i][k] + Dist[k][j]; //更新最短路径长度 Path[i][j] = k; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"图","slug":"图","permalink":"https://blog.zhuwenq.cc/tags/%E5%9B%BE/"},{"name":"最短路径","slug":"最短路径","permalink":"https://blog.zhuwenq.cc/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/"}]},{"title":"数据结构-图的遍历","slug":"数据结构-图的遍历","date":"2022-03-20T08:11:56.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-图的遍历/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E9%81%8D%E5%8E%86/","excerpt":"本文写于 2020 年 4 月 2 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 4 月 2 日，2022 年 3 月 20 日重新整理 两种遍历 深度优先搜索 (Depth First Search,DFS) 深度优先搜索顾名思义就是在搜索过程中瞅准某条路径一直向下访问，直到访问到了这条路径的终点才切换到其他路径。问题是，在某条路径上一路向下访问时，需要保存住来时的路径信息，否则在到达终点的时候将无法返回到起点切换其他路径。 这种一路向下直到某个条件（终点）再原路返回的过程很像程序设计中的递归思想，因此，深度优先搜索可以方便的使用栈 (Stack) 实现。 如下面所示的图： 假设使用深度优先搜索从节点 0 开始遍历，则首先对节点 0 的所有邻居节点（与 0 直接相连的节点）中的一个访问，假设访问到了节点 2 ，则对节点 2 的邻居节点进行访问。首先访问邻居节点 0 ，0 已经被访问，因此切换到节点 1 。接着对节点 1 的邻居节点 2 访问， 2 已被访问，因此切换到 3 。接着访问 3 的邻居节点 4 。访问到节点 4 之后，对节点 4 的所有邻居节点访问，发现均被访问过。然后开始回溯，查找是否有尚未访问到的分支。比如此例中回溯到节点 3 ，发现所有邻居节点都被访问过，继续回溯亦然，当回溯到起始节点 0 时，便知道所有节点都被访问过，遍历完成。 实际的程序过程可以描述为： 建立一个空栈，访问起始顶点 0 ，并把 0 压栈 访问节点 0 的邻居节点 2 ，把 2 压栈 访问节点 2 的未访问节点 1 ，把 1 压栈 访问节点 3,4 ，把 3,4 压栈 遍历节点 4 的邻居节点之后，前方无路可走，开始出栈 首先出栈节点 3 ，访问节点 3 的未访问邻居节点并压栈，一直访问并压栈到尽头。之后对每个出栈的顶点做同样的操作，直到栈为空（顶点全部弹出）。 12345678910111213141516171819202122232425262728int Visited[Size] = 0;//深度优先搜索void depthFirstSearch(Graph *graph, Vertex *vertex)&#123; Stack *stack = createStack(); Vertex *nextNode = vertex; while (!isEmpty(stack)) &#123; visit(nextNode); //visit vertex push(stack, nextNode); //push vertex to stack Visited[nextNode-&gt;id] = 1; //shows that this vertex was visted int end = 1; //flag that shows if route comes to end for (int i = 0; i &lt; nextNode-&gt;neighborNum; ++i) //neighbor of vertex &#123; Vertex *neighbor = nextNode-&gt;neighbors[i]; if (!Visited[neighbor-&gt;id]) //if not visited &#123; nextNode = neighbor; //set nextNode neighbor not visited end = -1; //shows that end is not coming break; &#125; &#125; if (end) &#123; nextNode = pop(stack); //if end comes,pop a vertex from stack &#125; &#125;&#125; 这个实现首先构造了一个顶点个数长度的数组来标识顶点是否被访问，然后利用循环和堆栈实现遍历过程。这是一个并不优雅的实现，甚至不知道是否正确。 如果直接用递归程序会非常清晰：(两段代码实际上都是伪代码，因为节点的邻居节点信息的取出是不规范的，不正确的) 123456789101112int Visited[Size] = 0;void depthFirstSearch(Graph *graph, Vertex *vertex)&#123; visit(vertex); Visted[vertex-&gt;id] = 1; for(int i = 0;i &lt; vertex-&gt;neighborNum;++i) &#123; if(!Visited[vertex-&gt;neighbor[i]])&#123; depthFirstSearch(graph,vertex); &#125; &#125;&#125; 广度优先搜索 (Breadth First Search,BFS) 广度优先搜索过程类似于二叉树的层序遍历，都是先访问某节点，然后把该节点的相邻节点挨个访问，同样类似的是，广度优先遍历的实现也可以使用 队列 (Queue) 实现。 同一个图： 使用广度优先遍历的过程大致是，首先从起始顶点 0 出发，挨个访问它的邻居顶点 2,4 。然后访问节点 2 和它的邻居节点 0 (已被访问), 1,3 ，然后访问节点 4 和它的邻居节点 0,1,3 。最后对节点 0,1,3 挨个进行上述的动作。 使用队列实现的程序过程为： 首先建立空的队列，将节点 0 入队， 弹出节点 0 进行访问，同时把节点 0 的所有 未被访问过的 邻居节点 2,4 加入队列 从队列弹出一个顶点，对其进行访问并把它的所有 未被访问过的 邻居顶点加入队列。直到队列为空，说明遍历完成。 123456789101112131415161718192021void breadthFirstSearch(Graph *graph, Vertex *vertex)&#123; visit(vertex); //visite vertex first Visited[vertex-&gt;id] = 1; //set visited true Queue *queue = createQueue(); add(queue, vertex); //add to queue to find its neighbors while (!isEmpty(queue)) &#123; Vertex *v = delete (queue); for (int i = 0; i &lt; v-&gt;neighborNum; ++i) //find all neighbors &#123; Vertex *neighbor = v-&gt;neighbors[i]; if (!Visited[neighbor-&gt;id]) //if not visted,visit and add to queue &#123; visit(neighbor); Visited[neighbor-&gt;id] = 1; add(queue, neighbor); &#125; &#125; &#125;&#125; 这个实现仍然是伪代码，需要更加合理的查找顶点邻居信息的方法才能工作。 两种遍历的时间复杂度实际上是相同的（全部遍历时），但是对于遍历的中途退出的问题，需要认真思考选择更合适的遍历方式。如寻路问题中需要根据地图的样式选择遍历方式。 图的一些术语 连通：如果从顶点 v 到顶点 w 之间存在一条（无向）路径，则称顶点 v 和顶点 w 之间是连通的。 强连通：有向图中顶点 v 和 w 之间的路径是双向路径，则 v 和 w 之间是强连通的。 路径：顶点 v 到顶点 w 之间的路径是一系列顶点的序列，其中任意一对相邻的顶点之间在图中都有 边 (Edge) 。路径的长度是路径中的边数（对于带权图，路径长度是边的权值之和）。如果路径序列中所有的顶点都不相同（路径是无环的），则称路径为 简单路径 。 回路：起点等于终点的路径 连通图：图的 任意两点 均连通 强连通图：有向图 的 任意两点 均强连通 连通分量：无向图的极大连通子图： 极大顶点数：子图中再加任意一个点就不是连通图了 极大边数：子图中所有顶点之间的边数 强连通分量：有向图 的极大强连通分量 对于不连通的图，可以对图的每个连通分量调用上面两个遍历函数实现整个图的遍历。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"图","slug":"图","permalink":"https://blog.zhuwenq.cc/tags/%E5%9B%BE/"},{"name":"遍历","slug":"遍历","permalink":"https://blog.zhuwenq.cc/tags/%E9%81%8D%E5%8E%86/"}]},{"title":"数据结构-图","slug":"数据结构-图","date":"2022-03-20T07:50:09.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-图/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE/","excerpt":"本文写于 2020 年 4 月 1 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 4 月 1 日，2022 年 3 月 20 日重新整理 图是一种表示多对多关系的数据结构，它包含一组顶点 (Vertex) 和顶点之间的连接关系（称为边 (Edge) ）。 对于节点 viv_ivi​ 和 vjv_jvj​ ，一般用 &lt;vi,vj&gt;&lt;v_i, v_j&gt;&lt;vi​,vj​&gt; 表示由 viv_ivi​ 指向 vjv_jvj​ 的边。图中的相同节点对一般只考虑一条边，不考虑重复的边和指向自己的边，根据边的单向双向，带权不带权，图可以分为 无向无权图，无向带权图，有向无权图，有向带权图 图的操作集 1234567Graph *create(int vertexCnt); //创建空图返回void insertVertex(Graph *graph, Vertex *vertex); //将节点插入图void insertEdge(Graph *graph, Edge *edge); //将边插入图void depthFirstSearch(Graph *graph, Vertex *vertex); //从顶点vertex出发深度优先遍历void breadthFirstSearch(Graph *graph, Vertex *vertex); //从顶点vertex出发宽度优先遍历void shortestPath(Graph *graph, Vertex *vertex, int dist[]); //计算从节点vertex到任意节点的最短路径void minSpanningTree(Graph *graph); //计算图的最小生成树 图的建立实现在后文给出 如何表示一个图 邻接矩阵 使用一个二维矩阵可以描述一个图。对于有 n 个顶点的图，可以用一个 n*n 大小的矩阵 G[n][n] 来表示。 首先给顶点编号 0 - n-1 ，矩阵的值 G[i][j]=1G[i][j] = 1G[i][j]=1 (若 &lt;vi,vj&gt;&lt;v_i, v_j&gt;&lt;vi​,vj​&gt; 存在) ，G[i][j]=0G[i][j] = 0G[i][j]=0 若 &lt;vi,vj&gt;&lt;v_i, v_j&gt;&lt;vi​,vj​&gt; 不存在 。对于带权图，矩阵的值一般是边的权重。 如下面的有向带权图： 其邻接矩阵为： v0v_0v0​ v1v_1v1​ v2v_2v2​ v3v_3v3​ v4v_4v4​ v0v_0v0​ 0 2 0 0 0 v1v_1v1​ 0 0 2 2 3 v2v_2v2​ 0 0 0 0 0 v3v_3v3​ 0 0 0 0 0 v4v_4v4​ 0 0 0 0 0 对于无向图，他们的邻接矩阵一般是对称的： 邻接矩阵为： v0v_0v0​ v1v_1v1​ v2v_2v2​ v3v_3v3​ v4v_4v4​ v0v_0v0​ 0 1 0 1 0 v1v_1v1​ 1 0 1 1 0 v2v_2v2​ 0 1 0 0 1 v3v_3v3​ 1 1 0 0 0 v4v_4v4​ 0 0 1 0 0 因此，对于无向图，可以使用一个长度为 N(N+1)/2N(N+1) / 2N(N+1)/2 的序列储存，这样可以节省一半的空间。例如取上述矩阵的下半部分： v0v_0v0​ v1v_1v1​ v2v_2v2​ v3v_3v3​ v4v_4v4​ v0v_0v0​ 0 v1v_1v1​ 1 0 v2v_2v2​ 0 1 0 v3v_3v3​ 1 1 0 0 v4v_4v4​ 0 0 1 0 0 将上述数据按行优先顺序储存在一个数组里，数组的长度为 N(N+1)/2N(N+1)/2N(N+1)/2 。要访问节点 i,j(i行，j列) 之间的连接，可以访问数组的 i∗(i+1)/2+ji*(i+1)/2+ji∗(i+1)/2+j 位置。 邻接矩阵是一种较为直观的表示方法，它的好处有： 方便检查任意一对节点之间是否存在边 方便查找所有与某一结点直接相连的节点 方便计算节点的度（指向节点的边个数叫入度，从节点指向别的节点的边个数叫出度）。 它的缺点也很明显：对较为稀疏的图（点多边少）而言，空间利用效率不高，计算图的总边数是时间效率不高。 邻接表 使用一个长度为 n (节点个数)的链表数组 G[n] 储存图。数组的元素 G[i] 表示编号为 i 的节点， G[i] 后接它指向的所有节点指针。 如： 上图的邻接表为： v0v_0v0​ 3 1 v1v_1v1​ 0 2 3 v2v_2v2​ 1 4 v3v_3v3​ 1 1 v4v_4v4​ 2 这样的表示方法不会储存没有连接的无效信息，但是它把每条边都储存了两遍。对于稀疏的图而言，它的空间利用率较高，但是较为稠密的图会浪费较多空间。 邻接表可以方便的查找一个节点的所有直接相连节点，如果储存的图是无向图，它也可以方便的计算出节点的度。但是如果储存的是有向图，邻接表将无法方便的计算出节点的入度，同时它也无法方便地判断出给定节点对之间是否存在边。 图的建立 邻接矩阵 首先给出图的原型： 1234567typedef struct graph&#123; int vertexCount; //顶点数 int edgeCount; //边数 WeightType graphMat[MaxSize][MaxSize]; //邻接矩阵，元素表示边的权重 DataType data[MaxSize]; //节点数据&#125; Graph; 图的建立首先需要建立一个只有节点没有边的图： 123456789101112131415Graph *create(int vertexCnt)&#123; Graph *graph = (Graph *)malloc(sizeof(Graph)); graph-&gt;vertexCount = vertexCnt; //初始化顶点数 graph-&gt;edgeCount = 0; //初始化边数 for (int i = 0; i &lt; graph-&gt;vertexCount; ++i) &#123; graph-&gt;data[i] = 0; //数据初始化，应该初始化为用户给定的数据 for (int j = 0; j &lt; graph-&gt;vertexCount; ++j) &#123; graph-&gt;graphMat[i][j] = 0; //连接初始化 &#125; &#125; return graph;&#125; 然后把边插入到图中，边应该保存起点和终点以及权重数据： 123456789101112131415161718typedef int Vertex;//邻接矩阵中，节点可用坐标表示typedef struct edge&#123; Vertex start; //起点 Vertex end; //终点 WeightType weight; //权重&#125; Edge;void insertEdge(Graph *graph, Edge *edge)&#123; if (graph == NULL) return NULL; if (edge-&gt;start &gt;= graph-&gt;vertexCount || edge-&gt;end &gt;= graph-&gt;vertexCount) return NULL; graph-&gt;graphMat[edge-&gt;start][edge-&gt;end] = edge-&gt;weight; //对于无向图，还需要下面一句 graph-&gt;graphMat[edge-&gt;end][edge-&gt;start] = edge-&gt;weight;&#125; 对于给定的边的集合 Edge edges[someNumber] 只要对其中每个边调用插入函数就可以了 12345Edge edges[cnt];for(int i = 0;i &lt; cnt;++i)&#123; insertEdge(graph,edges[i]);&#125; 邻接表 邻接表表示的图原型有所不同，首先建立节点的原型，根据邻接表的结构，节点应该是一个链表。 123456typedef struct lnode&#123; int vertexPosition; //邻接点下标，即顶点编号 WeightType weight; //边权重 LVertex *next;&#125; LVertex; 邻接表应该是一个指针数组，数组元素是链表指针： 邻接表应该是一个指针数组，数组元素是链表指针： 1234typedef struct table&#123; //忽略顶点数据 LVertex *firstVertex; //第一个顶点&#125; LTable; 根据上面两个结构实现图的结构原型： 123456typedef struct lgraph&#123; int vertexCount; //顶点数 int edgeCount; //边数 LTable graphTable[MaxSize]; //邻接表&#125; LGraph; 邻接表实现的图建立第一步仍然是建立一个一定数量节点的空图： 1234567891011LGraph *createGraph(int vertexCount)&#123; LGraph *graph = (LGraph *)malloc(sizeof(LGraph)); graph-&gt;edgeCount = 0; graph-&gt;vertexCount = vertexCount; for (int i = 0; i &lt; graph-&gt;vertexCount; ++i) &#123; graph-&gt;graphTable[i].firstVertex = NULL; //邻接表链表初始化为空 &#125; return graph;&#125; 然后将边插入，由于邻接表中只保存了头节点，因此插入方式采用头插法。 12345678910111213141516171819void insertLEdge(LGraph *graph, Edge *edge)&#123; //建立要插入的节点 LVertex *vertex = (LVertex *)malloc(sizeof(LVertex)); vertex-&gt;vertexPosition = edge-&gt;end; vertex-&gt;weight = edge-&gt;weight; //插入邻接表 vertex-&gt;next = graph-&gt;graphTable[edge-&gt;start].firstVertex; graph-&gt;graphTable[edge-&gt;start].firstVertex = vertex; //对于无向图，还需要插入边&lt;end,start&gt; LVertex *sVertex = (Vertex *)malloc(sizeof(Vertex)); sVertex-&gt;vertexPosition = edge-&gt;start; sVertex-&gt;weight = edge-&gt;weight; sVertex-&gt;next = graph-&gt;graphTable[edge-&gt;end].firstVertex; graph-&gt;graphTable[edge-&gt;end].firstVertex = sVertex;&#125; 通过给定边数据建立完整图的过程与邻接矩阵无异。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"图","slug":"图","permalink":"https://blog.zhuwenq.cc/tags/%E5%9B%BE/"}]},{"title":"数据结构-哈夫曼树","slug":"数据结构-哈夫曼树","date":"2022-03-20T07:37:30.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-哈夫曼树/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/","excerpt":"本文写于 2020 年 3 月 31 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 3 月 31 日，2022 年 3 月 20 日重新整理 编码 考虑这样一种情形：在一篇若干字符组成的文章中，各个字符出现的频率一般是不同的。假如我们可以设计出一种不等长编码，让出现频率较高的字符使用更短的编码，出现频率较低的字符使用较长的编码，这样可以实现对文章的无损压缩。 哈夫曼编码就是这样一种压缩编码。 哈夫曼树(Huffman Tree) 考虑下面这种判定树： 1234567891011if(num&lt;60)&#123; return 1;&#125;else if(num &lt;70)&#123; return 2;&#125;else if(num&lt;80)&#123; return 3;&#125;else if(num&lt;90)&#123; return 4;&#125;else&#123; return 5;&#125; 如果对小规模的学生成绩进行 5 分制取整，可以使用上面这段愚蠢的程序。如果要处理的是较大规模的数据，那么上面的判断过程还有优化的空间。 根据上面的判断过程，对于 60 以下的数据，需要判断一次， 60-70 的数据，需要判断两次， 70-80 的数据，需要判断三次。假如要处理的数据集对不同分段的数据有个出现频率的信息，比如： | 数据段 | &lt;60 | 60-70 | 70-80 | 80-90 | &gt;90 | | ------ | ---- | ----- | ----- | ----- | | 频率 | 0.05 | 0.15 | 0.4 | 0.3 | 0.1 | 那么上面的判断过程平均判断次数为：0.05∗1+0.15∗2+0.4∗3+0.3∗4+0.1∗4=3.150.05 * 1 + 0.15 * 2 + 0.4 * 3 + 0.3 * 4 + 0.1 * 4 = 3.150.05∗1+0.15∗2+0.4∗3+0.3∗4+0.1∗4=3.15 次 根据上面的频率表可以查看出， 70-90 的占比较大，而他们需要判断 3,4 次。如果让占比较大的只判断一次，而让占比较小的 &lt;60 分段判断 4 次，似乎可以减少平均判断次数，提高效率。例如改进为如下判断： 1234567891011121314151617if(num&lt;80)&#123; if(num&lt;70)&#123; if(num&lt;60)&#123; return 1; &#125;else&#123; return 2; &#125; &#125;else&#123; return 3; &#125;&#125;else&#123; if(num&lt;90)&#123; return 4; &#125;else&#123; return 5; &#125;&#125; 则平均判断次数为：0.05∗3+0.15∗3+0.4∗2+0.3∗2+0.1∗2=2.20.05 * 3 + 0.15 * 3 + 0.4 * 2 + 0.3 * 2 + 0.1 * 2 = 2.20.05∗3+0.15∗3+0.4∗2+0.3∗2+0.1∗2=2.2 次，优化了不少。 这就是哈夫曼树的思想：根据元素出现的频率构造搜索树。 带权路径长度(WPL) 假设二叉树有 nnn 个叶子节点，每个叶子节点的权值为，从根节点到每个叶子节点的长度为，则每个叶子节点的带权路径和就是。哈夫曼树就是 WPL=∑WkLkWPL=\\sum W_kL_kWPL=∑Wk​Lk​ 最小的二叉树，又叫最优二叉树。 哈夫曼树的构造 哈夫曼树的构造过程很简单，从一个带权节点的序列中取出权值最小的两个节点 a 和 b ，将 a 和 b 合并，并且构造一个新节点作为 a 和 b 的父节点，父节点的权值为 a,b 权值之和。然后把父节点放回到序列中，再取出权值最小的两个节点合并。一直做到序列为空，哈夫曼树就形成了。 哈弗曼数结构上就是普通的二叉树，因此它的原型为： 123456typedef struct hafman&#123; WeightType weight; HuffmanTree *left; HuffmanTree *right;&#125; HuffmanTree; 构造的过程需要取出权值最小的元素，不妨用最小堆(MinHeap)储存节点，最小堆的实现与[[数据结构-堆]]类似。如果使用排好序的序列也可以实现。 1234567891011121314HuffmanTree *constructHuffmanTree(MinHeap *heap)&#123; HuffmanTree *huffman; for (int i = 1; i &lt; heap-&gt;size; ++i) //size nodes need size-1 times insert &#123; huffman = (HuffmanTree *)malloc(sizeof(HuffmanTree)); huffman-&gt;left = deleteMin(heap); huffman-&gt;right = deleteMin(heap); huffman-&gt;weight = huffman-&gt;left-&gt;weight + huffman-&gt;right-&gt;weight; insert(heap, huffman); &#125; return deleteMin(heap);&#125; 哈夫曼树的特点 根据哈夫曼树构造的过程，可以看出哈夫曼树具有以下特点： 没有度为 1 的节点: 哈夫曼树构造时总是两两合一，因此它的所有节点要么度为 2 ，要么度为 0 n 个叶节点的哈夫曼树总结点数为 2n-1: 二叉树的节点分为三类：度为 0，1，2 的三类节点。设度为 i 的节点数量为 nin_ini​ ，同时满足关系：n2=n0−1n_2 = n_0 - 1n2​=n0​−1 。因此在哈夫曼树中：叶节点数量n即为 n0n_0n0​ ，由于没有度为 1 的点，所以节点总数是 n0+n2=2n−1n_0 + n_2 = 2n - 1n0​+n2​=2n−1 。 哈夫曼树交换左右子树仍然是哈夫曼树: 对于同一组带权节点序列，可能存在不同构的两颗哈夫曼树，但是他们的 WPL 相同。 哈夫曼编码 是一种字符串的不等长编码方式，可以使字符串占用的空间最小。使用不等长编码有一个问题：给定一串编码，无法准确的把每个字符分割出来，不同的分割方式有不同的含义，即二义性问题。针对这个问题，又有聪明人提出了解决的方法：使用前缀码 (Prefix Code) 。 所谓前缀码，即满足：任何字符的编码都不是其他字符编码的前缀。例如给出下面的三个字母的非前缀编码： 123a: 1b: 0c: 10 则字符 a 的编码 1 是字符 c 的编码 10 的前缀，当给出编码 10 时，无法分辨究竟是 ab 还是 c 。 哈夫曼编码是在一棵二叉树上进行的，规定 0 代表左子树， 1 代表右子树，同时字符只储存在叶节点上。这样每个字符的编码实际上描述了从根节点到该字符所在叶节点的路径，同时任何一个字符的编码都不可能是其他字符编码的前缀。 解决了二义性问题，哈夫曼编码下一个要解决的问题就是效率。结合哈夫曼树的思想，把字符串中每个字符的出现次数作为该字符的权值建立哈夫曼树，这样出现次数多的节点将位于较浅位置处的叶节点，其路径编码（ 0 代表左， 1 代表右）长度也较短。这样就实现了对一个字符串或者一篇文章的字符不等长编码，也就实现了对字符串的压缩。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"哈夫曼树","slug":"哈夫曼树","permalink":"https://blog.zhuwenq.cc/tags/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/"}]},{"title":"数据结构-堆","slug":"数据结构-堆","date":"2022-03-20T07:20:51.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-堆/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%A0%86/","excerpt":"本文写于 2020 年 3 月 30 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 3 月 30 日，2022 年 3 月 20 日重新整理 堆 Heap 即 优先队列 Priority Queue 几种实现方式 一般数组（无序）： 插入：对于一般数组，可以总是把元素插入到数组的头部或尾部，因此它的时间复杂度是 O(1)O(1)O(1) 删除：首先要在数组中找到最大(或最小)元素，查找的时间复杂度是 O(n)O(n)O(n) 的。然后将元素删除，被删除元素的后方元素需要前挪一位，时间复杂度也是 O(n)O(n)O(n) 有序数组： 插入：对于有序数组，插入时需要找到合适的位置，这一操作的时间复杂度一般是 O(n)O(n)O(n) 或者 O(log⁡n)O(\\log n)O(logn) 的 删除：由于数组是有序的，删除的位置总是在数组的头部或尾部，它的时间复杂度是 O(1)O(1)O(1) 的 一般链表（无序）： 插入：插入可以简单的插入在链表头部，时间复杂度是 O(1)O(1)O(1) 的 删除：首先找到最大（或最小）元素，时间复杂度是 O(n)O(n)O(n) ，然后删除掉该节点，删除操作对链表而言是 O(1)O(1)O(1) 的 有序链表： 插入：首先找到合适的位置，时间复杂度是 O(n)O(n)O(n) ，然后插入，操作是 O(1)O(1)O(1) 的 删除：删除总是在两端进行，时间复杂度是 O(1)O(1)O(1) 二叉树： 二叉搜索树：二叉搜索树的最大值在最右侧，最小值在最左侧。对于优先队列这个使用场景，经过若干次删除操作之后二叉搜索树将退化为一颗斜二叉树，时间效率会迅速下降。 最大堆 注意到优先队列只有两个主要操作： 插入 (Insert) 和 删除 (Delete) 如果我们使用一颗二叉树储存优先队列的数据，应该着重优化更加困难的删除操作。考虑这样一种结构：二叉树的最大节点始终在它的根节点处，对每一棵子树而言亦如此，那么删除操作将始终在根节点进行。为了避免二叉树变斜，我们使用完全二叉树储存数据，这样就形成了一个 最大堆(MaxHeap) 最大堆是一颗 完全二叉树 (Complete Binary Tree) ，同时它的每个节点比它的左右子树中每个节点大（比子树小称为最小树）。 如下图就是一个最大堆： 最大堆的实现 [[数据结构-二叉树和二叉树的储存结构]] 中提到，对于完全二叉树，它具有父节点序号是子节点序号的二分之一这一性质，因此完全二叉树内部可以使用数组实现，同时数组实现的完全二叉树具有很高的效率也很方便操作，只是容量一般是固定的，空间效率较低。数据结构原型如下： 123456typedef struct maxheap&#123; KeyType *data; //堆的数据 int size; //当前元素个数 int capacity; //堆的最大容量&#125; MaxHeap; 主要操作有： 12345MaxHeap *createHeap(const int maxSize); //创建空堆，最大空间为maxSizeint isFull(MaxHeap *heap); //判断堆是否已满int isEmpty(MaxHeap *heap); //判断堆是否为空void insert(MaxHeap *heap, KeyType key); //把元素key插入堆KeyType deleteMax(MaxHeap *heap); //删除并返回堆中的最大值 下面只讨论插入和删除操作 插入 一般的数组实现的完全二叉树可以很方便的插入。我们在原型中维护有表示二叉树当前元素个数的 tag，并且完全二叉树的节点下标有简单的层序规律（自上而下，自左而右）。因此可以直接把元素插入到数组的 size + 1 处。 然而作为一个最大堆，这样的插入可能会破坏最大堆的另一个原则：根节点总是大于左右子树节点。因此，在插入到 size + 1 位置后，还需要判断该原则是否被破坏，如果被破坏则需要修复这一原则。 例如上面图片中的堆如果插入元素 2 ，被插入的位置下标为 10 ，是下标为 5 的节点 3 的左子树位置。同时 2 小于 3 ，这个插入不会破坏根最大原则。 假如插入的元素是 5 ，则会因为 5 大于 3 而需要对最大堆进行修复。修复的过程也很简单，只要把节点 5 与节点 3 交换位置就行了（插入的节点与父节点交换位置）。假如插入的元素是 91 ，则需要一直交换到整棵树的根节点处。 这个实现采用了另一个方案，插入时先根据根最大原则判断合适的位置在哪里，在判断的同时把不合适的节点统统后移，为新插入的节点腾出位置。 1234567891011121314void insert(MaxHeap *heap, KeyType key)&#123; if (heap == NULL) return; //if heap is NULL,do nothing else if (isFull(heap)) return; //if heap is full,do nothing int position = ++heap-&gt;size; //insert position should be 1 after size //when break out loop,heap-&gt;data[position]&gt;key &amp;&amp; position is the right position for (; heap-&gt;data[position] &lt; key; position /= 2) &#123; //in loop,heap-&gt;data[position]&gt;key,move data down heap-&gt;data[position] = heap-&gt;data[position / 2]; &#125; heap-&gt;data[position] = key; //insert key to the right pos&#125; 删除 最大堆删除的位置是确定的：总是二叉树的根节点位置。然而不能直接地就把根节点删除掉，因为一方面根节点在数组中的下标是 0 ，把它删除了以后后面所有的元素都要前移，时间复杂度是 O(n)O(n)O(n) （如果不移迟早空间会被全部丢失）；另一方面是因为我们有更好的选择：删掉数组尾部位置的节点，然后把尾部节点的元素拿到根节点替换原来的根节点。这样删除过程可以简述为：删掉尾部节点，替换根节点。 与插入过程类似，根节点被尾节点替换之后最大堆的根最大原则无法得到保证。 如下图最大堆： 删掉最大值之后，使用尾部节点 3 代替根节点，大小顺序被破坏： 大小顺序恢复的过程为：找到根节点左右子节点中较大的一个，让他与根节点交换位置，然后在被交换的子节点位置对它的子树重复前述操作。 例如上面的最大堆恢复之后如下： 下面的实现采用了与插入类似的方案：先不做替换，而是先让不合适的节点移位，腾出合适的节点把被删掉的尾部节点插入。 12345678910111213141516171819202122232425KeyType deleteMax(MaxHeap *heap)&#123; if (heap == NULL) return; else if (isEmpty(heap)) return; KeyType target = heap-&gt;data[1]; KeyType temp = heap-&gt;data[heap-&gt;size--]; int child = 0; int parent = 1; for (; parent * 2 &lt;= heap-&gt;size; parent = child) &#123; child = parent * 2; //left child index,may overflow if (child &lt;= heap-&gt;size &amp;&amp; heap-&gt;data[child] &lt; heap-&gt;data[child + 1]) &#123; //if right child is bigger,turn to right child child++; &#125; if (heap-&gt;data[child] &lt;= temp) break; //if order is corrict,break else //replace nodes heap-&gt;data[parent] = heap-&gt;data[child]; &#125; heap-&gt;data[parent] = temp; //put temp to right position return target;&#125; 或者也可以直接把尾部节点放在根节点，然后从上到下判断顺序是否正确，不正确就交换位置。 1234567891011121314151617181920KeyType deleteMax(MaxHeap *heap)&#123; if (heap == NULL) return; else if (isEmpty(heap)) return; KeyType target = heap-&gt;data[1]; heap-&gt;data[1] = heap-&gt;data[heap-&gt;size--]; int child = 0; for (int i = 1; i * 2 &lt; heap-&gt;size; i = child) &#123; child = i * 2; if (heap-&gt;data[child] &lt; heap-&gt;data[child + 1]) child++; if (heap-&gt;data[i] &lt; heap-&gt;data[child]) swap(&amp;heap-&gt;data[i], &amp;heap-&gt;data[child]); else break; &#125;&#125; 从序列建堆 1MaxHeap* createHeap(KeyType* list, int size); 有两种思路，一种是先建立一个空堆，然后把元素一个一个插入到空堆中，这种方法的时间复杂度是 O(nlog⁡n)O(n\\log n)O(nlogn) 。另一种是将序列中的元素直接放在一个数组里，按照完全二叉树的构造，这个数组就是一个完全二叉树。然后对完全二叉树中的每个父节点进行修正，最终得到一个最大堆，这个操作是线性的复杂度 O(n)O(n)O(n) (不包括数组拷贝的时间)。 1234567891011121314151617181920//MaxHeap *createHeap(KeyType *list,int size)&#123; MaxHeap *heap = (MaxHeap*)malloc(sizeof(MaxHeap)); heap-&gt;data = (KeyType*)malloc(sizeof(KeyType) * (size + 1)); for (int i = 1;i&lt;size;++i)&#123; heap-&gt;data[i] = list[i]; &#125; int parent = heap-&gt;size / 2; for(;parent &gt; 0;parent--) &#123; int child = parent * 2; if(child + 1 &lt;= heap-&gt;size) &#123; child += heap-&gt;data[child] &lt; heap-&gt;data[child + 1]; &#125; if(heap-&gt;data[parent] &lt; heap-&gt;data[child]) swap(&amp;heap-&gt;data[parent],&amp;heap-&gt;data[child]); &#125;&#125;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"堆","slug":"堆","permalink":"https://blog.zhuwenq.cc/tags/%E5%A0%86/"}]},{"title":"数据结构-平衡二叉树","slug":"数据结构-平衡二叉树","date":"2022-03-20T06:40:58.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"数据结构-平衡二叉树/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"本文写于 2020 年 3 月 27 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 3 月 27 日，2022 年 3 月 20 日重新整理 写在前面 假如搜索一颗二叉搜索树 (Binary Search Tree) 中的某个节点，那么搜索的最坏次数将是该二叉搜索树的高度。再假如被搜索的二叉树是一颗斜二叉树，那么搜索的最坏情况将需要树的节点个数 n 次。如果二叉搜索树的高度尽可能小，将大大降低搜索的时间复杂度，相比斜二叉树的最坏时间复杂度为 O(n) ，较为平衡的二叉树将把时间复杂度降低为 O(log n) 。 按照高度尽可能低或者左右尽可能平衡来排列节点的二叉搜索树就是平衡二叉树 (Balance Binary Tree) or (AVL Tree)( AVL 是以发明这一数据结构的科学家命名的)。 平衡因子 平衡因子 (Balance Factor) 表示一个树节点的平衡程度，它的值 BF(T)BF(T)BF(T) 由节点 T 的左右子树高度差决定： BF(T)=Hl(T)−Hr(T)BF(T) = Hl(T) - Hr(T)BF(T)=Hl(T)−Hr(T) 平衡二叉树 平衡二叉树的任一节点它的左右子树高度差即平衡因子的绝对值不超过 1，即 ∣BF(T)∣&lt;=1|BF(T)|&lt;=1∣BF(T)∣&lt;=1 平衡二叉树的高度 设一颗高度为 hhh 的平衡二叉树能拥有的最小的节点数为 nhn_hnh​, 则对于该二叉树的任一节点，其左右子树的高度分别为 h−1h - 1h−1 和 h−2h - 2h−2, 其节点数分别为 nh−1n_{h-1}nh−1​ 和 nh−2n_{h-2}nh−2​, 则该二叉树的总节点数为 nh=nh−1+nn−2+1n_h = n_{h-1} + n_{n-2} + 1nh​=nh−1​+nn−2​+1. 该递推公式推广到该二叉树的任一子树都成立，因此，高度为 hhh 的平衡二叉树节点数满足类斐波那契数列递推公式 设斐波那契数列的第 hhh 项值为 Fh=Fh−1+Fh−2F_h = F_{h-1} + F_{h-2}Fh​=Fh−1​+Fh−2​, 则 nh=Fh−1n_h = F_h - 1nh​=Fh​−1. 又因为 Fh≈15(1+52)hF_h \\approx \\frac{1}{\\sqrt{5}}(\\frac{1 + \\sqrt{5}}{2})^hFh​≈5​1​(21+5​​)h, 故 nh≈15(1+52)h−1n_h \\approx \\frac{1}{\\sqrt{5}}(\\frac{1 + \\sqrt{5}}{2})^h - 1nh​≈5​1​(21+5​​)h−1，所以，对于节点数为 nnn 的平衡二叉树，他的高度是 log⁡2n\\log_2nlog2​n 数量级的，搜索的最坏时间复杂度是 O(log⁡n)O(\\log n)O(logn) 平衡二叉树的自平衡 由于平衡二叉树也是一颗二叉搜索树，它在节点插入时仍需满足二叉搜索树左小右大的规则。这种情况下难免会破坏原有的平衡（平衡因子会改变）。如： 对这颗树而言，它是平衡的，除最倒数第二层节点外其他节点的平衡因数都是 0 。此时如果要插入元素 8 ，根据二叉搜索树的插入规则，节点将被插入到节点7的右子节点位置，不会破坏树的平衡。 如果被插入的元素是 79 ，则会被插入到节点 78 的右子节点位置，此时节点 71 的平衡因数将变为 -2 ，节点 50 的平衡因数也会变为 -2 ，平衡性被破坏。 我们称平衡性被破坏的节点为发现者，破坏平衡性的节点为破坏者。根据破坏者与发现者的相对位置，把破坏平衡的插入分为 4 种情况： RR插入 —破坏者位于发现者的右子树的右边 LL插入 —破坏者位于发现者的左子树的左边 RL插入 —破坏者位于发现者的右子树的左边 LR插入 —破坏者位于发现者的左子树的右边 对这四种情况分别进行处理，把被破坏的平衡树重新构造成平衡二叉树，就是平衡二叉树的自平衡。 四种处理方式分别为：RR旋转，LL旋转，RL旋转，LR旋转，旋转的结果需要满足二叉查找树的规则。 RR旋转 如图所示的平衡二叉树，假设插入元素 80 此时发现者是 50 ，破坏者是 80 。 根据二叉搜索树的性质不难知道，发现者 50 的右子树所有节点均大于 50 。因此，可以用 50 的右子节点替换 50 的位置，同时把 50 右子节点 78 的左子树（一定小于 78 ）挂在 50 的右子节点处（一定大于 50 ），最后把 50 这一节点连同它的子树挂在节点 78 的左子节点位置。 过程可以简述为： 使用发现者 F 的右子节点 r 代替发现者 把发现者 F 挂到 r 的左子节点处 把 r 的左子节点（可以为空）重新挂在 F 的右子节点处 返回占据原 F 节点位置的节点（应该为 r ）。 123456789101112//注意：此函数不应单独使用，它应该配合插入操作使用// 因为单单将finder的子树旋转是不够的，还需要把返回的节点挂载到原树上AVLTree *singleRightRotation(AVLTree *tree)&#123; //tree is the finder AVLTree *replacer = tree-&gt;right; //right child of finder tree-&gt;right = replacer-&gt;left; //right child&#x27;s left child to finder&#x27;s right child replacer-&gt;left = tree; //finder to right child&#x27;s left child //update heights of changed nodes replacer-&gt;height = maxOfTwo(replacer-&gt;left-&gt;height, replacer-&gt;right-&gt;height) + 1; tree-&gt;height = maxOfTwo(tree-&gt;left-&gt;height, tree-&gt;right-&gt;height) + 1; return replacer;&#125; LL旋转 类似的，假设上图所示的平衡二叉树要插入元素 2 则新插入的元素位于最左侧节点 1 的右子节点位置。此时发现者是 6 ，破坏者是 1 ，破坏者位于发现者的左子树的左子树的右子节点处（注意：虽然位于右子节点，但仍然是发现者的左子树的左子树上）,因此是 LL插入。 与 RR插入 类似，发现者的左子树所有节点都小于发现者，因此可以: 用发现者的左子节点代替发现者的位置 把发现者挂在其左子节点的右子节点处（发现者一定大于它的左子节点） 把发现者的左子节点的右子树挂在发现者的左子节点位置 12345678910AVLTree *singleLeftRotation(AVLTree *tree)&#123; AVLTree *replacer = tree-&gt;left; tree-&gt;left = replacer-&gt;right; replacer-&gt;right = tree; //update heights of changed nodes replacer-&gt;height = maxOfTwo(replacer-&gt;left-&gt;height, replacer-&gt;right-&gt;height) + 1; tree-&gt;height = maxOfTwo(tree-&gt;left-&gt;height, tree-&gt;right-&gt;height) + 1; return replacer;&#125; RL旋转 如上平衡二叉树，插入元素 49 49 插入到节点 47 的右子节点处，此时发现者是 46 ，破坏者是 49 。破坏者位于发现者的右子树的左子树上，因此是 RL插入。 此时，发现者 F 必有一个右子节点 R ，节点 R 必有一个左子节点 L。 所谓 RL旋转 就是先对节点 R 做右旋: 然后对节点 F 做左旋： 12345AVLTree *rightLeftRotation(AVLTree *tree)&#123; tree-&gt;right = singleRightRotation(tree-&gt;right); return singleLeftRotation(tree);&#125; LR旋转 上图所示树插入元素 25 发现者是 30 ，破坏者是 25 。先对节点 20 做左旋: 在对节点 30 右旋： 12345AVLTree *leftRightRotation(AVLTree *tree)&#123; tree-&gt;left = singleLeftRotation(tree-&gt;left); return singleRightRotation(tree);&#125; 插入操作 在上述四种旋转的基础上，可以实现新的具有自平衡特点的插入操作。插入之后判断其父节点的平衡因子，如果平衡被破坏，使用对应的旋转操作恢复平衡。 123456789101112131415161718192021222324252627282930313233343536373839404142AVLTree *insert(AVLTree *tree, KeyType key)&#123; //插入完成后该程序的递归过程会从下而上检查父节点的平衡性 if (tree == NULL) &#123; tree = (AVLTree *)malloc(sizeof(AVLTree)); tree-&gt;key = key; tree-&gt;left = tree-&gt;right = NULL; &#125; else if (key &lt; tree-&gt;key) &#123; //insert left sub tree tree-&gt;left = insert(tree-&gt;left, key); if (getHeight(tree-&gt;left) - getHeight(tree-&gt;right) == 2) &#123; //balance broken if (key &lt; tree-&gt;key) &#123; //single left rotation tree = singleLeftRotation(tree); &#125; else &#123; //signle left right rotation tree = leftRightRotation(tree); &#125; &#125; &#125; else if (key &gt; tree-&gt;key) &#123; tree-&gt;right = insert(tree-&gt;right, key); if (getHeight(tree-&gt;right) - getHeight(tree-&gt;left) == 2) &#123; //balance broken if (key &gt; tree-&gt;key) &#123; //single right rotation tree = singleRightRotation(tree); &#125; else &#123; //right left rotation tree = rightLeftRotation(tree); &#125; &#125; &#125; //update height tree-&gt;height = maxOfTwo(getHeight(tree-&gt;left), getHeight(tree-&gt;right)) + 1; return tree;&#125; 删除操作 TODO","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"平衡二叉树","slug":"平衡二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"数据结构-二叉搜索树","slug":"数据结构-二叉搜索树","date":"2022-03-20T06:28:47.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-二叉搜索树/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","excerpt":"本文写于 2020 年 3 月 26 日，2022 年 3 月 20 日重新整理","text":"本文写于 2020 年 3 月 26 日，2022 年 3 月 20 日重新整理 二叉搜索树 (Binary Search Tree) 又称为二叉查找树。二叉搜索树本质上是一颗二叉树，在非空时它具有以下性质： 左子树的所有键值 key 小于根节点键值 右子树的所有键值 key 大于根节点键值 左子树和右子树也是二叉搜索树 （每个节点的键值是唯一的） 操作集 12345678910//从二叉搜索树中找出键值为key的节点地址BinarySearchTree *find(BinarySearchTree *tree, KeyType key);//找出二叉搜索树的最小键值节点BinarySearchTree *findMin(BinarySearchTree *tree);//找出二叉搜索树的最大键值节点BinarySearchTree *findMax(BinarySearchTree *tree);//向二叉搜索树插入键值为key的节点BinarySearchTree *insert(BinarySearchTree *tree, KeyType key);//从二叉搜索树删除键值为key的节点BinarySearchTree *delete (BinarySearchTree *tree, KeyType key); 实现 find 查找的操作较为简单 若树为空，返回 NULL 若树非空，将 key 与根节点的 key 进行比较： 若大于根节点的 key ，说明待查找节点在根节点的右侧，在右子树中继续查找 若小于根节点的 key ，说明待查找节点在根节点的左侧，在左子树中继续查找 若等于根节点的 key ，说明待查找节点已找到，直接返回跟节点 123456789101112131415161718//递归实现BinarySearchTree *find(BinarySearchTree *tree, KeyType key)&#123; if (tree == NULL) return NULL; if (key &gt; tree-&gt;key) &#123; return find(tree-&gt;right, key); &#125; else if (key &lt; tree-&gt;key) &#123; return find(tree-&gt;left, key); &#125; else &#123; return tree; &#125;&#125; 上述尾递归实现效率不高而且有爆栈的风险，下面是将尾递归展开为循环的实现。 12345678910111213141516//循环实现BinarySearchTree *find(BinarySearchTree *tree, KeyType key)&#123; if (tree == NULL) return NULL; BinarySearchTree *temp = tree; while (temp) &#123; if (key &gt; temp-&gt;key) temp = temp-&gt;right; else if (key &lt; temp-&gt;key) temp = temp-&gt;left; else return temp; &#125;&#125; findMin 观察二叉查找树的结构特点，容易知道其最小的元素一定在树的最左侧。 1234567891011BinarySearchTree *findMin(BinarySearchTree *tree)&#123; if (tree == NULL) return NULL; BinarySearchTree *temp = tree; while (temp-&gt;left) &#123; temp = temp-&gt;left; &#125; return temp;&#125; 1234567//递归实现BinarySearchTree *findMin(BinarySearchTree *tree)&#123; if(tree==NULL) return NULL; if(tree-&gt;left) return findMin(tree-&gt;left); else return tree;&#125; findMax 同理，最大元素一定在二叉搜索树的最右侧。 insert 插入操作较为复杂，先找到要插入的位置，然后构造节点将节点插入合适的位置。 首先看简单的递归实现 123456789101112131415BinarySearchTree *insert(BinarySearchTree *tree, KeyType key)&#123; if (tree == NULL) &#123; tree = (BinarySearchTree *)malloc(sizeof(BinarySearchTree)); tree-&gt;key = key; tree-&gt;left = tree-&gt;right = NULL; return tree; &#125; if (key &gt; tree-&gt;key) return insert(tree-&gt;right, key); else if (key &lt; tree-&gt;key) return insert(tree-&gt;left, key); //else key==tree-&gt;key; do nothing&#125; 循环实现： 12345678910111213141516171819//循环实现BinarySearchTree *insert(BinarySearchTree *tree, KeyType key)&#123; BinarySearchTree *temp = tree; while (temp) &#123; if (key &gt; temp-&gt;key) temp = temp-&gt;right; else if (key &lt; temp-&gt;key) temp = temp-&gt;left; else break; //if equals,I dont know what&#x27;s right,lets just rewrite it. &#125; //when break out while loop,temp must be NULL temp = (BinarySearchTree *)malloc(sizeof(BinarySearchTree)); temp-&gt;key = key; temp-&gt;left = temp-&gt;right = NULL; return temp;&#125; delete 删除操作是最为复杂的，因为被删除的节点处在不同位置时，操作的行为也是不同的。首先找到要删除的节点 若删除的节点是叶节点，可以直接将其置为NULL 若删除的节点有一个子节点，可以使用它的孩子节点代替被删除的节点 若删除的节点有两个子节点，则有两种选择： 使用被删除节点的左子树的最大值节点代替被删除节点 使用被删除节点的右子树的最小值节点代替被删除节点 然后还需要删除左子树/右子树中那个拿去代替被删除节点的节点，过程较为复杂，删除子树节点的操作需要递归完成（其他的实现不会😁） 123456789101112131415161718192021222324252627BinarySearchTree *delete (BinarySearchTree *tree, KeyType *key)&#123; BinarySearchTree *target = find(tree, key); if (target == NULL) return NULL; if (target-&gt;left &amp;&amp; target-&gt;right) &#123; //both left child and right child are valid KeyType min = findMin(target-&gt;right)-&gt;key; //or findMax(target-&gt;left) target-&gt;key = min; delete (target-&gt;right, min); &#125; else &#123; BinarySearchTree *temp = target; //copy target address,it need to be freed if (target-&gt;left != NULL) &#123; //only left child target = target-&gt;left; &#125; else if (target-&gt;right != NULL) &#123; //only right child target = target-&gt;right; &#125; //else no child,just free the node will be fine free(temp); &#125; return tree;&#125; 最后 注意到根据二叉搜索树的插入规则，假如要插入的元素一个比一个大，则树的右侧将形成一个长链，高度会迅速增加。为了解决这个问题，应该在插入时引入一种尽量平衡的插入规则，这就是平衡二叉树。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"二叉搜索树","slug":"二叉搜索树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"}]},{"title":"Latex Citation Style","slug":"Latex-Citation-Style","date":"2022-03-18T15:03:43.000Z","updated":"2023-06-21T06:51:28.800Z","comments":true,"path":"Latex-Citation-Style/","link":"","permalink":"https://blog.zhuwenq.cc/Latex-Citation-Style/","excerpt":"LaTeX\\LaTeXLATE​X 中有三个主流的 bibliography 包：bibtex, natbib 和 biblatex, 其中 biblatex 支持本地化(多语言), 也更加现代化更加易用。","text":"LaTeX\\LaTeXLATE​X 中有三个主流的 bibliography 包：bibtex, natbib 和 biblatex, 其中 biblatex 支持本地化(多语言), 也更加现代化更加易用。 12% 导入 biblatex , 使用 biber 后端 , author-year cite style\\usepackage[backend=biber,style=authoryear]&#123;biblatex&#125; 导入 biblatex 可以传入的参数[1]： backend=biber: 排序 bibliography 的后端程序，默认的 biber 支持不同的排序方式和多种样式，同时也更易于使用。另一个选项是 bibtex, 仅支持排序，没有样式支持。 style=alphabetic: bibliography 样式和 citation 样式 sorting=ynt: 定义排序方式 option 排序顺序 nty name, title, year nyt name, year, title nyvt name, year, volume, title anyt alphabetic label, name, year, title anyvt alphabetic label, name, year, volume, title none 以 citation 顺序排列 bibliography 样式 style 字段支持的 bibtex 样式[2]： 拓展的样式： 除此之外还可以自定义 bibliography section 的标题： 1\\printbibliography[title=&#123;Whole bibliography&#125;] 或者以某个标准过滤打印 bibliography： 12345\\printbibliography[type=article,title=&#123;Articles only&#125;]\\printbibliography[type=book,title=&#123;Books only&#125;]\\printbibliography[keyword=&#123;physics&#125;,title=&#123;Physics-related only&#125;]\\printbibliography[keyword=&#123;latex&#125;,title=&#123;\\LaTeX-related only&#125;] 添加 bibliography section 到目录： 1234% 顶级目录\\printbibliography[heading=bibintoc,title=&#123;Whole bibliography&#125;]% 次级目录\\printbibliography[heading=subbibintoc,type=article,title=&#123;Articles only&#125;] Citation 样式 没有显示设置时，biblatex 会使用最符合 bibliography 样式的 citation 样式[3] numberic: 数字样式 numberic-comp: 自动聚合连续的数字：[1, 2, 3] 聚合为 [1-3] numberic-verb: 自动分解连续的数字：[1, 2, 3] 分解为 [1];[2];[3] alphabetic: alphabetic-verb: 分解连续的 citation authoryear: authoryear-comp: 同一个作者的不同年份的引用聚合：Doe 1992, Doe 1995 聚合为 Doe 1992, 1995 authoryear-ibid: authoryear-icomp: authortitle: authortitle-comp: authortitle-ibid: authortitle-icomp: authortitle-terse: authortitle-tcomp: authortitle-ticomp: verbose: reading: 期刊样式 Citation style biblatex stylename description ACS chem-acs American Chemical Society (ACS) style AIP phys (*) American Institute of Physics (AIP) style Nature nature Nature style Science science Science style IEEE ieee Institute of Electrical and Electronics Engineers (IEEE) style Chicago chicago-authordate Chicago Style MLA mla MLA style APA apa American Psychological Association (APA) style Bibliography management with biblatex ↩︎ Biblatex bibliography styles ↩︎ Biblatex Citation styles ↩︎","categories":[{"name":"记录备忘","slug":"记录备忘","permalink":"https://blog.zhuwenq.cc/categories/%E8%AE%B0%E5%BD%95%E5%A4%87%E5%BF%98/"}],"tags":[{"name":"Latex","slug":"Latex","permalink":"https://blog.zhuwenq.cc/tags/Latex/"},{"name":"Citation Style","slug":"Citation-Style","permalink":"https://blog.zhuwenq.cc/tags/Citation-Style/"}]},{"title":"Knowledge-Aware Languege Model Pretraining","slug":"Knowledge-Aware-Languege-Model-Pretraining","date":"2022-03-12T08:44:33.000Z","updated":"2023-06-21T06:51:28.800Z","comments":true,"path":"Knowledge-Aware-Languege-Model-Pretraining/","link":"","permalink":"https://blog.zhuwenq.cc/Knowledge-Aware-Languege-Model-Pretraining/","excerpt":"文章研究向 PLM 注入知识的方法，提出一种不需更改 transformer layer，不需添加额外的 knowledge-aware layer 的知识注入方法：KALM。","text":"文章研究向 PLM 注入知识的方法，提出一种不需更改 transformer layer，不需添加额外的 knowledge-aware layer 的知识注入方法：KALM。 Introduction PLM 生成的语言表征被认为含有丰富的词法语法知识，但是在缺乏显示知识性任务训练时，PLM 常常生成语法上正确，但事实上错误的文本。这说明 PLM 中缺乏语义，以及事实知识。 作者提出一种促使 PLM 注意输入文本中的实体以及其在文本中的角色的预训练模式，在没有使模型变得更大的情况下向 PLM 注入知识。 作者首先将输入语句中的 word span 连接到其指代的实体上，然后同时为 word 生成 word embedding 和 entity embedding。在输出层，除去 PLM 的语言建模目标之外，作者添加了一个 entity prediction task 引导模型从干扰项中分辨出 word 所指向的实体。这两个训练目标综合起来即显示地引导模型不仅要预测出正确的词（语法，句法，语义知识），还要预测出这些词所指代的实体（事实知识）。 KALM 作者在自回归模型的基础上设计 KALM，对于一个 nnn 个 tokens 的序列 X={w1,w2,...,wn}X = \\{w_1, w_2, ..., w_n\\}X={w1​,w2​,...,wn​}, 自回归模型由以下语言概率分解描述： p(X)=∏ip(wi∣w&lt;i)p(X) = \\prod_i p(w_i|w_{&lt;i}) p(X)=i∏​p(wi​∣w&lt;i​) 在 PLM 中，通常由 transformer layer 计算上述概率： p(wi∣w&lt;i)=transformer(wi∣w&lt;i)p(w_i|w_{&lt;i}) = \\text{transformer}(w_i|w_{&lt;i})p(wi​∣w&lt;i​)=transformer(wi​∣w&lt;i​) 上述过程中，PLM 间接通过词之间的共现模式捕捉语义知识。作者则通过向模型提供一个信号提示输入/输出中实体的存在以促使模型对知识的注意，期望 PLM 能够从输入语句中捕捉事实知识 Entity Tokenizer 作者首先使用一个 Entity Tokenizer 将输入文本中所有的 token 与其所指代的最常出现的实体连接：wi:i+k→eiw_{i:i+k}\\rightarrow e_iwi:i+k​→ei​, 其中 eie_iei​ 是 word span wi:i+kw_{i:i+k}wi:i+k​ 所最经常指代的实体。当 wiw_iwi​ 不属于任何已知的实体时：ei=nulle_i = nullei​=null. 上述过程通过在一个预定义的实体词典上进行文本匹配进行。 经过 tokenize 后，输入语句被分成词-实体两个 token 序列： Xduet={{w1,w2,…,wT}Word Sequence{e1,e2,…,eT}Entity SequenceX_{duet} = \\begin{cases} \\{w_1, w_2, \\dots, w_T\\} &amp;\\text{Word Sequence}\\\\ \\{e_1, e_2, \\dots, e_T\\} &amp;\\text{Entity Sequence} \\end{cases}Xduet​={{w1​,w2​,…,wT​}{e1​,e2​,…,eT​}​Word SequenceEntity Sequence​ 上述两个序列逐位置对齐，当有多个词对应同一个实体时，如 wi:i+kw_{i:i+k}wi:i+k​ 对应一个实体，则 eie_iei​ 到 ei+ke_{i+k}ei+k​ 是相同的。 Knowledge-Aware Input 经过 tokenize 后，作者为两个 token 序列分别生成嵌入： ei=Embeddinge(ei)∈Rdewi=Embeddingw(wi)∈Rdw\\begin{aligned} \\mathbf{e}_i &amp;= \\text{Embedding}_e(e_i)\\in\\mathbb{R}^{d_e}\\\\ \\mathbf{w}_i &amp;= \\text{Embedding}_w(w_i)\\in\\mathbb{R}^{d_w} \\end{aligned}ei​wi​​=Embeddinge​(ei​)∈Rde​=Embeddingw​(wi​)∈Rdw​​ 两个嵌入线性相加作为模型的输入嵌入：ti=wi+Lineart(ei)\\mathbf{t}_i = \\mathbf{w}_i + \\text{Linear}_t(\\mathbf{e}_i)ti​=wi​+Lineart​(ei​), 其中 Lineart∈Rde×dw\\text{Linear}_t\\in\\mathbb{R}^{d_e\\times d_w}Lineart​∈Rde​×dw​ Knowledge-Aware Output 在输出层上，除自回归模型的 next-word prediction 任务外，作者添加了一个 next-entity prediction 任务。 具体来说，作者添加了一个 output head 进行实体辨别。记 LLL 层 transformer 层输出的第 iii 个 token 的表征为 hiL\\mathbf{h}_i^LhiL​, 则第 iii 个位置的实体损失计算为： le(ei∣t&lt;i)=max⁡(0,s(hiL,ei)−s(hiL,e−)+λ)s(hiL,ej)=cos⁡(Linear(hiL),ej)hiL=transformerL(t&lt;i)\\begin{aligned} l_e(e_i|t_{&lt;i}) &amp;= \\max(0, s(\\mathbf{h}_i^L, \\mathbf{e}_i) - s(\\mathbf{h}_i^L, \\mathbf{e}_-)+\\lambda)\\\\ s(\\mathbf{h}_i^L, \\mathbf{e}_j) &amp;= \\cos(\\text{Linear}(\\mathbf{h}_i^L), \\mathbf{e}_j)\\\\ \\mathbf{h}_i^L &amp;= \\text{transformer}^L(t_{&lt;i}) \\end{aligned}le​(ei​∣t&lt;i​)s(hiL​,ej​)hiL​​=max(0,s(hiL​,ei​)−s(hiL​,e−​)+λ)=cos(Linear(hiL​),ej​)=transformerL(t&lt;i​)​ 上式中，eie_iei​ 指第 iii 个 token 所指代的实体，e−e_-e−​ 指作者从除 eie_iei​ 之外的实体中采样得到的负例，该损失促使模型分辨 tokenitoken_itokeni​ 所指代的实体。 值得注意的是，在实验中，作者使用的负例采样策略为：1% 的 nullnullnull , 49% 的随机采样实体，50% 的从目标实体的 Trans-E 嵌入空间中最近的 100 个实体中采样（被认为是难以分辨的负例）。 Pretraining KALM 的总体损失是： lKALM(Xduet=∑ilw(p(wi∣t&lt;i))+αle(ei∣t&lt;i)l_\\text{KALM}(X_{duet} = \\sum_i l_w(p(w_i|t_{&lt;i})) + \\alpha l_e(e_i|t_{&lt;i}) lKALM​(Xduet​=i∑​lw​(p(wi​∣t&lt;i​))+αle​(ei​∣t&lt;i​) Inference 推理时，仅使用 word prediction head。 实验 作者进行了知识嗅探评测和 zero-shot QA 任务评测。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Injection","slug":"Knowledge-Injection","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Injection/"},{"name":"Entity Signal","slug":"Entity-Signal","permalink":"https://blog.zhuwenq.cc/tags/Entity-Signal/"}]},{"title":"Differentiable Reasoning over a Virtual Knowledge Base","slug":"Differentiable-Reasoning-over-a-Virtual-Knowledge-Base","date":"2022-03-08T08:59:37.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"Differentiable-Reasoning-over-a-Virtual-Knowledge-Base/","link":"","permalink":"https://blog.zhuwenq.cc/Differentiable-Reasoning-over-a-Virtual-Knowledge-Base/","excerpt":"文章主要思想是利用包含事实的语料库作为虚拟知识库进行自然语言问答。2021 年 EMNLP 的文章 TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph 中提到的在 text form 关系的知识图谱上进行问答的思想与此类似。","text":"文章主要思想是利用包含事实的语料库作为虚拟知识库进行自然语言问答。2021 年 EMNLP 的文章 TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph 中提到的在 text form 关系的知识图谱上进行问答的思想与此类似。 作者首先在语料库上预先提取出所有的实体提及，并为其生成上下文表征，并基于处理的结果制作出实体-&gt;提及以及提及-&gt;实体的映射矩阵。随后在执行问答任务时，首先从问题中提取出涉及到的实体，编码成向量。然后通过与实体-&gt;提及矩阵相乘初步选出提及，随后作者根据整个问题为提及评分，筛选掉与问题不相关的提及。筛选出的结果向量乘以提及-&gt;实体矩阵得到推理的结果实体向量。对于 k-hop 的问题，只需迭代式地执行 k 次上述过程即可得到结果。 Methodology 作者提出的主要思想是将语料库看作知识库，并基于此回答自然语言问题 qqq. 首先从问题 qqq 中提取出涉及到的实体集 zzz, 然后让 zzz 中的实体跟随问题中蕴含的关系在知识库中找到答案。作者使用 IFIDF 模拟该过程，首先将实体集 zzz 中的实体扩展到语料库中所有跟其共同出现过的实体提及 mmm, 此时并非 mmm 中的所有共同出现过的提及都是与问题 qqq 相关的。因此作者使用一个神经网络根据问题 qqq 筛选这些提及，筛选后的提及被认为有两个性质： 与 zzz 中的实体共同出现过 与 zzz 共同出现的语句描述了和问题 qqq 类似的关系 根据这些筛选过的提及，作者又将其转换到他们所指代的实体集 z′z&#x27;z′，z′z&#x27;z′ 即候选答案。如果问题是多跳的，则迭代式多次重复上述过程即可。 Formal Description 记语料库为 D={d1,d2,...,d∣D∣}\\mathcal{D} = \\{d_1, d_2, ..., d_|\\mathcal{D}|\\}D={d1​,d2​,...,d∣​D∣}, 其中 dk=(dk1,dk2,...,dkLk)d_k = (d_k^1, d_k^2, ..., d_k^{L_k})dk​=(dk1​,dk2​,...,dkLk​​) 是语料库中有 LkL_kLk​ 个 token 的文档。首先作者使用 entity linker 将一个固定的实体集 E\\mathcal{E}E 在语料库中的提及识别出来，每个提及 mmm 记作 (em,km,im,jm)(e_m, k_m, i_m, j_m)(em​,km​,im​,jm​), 表示文档 kmk_mkm​ 中的文本片段 dkmim,...,dkmjmd_{k_m}^{i_m}, ..., d_{k_m}^{j_m}dkm​im​​,...,dkm​jm​​ 表示实体 em∈Ee_m\\in\\mathcal{E}em​∈E. 记语料库中所有的实体提及组成的集合为 M\\mathcal{M}M, 则有 ∣M∣≫∣E∣|\\mathcal{M}| \\gg |\\mathcal{E}|∣M∣≫∣E∣. 作者从概率语言模型出发，考虑一个仅以最终答案 a∈Ea\\in\\mathcal{E}a∈E 为参考的 T-hop 问题的弱监督场景，记中间 hop 的答案为 z0,z1,...,zT∈Ez_0, z_1, ..., z_T\\in\\mathcal{E}z0​,z1​,...,zT​∈E, 其中 z0z_0z0​ 即问题中出现的实体，zTz_TzT​ 为最终答案 aaa. 则每一个 hop 的计算模型为： Pr⁡(zt∣q)=∑zt−1∈EPr⁡(zt∣q,zt−1)Pr⁡(zt−1∣q)\\Pr(z_t|q)=\\sum_{z_{t-1}\\in\\mathcal{E}}\\Pr(z_t|q, z_{t-1})\\Pr(z_{t-1}|q) Pr(zt​∣q)=zt−1​∈E∑​Pr(zt​∣q,zt−1​)Pr(zt−1​∣q) 由于推理过程基于语料库，作者结合语料库中的实体提及 mmm 来计算 Pr⁡(zt∣q,zt−1)\\Pr(z_t|q, z_{t-1})Pr(zt​∣q,zt−1​)： Pr⁡(zt∣q)=∑m∈M∑zt−1∈EPr⁡(zt∣m)Pr⁡(m∣q,zt−1)Pr⁡(zt−1∣q)\\Pr(z_t|q) = \\sum_{m\\in\\mathcal{M}}\\sum_{z_{t-1}\\in\\mathcal{E}}\\Pr(z_t|m)\\Pr(m|q,z_{t-1})\\Pr(z_{t-1}|q) Pr(zt​∣q)=m∈M∑​zt−1​∈E∑​Pr(zt​∣m)Pr(m∣q,zt−1​)Pr(zt−1​∣q) 上式中，Pr⁡(m∣q,zt−1)\\Pr(m|q, z_{t-1})Pr(m∣q,zt−1​) 表示 mmm 与问题 qqq 和 上一步的实体 zt−1z_{t-1}zt−1​ 之间的相关性。作者利用实体以及实体提及的 TFIDF 向量计算该相关性，同时利用一个神经网络筛选符合问题描述的提及： Pr⁡(m∣q,zt−1)∝1{G(zt−1)⋅F(m)&gt;ϵ}⏟共现提及展开×st(m,zt−1,q)⏟相关性筛选\\Pr(m|q, z_{t-1})\\propto \\underbrace{\\mathbb{1}\\{G(z_{t-1})\\cdot F(m) &gt; \\epsilon\\}}_{\\text{共现提及展开}}\\times \\underbrace{s_t(m, z_{t-1}, q)}_{\\text{相关性筛选}} Pr(m∣q,zt−1​)∝共现提及展开1{G(zt−1​)⋅F(m)&gt;ϵ}​​×相关性筛选st​(m,zt−1​,q)​​ 上式中第一项被认为基于和上一步的中间实体 zt−1z_{t-1}zt−1​ 共现与否筛选提及，第二项根据问题 qqq 和上一步的实体 zt−1z_{t-1}zt−1​ 计算第 t 个 hop 需要的关系类型，并据此进一步筛选提及。 除 Pr⁡(m∣q,zt−1)\\Pr(m|q, z_{t-1})Pr(m∣q,zt−1​) 外，Pr⁡(zt∣m)\\Pr(z_t|m)Pr(zt​∣m) 则根据 entity linker 的结果判断提及 mmm 是否指代实体 ztz_tzt​。指代则为 1， 否则为 0。 在实际实现时，作者使用矩阵计算实现上述模型。首先作者预计算语料库中所有实体和实体提及的 TFIDF 向量，并将其组织成一个稀疏矩阵 AE←M[e,m]=1(G(e)⋅F(m)&gt;ϵ)A_{E\\leftarrow M}[e, m] = \\mathbb{1}(G(e)\\cdot F(m) &gt; \\epsilon)AE←M​[e,m]=1(G(e)⋅F(m)&gt;ϵ)(即实体 eee 和提及 mmm 的 TFIDF 向量的积大于阈值 ϵ\\epsilonϵ 时，A[e,m]=1A[e, m] = 1A[e,m]=1). 此时共现提及展开可以通过实体向量 zt−1z_{t-1}zt−1​ 和 AE←MA_{E\\leftarrow M}AE←M​ 的乘积得到。 对于相关性筛选，记 TK(st(m,zt−1,q))\\mathbb{T}_K(s_t(m, z_{t-1}, q))TK​(st​(m,zt−1​,q)) 为 top-K 相关的提及，且编码到向量空间 R∣M∣\\mathbb{R}^{|\\mathcal{M}|}R∣M∣. 对于提及到实体的映射，作者使用另一个稀疏矩阵 BM←EB_{M\\leftarrow E}BM←E​ 进行，该矩阵表示与相同实体共现过的提及。 综合上述过程，ztz_tzt​ 的计算可以表示为： Zt=softmax([Zt−1TAE←M⊙TK(st(m,zt−1,q))]BM←E)Z_t = \\text{softmax}([Z_{t-1}^TA_{E\\leftarrow M}\\odot\\mathbb{T}_K(s_t(m, z_{t-1}, q))]B_{M\\leftarrow E}) Zt​=softmax([Zt−1T​AE←M​⊙TK​(st​(m,zt−1​,q))]BM←E​) 其中 ZtZ_tZt​ 是编码后的实体概率向量。 上式的计算过程是可微的，同时由于 top-K 的限制，每个 hop 的计算最多产生 K 个实体，避免了多跳场景下的实体爆炸。 作者在下文介绍了上式计算中考虑矩阵稀疏性的效率，同时说明了 st(m,zt−1,q)s_t(m, z_{t-1}, q)st​(m,zt−1​,q) 的计算方法：st(m,zt−1,q)∝exp⁡{f(m)⋅gt(q,zt−1)s_t(m, z_{t-1}, q)\\propto \\exp\\{f(m)\\cdot g_t(q, z_{t-1})st​(m,zt−1​,q)∝exp{f(m)⋅gt​(q,zt−1​) 使用 PLM 编码 mmm 的上下文表征，并用 mmm 中首尾词的表征的加权和表征 mmm： f(m)=WT[Hid;Hjd]f(m) = W^T[H_i^d; H_j^d]f(m)=WT[Hid​;Hjd​] 使用 PLM 编码问题 qqq，取 [CLS][CLS][CLS] 的表征作为 qqq 的表征 HqH^qHq。 在此基础上，对于每一个 hop t=1,...,Tt = 1, ..., Tt=1,...,T 使用2层 transformer 层根据 HqH^qHq 进行最大内积搜索(MIPS)，第一层 transformer layer 被认为产生起始词的 MIPS 查询向量 HstqH_{st}^qHstq​, 第二层被认为产生结束词的 MIPS 查询向量 HenqH_{en}^qHenq​, 定义 g~t(q)=VT[Hstq;Henq]\\tilde{g}_t(q) = V^T[H_{st}^q; H_{en}^q]g~​t​(q)=VT[Hstq​;Henq​] 记所有实体的表征矩阵为 E∈R∣E×p∣E\\in\\mathbb{R}^{|\\mathcal{E}\\times p|}E∈R∣E×p∣, Zt−1Z_{t-1}Zt−1​ 所标示的实体的表征平均值为 Zt−1TEZ_{t-1}^TEZt−1T​E, 则 gt(q,zt−1)=g~t(q)+Zt−1TEg_t(q, z_{t-1}) = \\tilde{g}_t(q) + Z_{t-1}^TEgt​(q,zt−1​)=g~​t​(q)+Zt−1T​E.(实体表征使用实体文本所有词的表征平均计算) 在上述计算过程中，作者发现直接使用 PLM 为实体生成表征效果不理想，仍需要使用知识图谱远程监督训练 PLM 改善实体表征。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"}]},{"title":"数据结构--二叉树的遍历","slug":"数据结构-二叉树的遍历","date":"2022-03-06T09:19:38.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-二叉树的遍历/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/","excerpt":"本文写于 2020 年 3 月 25 日，2022 年 3 月 6 日重新整理","text":"本文写于 2020 年 3 月 25 日，2022 年 3 月 6 日重新整理 写在前面 树是一种复杂的数据结构，同时也是一种重要的二维结构。树的遍历过程实际上就是一种把二维结构转换为一维序列即线性化的过程。 四种遍历 前序遍历 (PreOrderTraversal) ：根节点—&gt;左子树—&gt;右子树 中序遍历 (InOrderTraversal) ：左子树—&gt;根节点—&gt;右子树 后序遍历 (PostOrderTraversal) ：左子树—&gt;右子树—&gt;根节点 层序遍历 (LevelOrderTraversal) ：从上到下—&gt;从左到右 遍历实现 抽象一个函数 void visit(BinaryTree *tree) 用来访问节点： 12345678typedef struct treeNode&#123; ElementType data; BinaryTree *left; BinaryTree *right;&#125; BinaryTree;void visit(BinaryTree *tree); //访问节点 递归实现 前序遍历 前序遍历首先访问根节点，然后访问左子树的根节点，然后访问左子树的左子树的根节点…直到左子树一路向左访问完毕，它将自下而上访问右子树。对于右子树，它也将贯彻根—左—右的顺序一路访问，直到遍历完成。 递归实现如下： 123456789void preOrderTraversal(BinaryTree *tree)&#123; if (tree) &#123; //if empty tree or reaches the leaf,exit visit(tree); //pay a visit to the root preOrderTraversal(tree-&gt;left); //pre order,traversal left first preOrderTraversal(tree-&gt;right); &#125;&#125; 中序遍历 中序遍历先访问左子树，然后访问根节点，最后访问右子树。因此可以将前序递归的访问移到对左子树递归的后面简单的实现。 123456789void inOrderTraversal(BinaryTree *tree)&#123; if (tree) &#123; //if empty tree or reaches the leaf,exit inOrderTraversal(tree-&gt;left); //pre order,traversal left first visit(tree); //pay a visit to the root inOrderTraversal(tree-&gt;right); &#125;&#125; 后序遍历 同样的，可以把访问语句移到访问右子树的后面。 123456789void postOrderTraversal(BinaryTree *tree)&#123; if (tree) &#123; postOrderTraversal(tree-&gt;left); postOrderTraversal(tree-&gt;right); visit(tree); &#125;&#125; 层序遍历不具有明显的递归特征，暂不讨论其递归实现 非递归实现 众所周知，递归的程序虽然逻辑清晰代码简单，但是其运行起来有许多不稳定因素。由于函数在构建时是以函数栈的形式储存函数信息的，因此在处理较大规模的输入时，递归的程序常常会发生爆栈的严重异常。 同时，递归实现的遍历本质上是把节点压入堆栈并在适当的时机弹出，因此，可以考虑使用堆栈 (Stack) 实现二叉树的遍历。 首先实现一个大致的堆栈，关于堆栈的详细介绍和实现可见这篇博客 1234567891011121314151617181920212223242526272829303132333435363738typedef struct stack&#123; ElementType data; Stack *next;&#125; Stack;Stack *createEmpty()&#123; Stack *top = (Stack *)malloc(sizeof(Stack)); top-&gt;next = NULL; return top;&#125;int isEmpty(Stack *stack)&#123; return stack-&gt;next == NULL;&#125;void push(Stack *stack, ElementType item)&#123; Stack *temp = (Stack *)malloc(sizeof(Stack)); temp-&gt;data = item; temp-&gt;next = stack-&gt;next; stack-&gt;next = temp;&#125;ElementType pop(Stack *stack)&#123; if (isEmpty(stack)) &#123; return; //error!Empty stack &#125; ElementType targetValue = stack-&gt;next-&gt;data; Stack *target = stack-&gt;next; stack-&gt;next = target-&gt;next; free(target); return targetValue;&#125; 前序遍历 根据前序遍历的顺序，我们首先把二叉树 T 的左边最外侧节点依照从上到下的顺序依次访问（保证了先访问根节点和先处理左子树）和压栈（保存节点，后面才能取到右子节点）。左外侧访问并压栈完后，如果堆栈不为空，弹出一个节点，此节点是最后一个压栈的也是二叉树左下角的节点。将此节点转向它的右子节点，重复前面的过程（保证了后处理右子树）。 1234567891011121314151617181920void preOrderTraversal(BinaryTree *tree)&#123; BinaryTree *t = tree; //make a copy Stack *stack = createEmpty(); while (t || !isEmpty(stack)) &#123; //if t==null &amp;&amp; stack is empty,exit while (t) &#123; //push left nodes push(stack, t); visit(t); //visit root and left nodes first t = t-&gt;left; &#125; if (!isEmpty(stack)) &#123; t = pop(stack); t = t-&gt;right; //turn to right visit(t); //visit right node &#125; &#125;&#125; 中序遍历 中序遍历按照左子节点—根节点—右子节点的顺序访问，先把左外侧节点压栈但不访问，压栈完成后弹出节点进行访问（保证了先访问的是左子节点先处理的是左子树）。访问完成后转向弹出节点的右子节点重复过程。 12345678910111213141516171819void inOrderTraversal(BinaryTree *tree)&#123; BinaryTree *t = tree; Stack *stack = createEmpty(); while (t || !isEmpty(stack)) &#123; while (t) &#123; push(stack, t); t = t-&gt;left; &#125; if (!isEmpty(stack)) &#123; t = pop(stack); visit(t); t = t-&gt;right; &#125; &#125;&#125; 后序遍历 后序遍历先处理左子树，再处理右子树，最后处理根节点。在先处理左子树的中序遍历的基础上，弹出节点 T 后并不立刻进行处理，而是进行一次判断：若 T 没有右子节点 T-&gt;right == null ，则该节点为叶节点，进行访问；若节点存在右子节点，则不进行访问，把 T 转向 T 的右子节点，重复上面的过程转而处理右子树。 这个过程存在一个错误：当 T 存在右子节点时，把 T 转向了 T-&gt;right ，由于 T 是以 pop 的方式出栈，则堆栈中尚未进行处理的 T 节点（根节点）被删除了，这就导致了根节点缺失。 为了解决这个问题，我们可以在 T 存在右节点时，为 T 的值做一个拷贝 TCopy ，但不保留 T 的子节点信息。即： TCopy-&gt;left == TCopy-&gt;right == null 。然后把 TCopy 压栈，这样保证了根节点 T 的值能够被访问到，同时在 T 的右子树处理完成，堆栈再次弹出根节点时，其右子节点不会被重复处理。 123456789101112131415161718192021222324252627282930void postOrderTraversal(BinaryTree *tree)&#123; BinaryTree *t = tree; Stack *stack = createEmpty(); BinaryTree *tCopy = (BinaryTree *)malloc(sizeof(BinaryTree)); while (t || !isEmpty(stack)) &#123; while (t) &#123; push(stack, t); t = t-&gt;left; &#125; if (!isEmpty(stack)) &#123; t = pop(stack); tCopy-&gt;data = t-&gt;data; t = t-&gt;right; //turn to right if (t) &#123; tCopy-&gt;left = tCopy-&gt;right = NULL; push(stack, tCopy); //temp will be visited after right sub tree &#125; else &#123; visit(tCopy); &#125; &#125; &#125; free(tCopy);&#125; 层序遍历 层序遍历按照先上后下，先左后右的顺序，因此在访问左子节点时，需要暂存右子节点。可以采用堆栈 (Stack) 或者队列 (Queue) 储存。 堆栈实现 暂不讨论（不会） 队列实现 队列的详细实现见这篇博客 先将根节点入队，然后出队根节点进行访问，访问完成后顺序压栈根节点的左子节点，右子节点。 再次出队一个节点，访问完成后顺序入队它的左右子节点。重复本过程直到访问完毕。 12345678910111213141516void levelOrderTraversal(BinaryTree *tree)&#123; if (tree == NULL) return; //Error!empty tree Queue *queue = createQueue(); add(queue, tree); //add root to queue while (!isEmpty(queue)) &#123; //when queue is empty,traversal done BinaryTree *t = delete (queue); visit(t); if (t-&gt;left) add(queue, t-&gt;left); if (t-&gt;right) add(queue, t-&gt;right); &#125;&#125; 最后 二叉树十分复杂且重要，二叉树的遍历又是二叉树重要的基础操作之一，理解并掌握上述遍历方法十分必要。 由两种遍历唯一地确定一颗二叉树 设前序遍历为 Pre ，中序遍历为 In ，后序遍历为 Post 。则在已知 In &amp; Pre 或者 In &amp; Post 的情况下可以唯一低确定一颗二叉树。 分析： 一个先序遍历序列由根节点—左子树节点—右子树节点组成 一个中序遍历序列由左子树节点—根节点—右子树节点组成 一个后序遍历序列由左子树节点—右子树节点—根节点组成 例如已知 Pre &amp; In : 由先序序列的第一个元素可以知道根节点，利用根节点在中序序列中查询其位置，可以得知左子树的长度。然后在先序序列中的根节点后方截取该长度，即得二叉树的左子树序列。 左子树序列的首元素是左子树的根节点…如此迭代到叶节点便确定了左子树的构造，右子树同理。 如果只已知 Pre &amp; Post 则无法确定左子树序列或者右子树序列的长度，无法确定二叉树。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构--二叉树和二叉树的储存结构","slug":"数据结构-二叉树和二叉树的储存结构","date":"2022-03-06T09:05:20.000Z","updated":"2023-06-21T06:51:28.940Z","comments":true,"path":"数据结构-二叉树和二叉树的储存结构/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E4%BA%8C%E5%8F%89%E6%A0%91%E5%92%8C%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%82%A8%E5%AD%98%E7%BB%93%E6%9E%84/","excerpt":"本文写于 2020 年 3 月 24 日，2022 年 3 月 6 日重新整理","text":"本文写于 2020 年 3 月 24 日，2022 年 3 月 6 日重新整理 定义 二叉树是一个有穷的节点集合，这个集合可以为空。若集合不为空，则集合是由根节点 root 和被称为左子树和右子树的两颗不相交的二叉树组成。 特殊二叉树 斜二叉树 (Skewed Binary Tree) 如图： 完美二叉树 (Perfect Binary Tree) 或满二叉树 (Full Binary Tree) 完全二叉树 (Complete Binary Tree) 有 n 个节点的二叉树，对树中节点按从上到下，从左到右的顺序进行编号。与满二叉树相比，相同位置处的节点编号相同。 即：完全二叉树相当于满二叉树少了最下层叶子节点的右侧若干节点。 性质 二叉树的第 i 层最大节点个数为 2i−12^{i - 1}2i−1 深度为 k 的二叉树最大节点总数为 2k−12^k - 12k−1 对于任何非空二叉树 T ，若 n0n_0n0​ 表示叶节点的个数， n2n_2n2​ 是度为2的非叶节点的个数，那么两者满足关系 n0=n2+1n_0 = n_2 + 1n0​=n2​+1 证明：对于总结点数为 n 的二叉树，其边总数为 n-1 。总的节点数 n 由度为 0,1,2 的节点组成，即 n−1=n0+n1+n2−1n - 1 = n_0 + n_1 + n_2 - 1n−1=n0​+n1​+n2​−1 。 同时，对于二叉树的节点，其度数即为其对总边数的贡献值，即：度为 i 的节点将贡献 i 条边。所以边数 n-1 满足 n−1=0×n0+1×n1+2×n2n - 1 = 0\\times n_0 + 1\\times n_1 + 2\\times n_2n−1=0×n0​+1×n1​+2×n2​ 。 所以 n−1=n0+n1+n2−1=0×n0+1×n1+2×n2n-1 = n_0+n_1+n_2 - 1=0\\times n_0+1\\times n_1 + 2\\times n_2n−1=n0​+n1​+n2​−1=0×n0​+1×n1​+2×n2​ ，即： n0=n2+1n_0 = n_2 + 1n0​=n2​+1 。 ADT描述 操作集 123bool isEmpty(BinaryTree* tree); //判断二叉树是否为空BinaryTree* createBinaryTree(); //创建一个二叉树void traversal(BinaryTree* tree); //遍历二叉树 对于二叉树而言，遍历是其最重要的操作。常见的遍历方法有： 1234void preOrderTraversal(BinaryTree* tree); //先序遍历，根--左子树--右子树void inOrderTraversal(BinaryTree* tree); //中序遍历，左子树--根--右子树void postOrderTraversal(BinaryTree* tree); //后序遍历，左子树--右子树--根void levelOrderTraversal(BinaryTree* tree); //层次遍历，从上到下，从左到右 储存结构 顺序结构 对于完全二叉树，对其按从上到下，从左到右的顺序进行编号： 如图不难看出，父节点的编号为 i 时，其左子节点的编号为 2i ，其右子节点的编号为 2i+1 ，父节点的编号是子节点编号除以 2 的向下取整。 对于这样的二叉树，我们可以定义一个长度合适的数组来储存数的数据，由于其节点编号的规律性，树的遍历等操作不会困难。 节点 A B O C S M Q W K 编号 1 2 3 4 5 6 7 8 9 对于一般二叉树，也可以用这样的方式储存。不过首先需要把非完全二叉树用空节点变为完全二叉树： 用数组储存为： 节点 A B O null null M null null null null null null C 编号 1 2 3 4 5 6 7 8 9 10 11 12 13 可见，数组结构的二叉树会造成一定的空间浪费。 链式结构 将节点设计成如图样式，内部储存节点数据和指向左右子节点的指针。 12345typedef struct binaryTree&#123; ElementType data; BinaryTree* left; BinaryTree* right;&#125;BinaryTree;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"二叉树","slug":"二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"数据结构--树和树的表示","slug":"数据结构-树和树的表示","date":"2022-03-06T08:53:29.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"数据结构-树和树的表示/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%91%E5%92%8C%E6%A0%91%E7%9A%84%E8%A1%A8%E7%A4%BA/","excerpt":"本文写于 2020 年 3 月 23 日，2022 年 3 月 6 日重新整理","text":"本文写于 2020 年 3 月 23 日，2022 年 3 月 6 日重新整理 写在前面 客观世界中许多事物存在层次关系，如人类的族谱，社会组织的结构等。这些分层次的结构在管理方面具有一定的效率优势，树就是类似的一种具有层次结构关系的数据结构。 二分查找 二分查找 binarySearch 作为一种时间复杂度为的优秀查找算法，其在数据管理的查询方面具有重要地位。二分查找接受被查找元素所属的序列 list （有序的，假设为升序）和被查找元素的值 target 两个参数，它的基本流程如下： 定义两个变量 left 和 right 指向序列的左端和右端，变量 mid 指向两端的中间位置，查找将在这两端之间进行。 初始化 left = 1 ， right = list.length ， mid = (left + right) / 2 。 比较 mid 处的元素与待查找元素 target 之间的关系： 若 list[mid] = target ，说明在 mid 处找到了元素 target ，返回 mid 。 若 list[mid] &lt; target ，说明 target 在 mid 的右侧，这时应把 left 右移为 mid + 1 ，然后更新 mid 为 (left + right) / 2 ，重复过程 3。 若 list[mid] &gt; target ，说明 target 在 mid 的左侧，这时应把 right 左移到 mid - 1 ，然后更新 mid 为 (left + right) / 2 ，重复过程 3。 最后若始终未从过程3.1跳出，则会导致左右交叉即 left &gt; right ，此时说明元素未查找到，应退出程序。 算法C语言代码（伪）如下： 1234567891011121314151617int binarySearch(ElementType *list, int listLength, ElementType target)&#123; int left = 0; int right = listLength - 1; int mid = 0; while (left &lt;= right) &#123; mid = (left + right) / 2; if (list[mid] == target) return mid; else if (list[mid] &gt; target) right = mid - 1; else left = mid + 1; &#125; return -1; //means not frond!&#125; 二分查找算法之所以能获得较高的时间效率，原因之一就是它的搜索过程实际上是一颗搜索树。例如序列 [1,2,3,4,5,6,7,8,9,10,11] ，按照二分法建立树状结构如下： 则对于任何一个元素的搜索次数都将不超过树的深度 4 。而对于一颗有 n 个节点的树（长度为 n 的序列），其搜索树的深度为 log⁡2n+1\\log_2 n + 1log2​n+1 ，因此二分搜索的最坏时间复杂度是 O(log⁡2n)O(\\log_2 n)O(log2​n) 定义 树是由 n 个元素构成的有穷集合。当 n = 0 时，称为空树。对于任何一颗非空树，它具有以下性质： 一颗树有一个根节点 root 其余节点可分为m个互不相交的有限集，其中每个集合本身又可以视为一颗树，称为原树的子树 SubTree 。 术语解释 树的一些基本术语： 节点的度 Degree ：节点的子树个数 树的度：树的所有节点中最大的度数 叶节点 Leaf ：度为 0 的节点 父节点 Parent ：有子树的节点是其子树根节点的父节点 子节点 Child ：若 A 是 B 的父节点，则 B 是 A 的子节点 兄弟节点 Sibling ：具有同一父节点的各节点彼此之间是兄弟节点 路径和路径长度：从节点到的路径是一个节点序列，其中是的父节点。路径所包含的边的条数是路径长度 祖先节点 Ancestor ：沿着从根节点到某一节点的路径所包含的所有节点都是节点的祖先节点 子孙节点 Descendant ：某一节点的子树中的所有节点都是这个节点的子孙节点 节点的层次 Level ：规定根节点在 1 层，其他节点的层次为其父节点层次 +1 树的深度 Depth ：树中所有节点的最大层次称为树的深度 树的表示 对于下图所示的树： 可以用链表表示节点： 问题：节点规格不统一，有的节点度为 1 ，有的节点度为 3 。为了统一而采用树的度作为节点中需要保存的子节点个数过于浪费空间，可采用儿子-兄弟表示法 如图所示采用每个节点中保存一个子节点和相邻的兄弟节点的结构： 节点结构如下图： 这样的结构保证了节点的统一也保证了空间的利用效率，由于每个节点结构上连接其他两个节点，也称为二叉树。","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"树","slug":"树","permalink":"https://blog.zhuwenq.cc/tags/%E6%A0%91/"}]},{"title":"数据结构--队列","slug":"数据结构-队列","date":"2022-03-06T08:44:14.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"数据结构-队列/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%98%9F%E5%88%97/","excerpt":"本文写于 2020 年 3 月 21 日，2022 年 3 月 6 日重新整理","text":"本文写于 2020 年 3 月 21 日，2022 年 3 月 6 日重新整理 写在前面 队列 (Queue) 也是线性表中的一种重要数据结构，队列中的元素只能在队列的头部 (front) 出队 (delete) ，只能在队列的尾部 (rear) 入队 (add) 。因此它是一种先入先出 (First In First Out) 的数据结构。 定义 操作集 一个队列应有以下操作： 12345Queue *createQueue(); //生成队列int isFull(Queue *queue); //查询队列是否为满队列void add(Queue *queue, ElementType item); //入队int isEmpty(Queue *queue); //查询队列是否为空ElementType delete (Queue *queue); //出队 实现 链式结构 链式结构的队列由两个节点域组成，它们分别是 front 和 rear 节点，指向链表头和链表尾。 0. 原型 1234567891011typedef struct queueNode&#123; ElementType data; QueueNode *next;&#125; QueueNode;typedef struct queue_list_based&#123; QueueNode *front; //指向头节点 QueueNode *rear; //指向尾节点&#125; Queue; 1. 生成空队列 生成一个 Queue 结构体，并把它的 front 和 rear 节点赋值为 NULL 。 1234567Queue *createQueue()&#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); queue-&gt;front = NULL; queue-&gt;rear = NULL; return queue;&#125; 2. 查询是否为满队列 链式实现的队列不存在满队列 3. 查询是否为空队列 当且仅当链式实现的队列为空队列时，它的头节点 front 为 NULL 。 1234int isEmpty(Queue *queue)&#123; return queue-&gt;front == NULL;&#125; 4. 入队 入队时，若队列不为空把尾节点的 next 节点赋值为要入队的节点，然后把尾节点移动到新插入的节点，否则，把头节点赋值为新插入的节点。 12345678910111213141516void add(Queue *queue, ElementType item)&#123; QueueNode *node = (QueueNode *)malloc(sizeof(QueueNode)); node-&gt;data = item; node-&gt;next = NULL; if (isEmpty(queue)) &#123; queue-&gt;front = node; queue-&gt;rear = node; &#125; else &#123; queue-&gt;rear-&gt;next = node; queue-&gt;rear = node; &#125;&#125; 5. 出队 出队时，若队列不为空，先把头节点保存下来，然后把头节点后移，最后删除并返回原头节点。 1234567891011121314151617181920ElementType delete (Queue *queue)&#123; if (isEmpty(queue)) &#123; return NULL; //Error!queue is Empty &#125; QueueNode *frontNode = queue-&gt;front; if (queue-&gt;front == queue-&gt;rear) &#123; //if queue has just one element,make it empty. queue-&gt;front = NULL; queue-&gt;rear = NULL; &#125; else &#123; queue-&gt;front = queue-&gt;front-&gt;next; &#125; ElementType frontNodeValue = frontNode-&gt;data; free(frontNode); return frontNodeValue;&#125; 顺序结构 0. 原型 顺序结构的队列使用一个数组存放数据，它的内部维护两个整形变量 front 和 rear 分别指示队列头部和尾部节点的位置。 由于队列的入队和出队的操作将改变队列元素在内部数组中的起始和终止位置，因此顺序结构的队列常常是一个循环队列，即内部数组循环使用，数组下标对数组长度取模，这样可以提高空间的利用效率。 容易知道，当队列的 front 和 rear 值相同时，队列是空的，同时，当队列处于满的状态时， front 和 rear 也是相同的，这种情况将使我们无法分辨队列的满空状态。 这种情况的出现是必然的，比如对于一个长度为 10 的队列，它的内部可以有 0,1,2,3...10 个元素，即 11 个状态，我们使用头尾差值即 front - rear 的值表征这 11 中状态。然而 front - rear 只有 10 种可能的结果，无法表征 11 种状态。 这种矛盾的解决方案十分简单，我们可以在队列的内部增加一个 counter 计数器来记录队列内部的元素个数。或者我们可以让头节点 front 指向队列中第一个元素的前一位置，这样，队列内部实际只能储存数组长度减一个元素，头尾节点便不会因为队列满而相遇了。 123456typedef struct queue_array_based&#123; ElementType *data; int front; int rear;&#125;Queue; 1. 生成队列 生成最大长度为 maxSize - 1 的队列，并把 front,rear 赋值为 0 ，标志为空队列。 12345678Queue *createQueue(const int maxSize)&#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); queue-&gt;data = (ElementType *)malloc(maxSize * sizeof(ElementType)); queue-&gt;front = 0; queue-&gt;rear = 0; return queue;&#125; 2. 查询队列是否为满队列 当且仅当队列满时， rear 和 front 必定是相邻位置，且 rear 在 front 后面，因此，通过判断 front + 1 与数组长度的余与 rear 比较，可以获知队列是否已满。 1234int isFull(Queue *queue, const int maxSize)&#123; return ((queue-&gt;front + 1) % maxSize) == (queue-&gt;rear);&#125; 3. 查询队列是否为空 当且仅当队列空时， front 和 rear 的值相等。 1234int isEmpty(Queue *queue)&#123; return queue-&gt;front == queue-&gt;rear;&#125; 4. 入队 入队时，若队列未满， rear 的值变为 rear + 1 对数组长度的取余，然后把入对的元素放在 rear 的位置。 123456789void add(Queue *queue, ElementType item, const int maxSize)&#123; if (isFull(queue, maxSize)) &#123; return; //Error!queue is Full. &#125; queue-&gt;rear = (queue-&gt;rear + 1) % maxSize; queue-&gt;data[queue-&gt;rear] = item;&#125; 5. 出队 出队时，若队列非空，将 front 移到 front + 1 对数组长度取余的位置，然后返回 front 位置的元素。 123456789ElementType delete (Queue *queue, const int maxSize)&#123; if (isEmpty(queue)) &#123; return NULL; //Error!queue is Empty. &#125; queue-&gt;front = (queue-&gt;front + 1) % maxSize; return queue-&gt;data[queue-&gt;front];&#125;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"队列","slug":"队列","permalink":"https://blog.zhuwenq.cc/tags/%E9%98%9F%E5%88%97/"}]},{"title":"A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models","slug":"A-Simple-but-Effective-Pluggable-Entity-Lookup-Table-for-Pre-trained-Language-Models","date":"2022-03-06T08:35:45.000Z","updated":"2023-06-21T06:51:28.720Z","comments":true,"path":"A-Simple-but-Effective-Pluggable-Entity-Lookup-Table-for-Pre-trained-Language-Models/","link":"","permalink":"https://blog.zhuwenq.cc/A-Simple-but-Effective-Pluggable-Entity-Lookup-Table-for-Pre-trained-Language-Models/","excerpt":"作者提出一种向 PLM 注入实体知识的简单方法：首先利用 PLM 为实体提及生成的表征预构建实体嵌入查找表，并在模型推理时修改嵌入层，将实体提及的嵌入直接替换为查找表中的嵌入，实现无特殊预训练的条件下向 PLM 注入实体知识。","text":"作者提出一种向 PLM 注入实体知识的简单方法：首先利用 PLM 为实体提及生成的表征预构建实体嵌入查找表，并在模型推理时修改嵌入层，将实体提及的嵌入直接替换为查找表中的嵌入，实现无特殊预训练的条件下向 PLM 注入实体知识。 Methodology 构建实体嵌入 作者观察到在 PLM 如 BERT 中，词的嵌入和输出表征实际处于相同的向量空间，因此作者考虑有可能利用输出表征构建嵌入。 对于实体 eee, 其嵌入记作 E(e)\\mathbf{E}(e)E(e). 在实践中，作者按如下过程计算 E(e)\\mathbf{E}(e)E(e): 收集包含 eee 的句子集 SeS_eSe​ 对 SeS_eSe​ 中每个句子，替换 eee 为 [MASK][MASK][MASK] 记 xix_ixi​ 为每个句子中替换的 [MASK][MASK][MASK], E(e)=C⋅∑xi∈Serxi\\mathbf{E}(e) = C\\cdot\\sum_{x_i\\in S_e}\\mathbf{r}_{x_i}E(e)=C⋅∑xi​∈Se​​rxi​​ 其中 CCC 是缩放系数，rxir_{x_i}rxi​​ 是 PLM 为 xix_ixi​ 生成的表征。 (作者给出了为何如此设计的数学上的理由) 同时，作者将 E(e)\\mathbf{E}(e)E(e) 的范数限定为常数 LLL。 将实体嵌入注入 PLM 首先修改输入语句，在输入语句中实体提及的后面加上 “(e)”, 即一对括号和实体 eee，此时，实体 eee 作为词典中存在的 token 出现。 例如对于输入语句：Steve Job works for [MASK]\\text{Steve Job works for [MASK]}Steve Job works for [MASK], 修改后为 \\text{Steve Job(Steven_Job) works for [MASK]}. 随后在嵌入层，被插入的实体 \\text{Steve_Job} 的嵌入将使用查找表中预生成的嵌入，其他 token 使用原来的嵌入。随后将该嵌入直接送入 encoder。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Entity Represention","slug":"Entity-Represention","permalink":"https://blog.zhuwenq.cc/tags/Entity-Represention/"},{"name":"Entity Lookup Table","slug":"Entity-Lookup-Table","permalink":"https://blog.zhuwenq.cc/tags/Entity-Lookup-Table/"}]},{"title":"数据结构--栈","slug":"数据结构-栈","date":"2022-03-05T08:53:20.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"数据结构-栈/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%A0%88/","excerpt":"本文写于 2020 年 3 月 19 日，2022 年 3 月 5 日重新整理","text":"本文写于 2020 年 3 月 19 日，2022 年 3 月 5 日重新整理 后缀表达式 数学中常见的运算表达式如 a+b×c+d/ea + b \\times c + d / ea+b×c+d/e 称为中缀表达式，它的一个特点就是运算符号如 +++，−-−，×\\times×，/// 都是位于运算符的数据中间的。把运算符拿到数据的后面，如 abc×+de/+abc\\times + de / +abc×+de/+ 就称位后缀表达式。在使用计算机计算普通的中缀表达式求值时，一般就需要先把中缀表达式转换为后缀表达式，这是因为后缀表达式包含了表达式中各运算符的优先级信息，计算机解析执行起来逻辑清晰，容易实现。 而在中缀表达式转换为后缀表达式以及计算机解析执行后缀表达式时，就需要栈 (Stack) 这一数据结构。 栈 堆栈是一种只能在一端（栈顶）进行插入(push)，和删除pop操作的线性表，因此他也是一中后入先出(Last In First Out)的数据结构。 操作集 堆栈应具有以下基本操作： 12345Stack *createEmpty(); // 生成空栈int isFull(Stack *stack); // 查询是否满栈int isEmpty(Stack *stack); // 查询是否空栈void push(Stack *stack, ElementType item); // 在栈顶插入元素ElementType pop(Stack *stack); // 删除并返回栈顶元素 实现 链式结构 首先实现数据结构原型： 使用链表储存数据时，链表的头节点指向栈顶，为了便于操作，头节点的next节点指向的是堆栈的第一个元素，即头节点只作为指示作用。 12345#define ElementType inttypedef struct stack &#123; ElementType data; Stack* next;&#125;Stack; 生成空表 创建一个空的链表节点并返回： 12345Stack* createEmpty() &#123; Stack* top = (Stack*)malloc(sizeof(Stack)); top-&gt;next = NULL; return top;&#125; 查询是否满栈 链式结构不存在满栈 查询是否为空栈 由于链式结构的堆栈始终指向头节点，因此只要头节点的next节点不为NULL，栈就不是空的： 123int isEmpty(Stack* stack) &#123; return stack-&gt;next == NULL;&#125; 在栈顶插入元素 创建一个空节点temp，并把空节点的next指针指向头节点top的下一节点，然后把头节点的next指针指向temp。（temp的值初始化为要插入的值item） 123456void push(Stack *stack, ElementType item) &#123; Stack *temp = (Stack *)malloc(sizeof(Stack)); temp-&gt;data = item; temp-&gt;next = stack-&gt;next; stack-&gt;next = temp;&#125; 删除并返回栈顶元素 如果栈非空，首先把栈顶元素(top节点的next指针指向的节点的值targetValue)储存下来，然后删除栈顶节点，返回targetValue。 1234567891011ElementType pop(Stack *stack) &#123; if (isEmpty(stack)) &#123; return; //error!Empty stack &#125; ElementType targetValue = stack-&gt;next-&gt;data; Stack *target = stack-&gt;next; stack-&gt;next = target-&gt;next; free(target); return targetValue;&#125; 顺序结构（数组） 首先实现数据结构原型 在顺序结构中，由于数据在数组中储存，需要使用数组下标访问元素。因此需要一个下标始终指向栈顶。同时堆栈在创建时也需要指定最大的空间大小MaxSize. 12345typedef struct stack&#123; ElementType *data; int top;&#125;Stack; 生成空表 由于需要指定堆栈最大长度，因此需要有一个参数传入。同时，堆栈的内存空间将在堆栈创建时分配完成。 堆栈中指示栈顶位置的变量将被初始化为-1，它代表这是一个空栈。 1234567Stack *createEmpty(const int maxSize)&#123; ElementType *data = (ElementType *)malloc(maxSize * sizeof(ElementType)); Stack *stack = (Stack *)malloc(sizeof(Stack)); stack-&gt;data = data; stack-&gt;top = -1;//means an Empty Stack&#125; 查询是否满栈 由于数组实现的堆栈中不含堆栈长度的信息，因此被查询堆栈的大小需要显式传参。通过比交传入的大小数据和栈顶位置top，可以轻易得到堆栈是否满栈。 1234int isFull(Stack *stack, int maxSize)&#123; return stack-&gt;top &gt;= maxSize;&#125; 查询是否为空栈 当为空栈时，top值为-1 int isEmpty(Stack *stack) { return stack-&gt;top == -1; } 在栈顶插入元素 在栈未满的情况下，push操作可以简单地把top指示器+1并把栈顶赋值为要插入的元素完成： 12345678void push(Stack *stack, ElementType item, const int maxSize)&#123; if (isFull(stack, maxSize)) &#123; return; //Error!stack overflow &#125; stack-&gt;data[stack-&gt;top++] = item;&#125; 删除并返回栈顶元素 栈不为空时，删除操作可以简单地把top指示器-1完成，此时虽然原栈顶元素仍在data数组中，但是由于栈顶指示器已经下移，原栈顶地址处于可擦写的状态，相当于删除。并且由于栈的内存空间在栈被构建时已经申请完毕，在栈上进行的操作将不会影响栈所占用的内存大小。 12345678ElementType pop(Stack *stack)&#123; if (isEmpty(stack)) &#123; return NULL; //Error!stack is Empty &#125; return stack-&gt;data[stack-&gt;top--];&#125; 最后 栈的两种C语言（伪）实现大致如上，下面看看后缀表达式如何工作的。 转换中缀表达式为后缀表达式 首先建里堆栈用于储存运算符，然后顺序读取表达式每个对象（数值，符号等），对于每个对象，按如下法则处理： 若对象是运算数值，直接输出 若对象是左括号'('，压入(push)堆栈 若对象是右括号')'，将栈内的运算符挨个弹出，直到遇到左括号'(' 若对象是运算符，将其与栈顶运算符比较： 若优先级大于栈顶运算符，则把该运算符压栈 若优先级小于栈顶运算符，则把栈顶运算符弹出，继续与新的栈顶运算符比较，直到其优先级大于栈顶运算符为止。 若表达式中的对象读取完毕，挨个弹出栈内所有运算符。 这样就把一个中缀表达式转换为了后缀表达式，看这个例子： 计算后缀表达式 首先建立堆栈用于储存计算结果，然后顺序读取后缀表达式每个对象，并按如下法则处理： 如果对象是运算数，压栈 如果对象是操作符，弹出栈内的两个元素按该操作符计算，并把结果压栈 处理完所有对象后，栈顶的对象就是运算结果","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"栈","slug":"栈","permalink":"https://blog.zhuwenq.cc/tags/%E6%A0%88/"}]},{"title":"数据结构--线性表","slug":"数据结构-线性表","date":"2022-03-05T08:39:45.000Z","updated":"2023-06-21T06:51:28.944Z","comments":true,"path":"数据结构-线性表/","link":"","permalink":"https://blog.zhuwenq.cc/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%BF%E6%80%A7%E8%A1%A8/","excerpt":"本文写于 2020 年 3 月 19 日，2022 年 3 月 5 日重新整理","text":"本文写于 2020 年 3 月 19 日，2022 年 3 月 5 日重新整理 定义 线性表是由同类型的数据元素构成的有序序列数据结构，表中的元素个数称为线性表的长度，无元素的线性表称为空表，表起始位置称为表头，结束位置称位表尾。 线性表的ADT描述如下： 类型名: 线性表 数据对象集: 由 n(n&gt;=0)n(n &gt;= 0)n(n&gt;=0) 个元素构成的有序序列 (a1,a2,...,an)(a_1, a_2, ..., a_n)(a1​,a2​,...,an​) 操作集: 123456List makeEmptyList(); // 创建空表ElementType findKth(const int K,List list); // 找到第K个元素int find(const ElementType ele,List list); // 找到元素ele第一次出现的位置void insert(ElementType ele,const int position,List list); // 把元素ele插入position位置的前面void delete(const int position,List list); // 删除位置position处的元素int length(); // 返回线性表的长度 实现 线性表内部可以使用顺序储存的数组实现，或者使用链表实现。 链表实现 原型 12345678#define ElementType intList *makeEmptyList(); // 创建空表List *findKth(const int K, List *list); // 找到第K个元素int find(const ElementType ele, List *list); // 找到元素ele第一次出现的位置List *insert(ElementType ele, const int position, List *list); // 把元素ele插入position位置的前面List *delete (const int position, List *list); // 删除位置position处的元素int length(List *list); 创建空表 12345List *makeEmptyList()&#123; List *list = (List *)malloc(sizeof(List)); return list;&#125; 查找第K个元素 1234567891011List *findKth(const int K, List *list)&#123; List *listCopy = list; //make a copy int i = 0; while (listCopy &amp;&amp; i &lt; K) &#123; i++; listCopy = listCopy-&gt;next; &#125; return listCopy;&#125; 按值查找 123456789int find(const ElementType ele, List *list)&#123; List *listCopy = list; while (listCopy &amp;&amp; listCopy-&gt;data != ele) &#123; listCopy = listCopy-&gt;next; &#125; return listCopy;&#125; 插入 1234567891011121314151617181920List *insert(ElementType ele, const int position, List *list)&#123; if (position == 1) &#123; List *n = makeEmptyList(); n-&gt;data = ele; n-&gt;next = list; return n; &#125; List *target = findKth(position - 1, list); if (target == NULL) &#123; return NULL; //error &#125; List *n = makeEmptyList(); n-&gt;data = ele; n-&gt;next = target-&gt;next; target-&gt;next = n; return list;&#125; 删除节点 123456789101112131415161718192021222324252627282930List *delete (const int position, List *list)&#123; if (position == 1) &#123; if (list == NULL) return NULL; else &#123; List *copy = list; list = list-&gt;next; free(copy); return list; &#125; &#125; List *target = findKth(position - 1, list); List *lastNode = target; if (target == NULL) &#123; return; &#125; else if (target-&gt;next == NULL) &#123; return; &#125; target = target-&gt;next; lastNode-&gt;next = target-&gt;next; free(target); return list;&#125; 获取长度 1234567891011int length(List *list)&#123; int i = 0; List *listCopy = list; while (listCopy) &#123; listCopy = listCopy-&gt;next; i++; &#125; return i;&#125;","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"K-Adapter: Infusing Knowledge into Pre-Training Models with Adapters","slug":"K-Adapter-Infusing-Knowledge-into-Pre-Trained-Models-with-Adapters","date":"2022-02-28T16:00:00.000Z","updated":"2023-06-21T06:51:28.796Z","comments":true,"path":"K-Adapter-Infusing-Knowledge-into-Pre-Trained-Models-with-Adapters/","link":"","permalink":"https://blog.zhuwenq.cc/K-Adapter-Infusing-Knowledge-into-Pre-Trained-Models-with-Adapters/","excerpt":"文章主要研究向预训练语言模型中注入知识的方法，现存的工作主要是通过更新预训练模型参数的方式向模型注入知识（即 fine-tune 的方式）。但是这种参数更新过程会将整个模型的参数向新注入的知识上偏移，导致之前注入的知识被削弱或冲刷。作者提出的 K-Adapter 结构，通过将不同的知识分别注入到不同的 Adapter 中实现知识的持续注入，同时 Adapter 的参数量远小于预训练模型，在知识注入的过程中更加经济。","text":"文章主要研究向预训练语言模型中注入知识的方法，现存的工作主要是通过更新预训练模型参数的方式向模型注入知识（即 fine-tune 的方式）。但是这种参数更新过程会将整个模型的参数向新注入的知识上偏移，导致之前注入的知识被削弱或冲刷。作者提出的 K-Adapter 结构，通过将不同的知识分别注入到不同的 Adapter 中实现知识的持续注入，同时 Adapter 的参数量远小于预训练模型，在知识注入的过程中更加经济。 K-Adapter K-Adapter 由一个 Transformer Encoder 模型和一个 Adapter 模型组成，Adapter 层的输入来自 Transformer Encoder 模型中间隐藏层的输出向量和上一层 Adapter 的输出向量的拼接。模型的最终输出由 Adapter 的输出向量和 Transformer Encoder 模型的输出向量拼接。本文中作者选用 RoBERTa 作为 Transformer Encoder。 Adapter 与 Houlsby et al.[1] 在每一层 Transformer 层上添加 adapter 层的做法不同，作者提出的 K-Adapter 以外挂的形式工作。K-Adapter 模型中的 Adapter 模型有 K 个 Adapter 层，每个 Adapter 层中有 N 个 transformer 层和两个投影层，同时每层 Adapter 层中有一个残差连接。Adapter 层的结构如下图。 K-Adapter 中的 K 个 Adapter 层被插在 RoBERTa（代指 pre-trained model） 的 K 个不同的 Transformer 层上。每个 Adapter 层的输入由 transformer 层的输出向量和上一个 adapter 层的输出向量的拼接而成，整个 K-Adapter 模型的输出向量则由 RoBERTa 和 Adapter 的最终隐藏层向量拼接而成。 在预训练过程中，作者通过不同的预训练任务分别训练不同的 Adapter 模型，以实现不同知识的注入。在下游任务的微调时，使用的所有 Adapter 的输出向量拼接构成下游任务的输入。 预训练设置 实验中，作者使用的 transformer encoder 模型是 RoBERTaLARGE(L=24,H=1024,A=16,335M params\\text{RoBERTa}_{LARGE}(L=24, H=1024, A=16, 335M \\text{ params}RoBERTaLARGE​(L=24,H=1024,A=16,335M params, 记使用的 Adapter 模型每层中的 transformer 层数为 NNN, transformer 层的隐藏层维度为 HAH_AHA​, AAA_AAA​ 个 self-attention heads, 下投影和上投影层的维度分别为 H−d,HuH-d, H_uH−d,Hu​. 实验中采用的具体参数为：N=2,HA=768,AA=12,Hu=1024,Hd=768N=2, H_A=768, A_A=12, H_u=1024, H_d=768N=2,HA​=768,AA​=12,Hu​=1024,Hd​=768，adapter 层插入在 RoBERTa 的第 {0,11,23}\\{0, 11, 23\\}{0,11,23} 层。Adapter 模型的参数量总共为 42M。在预训练时，RoBERTa 的参数保持不变，仅更新 Adapter 的参数。作者在实验中设计了 Factual Adapter 和 Linguistic Adapter 分别注入事实知识和语法知识。 Factual Adapter 作者使用从 T-REx[2] 数据集中提取的子集 T-REx-rc 作为数据集，共包含 430 种关系和 5.5M 个句子。作者采用 relation classification 任务训练该 Adapter，具体地说，Adapter 和 RoBERTa 的输出向量拼接后作为关系分类层的输入，并且在输入实体的表征上使用了池化层。 Linguistic Adapter 语言知识包括语法，语义知识等，作者通过自然语言文本中词之间的依存关系提取语言知识。作者在 BooK Corpus[3] 数据集的一部分上使用现成的依存解析器构建了一个包含 1M 个样本的依存关系预测数据集。并在该数据集上进行依存关系预测任务，具体地说即预测输入句子中每个词的 head index。作者将 RoBERTa 和 Adapter 的输出向量拼接后经过一个线性分类层进行每个词的分类。 实验 作者在 entity typing，question answering 和 relation classification 等任务上对 K-Adapter 进行了评估。 Parameter-Efficient Transfer Learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan and Sylvain Gelly. [ICML 2019] ↩︎ T-REx: A Large Scale Alignment of Natual Language with Knowledge Base Triples. Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe and Elena Simperl. [LERC 2018] ↩︎ Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba and Sanja Fidler. [ICCV 2015] ↩︎","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Adapter","slug":"Adapter","permalink":"https://blog.zhuwenq.cc/tags/Adapter/"},{"name":"Knowledge","slug":"Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Knowledge/"}],"author":["Ruize Wang","Duyu Tang","Nan Duan","Zhongyu Wei","Xuanjing Huang","Jianshu Ji","Guihong Cao","Daxin Jiang","Ming Zhou"]},{"title":"LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention","slug":"LUKE-Deep-Contextualized-Entity-Representations-with-Entity-aware-Self-attention","date":"2022-02-25T16:00:00.000Z","updated":"2023-06-21T06:51:28.800Z","comments":true,"path":"LUKE-Deep-Contextualized-Entity-Representations-with-Entity-aware-Self-attention/","link":"","permalink":"https://blog.zhuwenq.cc/LUKE-Deep-Contextualized-Entity-Representations-with-Entity-aware-Self-attention/","excerpt":"作者提出一种实体表征方法，通过在预训练 Transformer Encoder 模型时将实体看作独立的 token 来为实体生成表征。作者提出类似于 MLM 的预训练目标，首先将语料中的实体文本提取并放在语句末端，然后以一定的比例将实体替换为 [MASK]\\text{[MASK]}[MASK] ，并训练模型预测被遮罩的实体。为了提升效果，作者为放在语句末端的实体设计了特殊的实体 positional embedding，同时在 Encoder 模型的基础上添加 entity-aware self-attention mechanism 来增强模型对 token type(entity or word) 的感知。作者在多个数据集多个实体相关的任务上进行了实验。","text":"作者提出一种实体表征方法，通过在预训练 Transformer Encoder 模型时将实体看作独立的 token 来为实体生成表征。作者提出类似于 MLM 的预训练目标，首先将语料中的实体文本提取并放在语句末端，然后以一定的比例将实体替换为 [MASK]\\text{[MASK]}[MASK] ，并训练模型预测被遮罩的实体。为了提升效果，作者为放在语句末端的实体设计了特殊的实体 positional embedding，同时在 Encoder 模型的基础上添加 entity-aware self-attention mechanism 来增强模型对 token type(entity or word) 的感知。作者在多个数据集多个实体相关的任务上进行了实验。 Introduction BERT, RoBERTa 等 Transformer Encoder 模型通过上下文词表征(Contextualized Word Representation, CWR)的方式提供了通用语言表征, 由于 CWR 是对 word 生成表征，无法对 span-level 的实体生成表征，通常需要在下游任务的小数据集上训练如何根据 word representation 生成 entity representation，CWR 在实体相关的任务上还需进一步探索。 本文提出的 LUKE 即利用 Transformer Encoder 模型为实体生成上下文表征，解决实体相关的问题。 LUKE 给定一个包含 mmm 个字 w1,w2,...,wmw_1, w_2, ..., w_mw1​,w2​,...,wm​ 和 nnn 个实体 e1,e2,...,ene_1, e_2, ..., e_ne1​,e2​,...,en​ 的序列，LUKE 为每个字计算一个 DDD 维的词表征 hw1,hw2,...,hwm\\mathbf{h}_{w_1}, \\mathbf{h}_{w_2}, ..., \\mathbf{h}_{w_m}hw1​​,hw2​​,...,hwm​​, 其中 hwi∈RD\\mathbf{h}_{w_i} \\in \\mathbb{R}^Dhwi​​∈RD, 为每个实体计算 DDD 维实体表征 he1,he2,...,hen\\mathbf{h}_{e_1}, \\mathbf{h}_{e_2}, ..., \\mathbf{h}_{e_n}he1​​,he2​​,...,hen​​, 其中 hei∈RD\\mathbf{h}_{e_i} \\in \\mathbb{R}^Dhei​​∈RD Input Represention 如上图所示，输入文本的 embedding 由以下三个 embedding 计算： Token Embedding：代表对应的词，词典中所有词的表征可记为 A∈RVw×D\\mathbf{A}\\in\\mathbb{R}^{V_w\\times D}A∈RVw​×D, 其中 VwV_wVw​ 是词典大小。由于实体数目太多，出于计算效率考虑，实体的 token embedding 被分解为两个小的矩阵 B∈RVe×H\\mathbf{B}\\in\\mathbb{R}^{V_e\\times H}B∈RVe​×H 和 U∈RH×D\\mathbf{U}\\in\\mathbb{R}^{H\\times D}U∈RH×D, 其中 VeV_eVe​ 是词典中的实体数量。 Position Embedding: 代表 token 在输入序列中的位置。在序列中出现在第 iii 位置的普通词和实体词的 position embedding 分别记作 Ci∈RD\\mathbf{C}_i\\in\\mathbb{R}^DCi​∈RD 和 Di∈RD\\mathbf{D}_i\\in\\mathbb{R}^DDi​∈RD. 对于实体，如果其包含多个词，则该实体的 position embedding 由所有词的 embedding 平均值计算：ei={wj,...,wk},Di=Σl=jkDlk−j+1e_i = \\{w_j, ..., w_k\\}, \\mathbf{D}_i = \\frac{\\Sigma_{l = j}^k \\mathbf{D}_l}{k - j + 1}ei​={wj​,...,wk​},Di​=k−j+1Σl=jk​Dl​​. Entity type embedding: 代表输入序列中的 token 是否是一个实体，entity type embedding 是一个向量 e∈RD\\mathbf{e}\\in\\mathbb{R}^De∈RD Entity-aware Self-attention 在 Transformer Encoder 的自注意力机制中，第 iii 个 token 的输入嵌入记作 xi\\mathbf{x}_ixi​, 输出表征记作 yi\\mathbf{y}_iyi​, 则其在自注意层的计算过程为： yi=∑j=1kαijVxjαij=softmax(eij)eij=KxjTQxiL\\begin{aligned} \\mathbf{y}_i &amp;= \\sum_{j = 1}^k\\alpha_{ij}\\mathbf{V}\\mathbf{x}_j\\\\ \\alpha_{ij} &amp;= \\text{softmax}(e_{ij})\\\\ e_{ij} &amp;= \\frac{\\mathbf{Kx}_j^T\\mathbf{Qx}_i}{\\sqrt{L}} \\end{aligned}yi​αij​eij​​=j=1∑k​αij​Vxj​=softmax(eij​)=L​KxjT​Qxi​​​ 其中 Q∈RL×D,K∈RL×D,V∈RL×D\\mathbf{Q}\\in\\mathbb{R}^{L\\times D}, \\mathbf{K}\\in\\mathbb{R}^{L\\times D}, \\mathbf{V}\\in\\mathbb{R}^{L\\times D}Q∈RL×D,K∈RL×D,V∈RL×D 是自注意力机制中的 query, key 和 value 矩阵。 LUKE 的输入 token 分为普通词和实体两类，在自注意力机制计算注意力分数 eije_{ij}eij​ 时，作者认为结合第 i,ji, ji,j 个 token 的类型是有益的。作者由此提出 entity-aware query mechanism，对于 i,ji, ji,j 个 token 的类型的不同组合，使用不同的 query matrix 进行计算： eij={KxjTQxi,if both xi,xj are wordsKxjTQw2exi,if xi is word and xj is entityKxjTQe2wxi,if xi is entity and xj is wordKxjTQe2exi,if both xi,xj are entitiese_{ij} = \\begin{cases} \\mathbf{Kx}_j^T\\mathbf{Qx}_i, &amp;\\text{if both }\\mathbf{x}_i, \\mathbf{x}_j \\text{ are words}\\\\ \\mathbf{Kx}_j^T\\mathbf{Q}_{w2e}\\mathbf{x}_i, &amp;\\text{if }\\mathbf{x}_i \\text{ is word and }\\mathbf{x}_j \\text{ is entity}\\\\ \\mathbf{Kx}_j^T\\mathbf{Q}_{e2w}\\mathbf{x}_i, &amp;\\text{if }\\mathbf{x}_i \\text{ is entity and }\\mathbf{x}_j \\text{ is word}\\\\ \\mathbf{Kx}_j^T\\mathbf{Q}_{e2e}\\mathbf{x}_i, &amp;\\text{if both }\\mathbf{x}_i, \\mathbf{x}_j \\text{ are entities} \\end{cases}eij​=⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​KxjT​Qxi​,KxjT​Qw2e​xi​,KxjT​Qe2w​xi​,KxjT​Qe2e​xi​,​if both xi​,xj​ are wordsif xi​ is word and xj​ is entityif xi​ is entity and xj​ is wordif both xi​,xj​ are entities​ 即在计算第 iii 个 token 对第 jjj 个 token 的注意力分数时，考虑它们分别的类型并作不同的计算。 Pretraining 作者在预训练中使用 Wikipedia 数据作为语料，将 Wikipedia 文档中所有带有超链接的文本片段看作实体，由此对数据进行实体标注。训练时，作者首先将文本中的实体标注复制到文本末尾，然后以一定概率遮罩这些实体。在训练模型预测被遮住的实体时，作者是通过将实体表征映射到实体词典大小的维度，然后在整个实体词典上进行 softmax 获得概率分布： y^=softmax(BTm+bo)m=layer_norm(gelu(Whhe+bh))\\begin{aligned} \\hat{\\mathbf{y}} &amp;= \\text{softmax}(\\mathbf{BTm} + \\mathbf{b}_o)\\\\ \\mathbf{m} &amp;= \\text{layer\\_norm}(\\text{gelu}(\\mathbf{W}_h\\mathbf{h}_e + \\mathbf{b}_h)) \\end{aligned}y^​m​=softmax(BTm+bo​)=layer_norm(gelu(Wh​he​+bh​))​ 其中 he\\mathbf{h}_ehe​ 是被遮住的实体的表征，T∈RH×D,Wh∈RD×D\\mathbf{T}\\in\\mathbb{R}^{H\\times D}, \\mathbf{W}_h\\in\\mathbb{R}^{D\\times D}T∈RH×D,Wh​∈RD×D 是投影矩阵，bo∈RVe,bh∈RD\\mathbf{b}_o\\in\\mathbb{R}^{V_e}, \\mathbf{b}_h\\in\\mathbb{R}^Dbo​∈RVe​,bh​∈RD 是偏置向量。 预训练的损失函数是 MLM 任务的损失和预测遮罩实体的交叉熵损失之和（两者形式一致，计算过程也一致）。 Experiment 作者利用新提出的表征在 entity typing(Open Entity Dataset[1]), relation classification(TACRED dataset[2]), NER(CoNLL-2003 dataset[3]), cloze-style QA(ReCoRD dataset[4]) 和 extractive QA(SQuAD 1.1 dataset[5]) 任务上做了实验，使用的模型结构类似：在表征后添加简单的线性分类层。实验的模型结构和细节值得参考。 Ultra-Fine Entity Typing. Eunsol Choi, Omer Levy, Yejin Choi and Luke Zettlemoyer. [ACL 2018] ↩︎ Positionaware Attention and Supervised Data Improve Slot Filling. Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D Manning. [EMNLP 2017] ↩︎ Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. Erik F. Tjong Kim Sang and Fien De Meulder. [NAACL 2003] ↩︎ ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. [arXiv: 1810.12885v1] ↩︎ SQuAD: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. [EMNLP 2016] ↩︎","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Represention","slug":"Knowledge-Represention","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Represention/"}],"author":["Ikuya Yamada","Akari Asai","Hiroyuki Shindo","Hideaki Takeda","Yuji Matsumoto"]},{"title":"Case-Based Reasoning for Natural Language Queries over Knowledge Bases","slug":"Case-Based-Reasoning-for-Natural-Language-Queries-over-Knowledge-Bases","date":"2022-02-23T16:00:00.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"Case-Based-Reasoning-for-Natural-Language-Queries-over-Knowledge-Bases/","link":"","permalink":"https://blog.zhuwenq.cc/Case-Based-Reasoning-for-Natural-Language-Queries-over-Knowledge-Bases/","excerpt":"作者提出一种 基于案例的推理(Case-Based Reasoning, CBR) 解决 KBQA 问题的模型 CBR-KBQA。文章关注于语义解析方式，根据自然语言问题文本生成可执行的逻辑表达式 (SparQL)，并在知识图谱上执行该逻辑表达式获得答案。 CBR-KBQA 由三部分组成： Retrieval Module Reuse Module Revise Module 首先通过 Retrieval Module 选择相似案例，该模块维护一个案例库，并根据问题中的关系的相似度选择相关案例。 随后在逻辑表达式的生成上，作者采用类似于 Prompt 的思想。首先采用一个 seq2seq 模型，将自然语言问题和案例库中与该问题相似的案例(自然语言问题+可执行逻辑表达式)拼接作为模型的输入，训练模型结合案例生成新问题的逻辑表达式，实现对案例的复用。 由于 Reuse Module 根据案例生成新的逻辑表达式，生成结果中的关系取自过往案例，同时知识图谱通常是不完整的，这就导致生成结果中的关系在知识图谱中不存在，导致逻辑表达式无法执行。因此作者提出一个 Revise Module，对生成结果中的关系进行修正。该模块通过将生成结果中的关系对齐到知识图谱中存在的最相似的关系实现对结果的修正。 由于 CBR-KBQA 根据案例库中的过往案例为新问题生成逻辑表达式，CBR-KBQA 可以通过向案例库添加新案例的方式泛化到训练过程中从未见过的关系上。","text":"作者提出一种 基于案例的推理(Case-Based Reasoning, CBR) 解决 KBQA 问题的模型 CBR-KBQA。文章关注于语义解析方式，根据自然语言问题文本生成可执行的逻辑表达式 (SparQL)，并在知识图谱上执行该逻辑表达式获得答案。 CBR-KBQA 由三部分组成： Retrieval Module Reuse Module Revise Module 首先通过 Retrieval Module 选择相似案例，该模块维护一个案例库，并根据问题中的关系的相似度选择相关案例。 随后在逻辑表达式的生成上，作者采用类似于 Prompt 的思想。首先采用一个 seq2seq 模型，将自然语言问题和案例库中与该问题相似的案例(自然语言问题+可执行逻辑表达式)拼接作为模型的输入，训练模型结合案例生成新问题的逻辑表达式，实现对案例的复用。 由于 Reuse Module 根据案例生成新的逻辑表达式，生成结果中的关系取自过往案例，同时知识图谱通常是不完整的，这就导致生成结果中的关系在知识图谱中不存在，导致逻辑表达式无法执行。因此作者提出一个 Revise Module，对生成结果中的关系进行修正。该模块通过将生成结果中的关系对齐到知识图谱中存在的最相似的关系实现对结果的修正。 由于 CBR-KBQA 根据案例库中的过往案例为新问题生成逻辑表达式，CBR-KBQA 可以通过向案例库添加新案例的方式泛化到训练过程中从未见过的关系上。 CBR-KBQA 案例：案例定义为一条自然语言问题文本与它相应的可执行逻辑表达式文本组成的文本对，作者主要关注于 SPARQL 这种逻辑表达式形式。 记自然语言问题文本为 qqq, KB 为 K\\mathcal{K}K，则 KBQA 模型需要根据 qqq 在 K\\mathcal{K}K 中查询出一个答案列表 A\\mathcal{A}A. 作者假设有一个可以访问的案例库 D={(q1,l1),(q2,l2),...,(qN,lN)}\\mathcal{D} = \\{(q_1, l_1), (q_2, l_2), ..., (q_N, l_N)\\}D={(q1​,l1​),(q2​,l2​),...,(qN​,lN​)}, 其中 qi,liq_i, l_iqi​,li​ 分别表示自然语言问题和它对应的逻辑表达式。CBR-KBQA 首先利用 retrieve module 从 D\\mathcal{D}D 中抽取 KKK 个与问题 qqq 相似的案例，记作 Dq\\mathcal{D}_qDq​。 然后通过 reuse module 复用 Dq\\mathcal{D}_qDq​ 中案例的组件生成中间逻辑表达式 linterl_\\text{inter}linter​。最后通过 revise module 将 linterl_\\text{inter}linter​ 中的关系与知识图谱中同一个实体的最相似的关系对齐，对齐后的逻辑表达式记作 lll. 最后，CBR-KBQA 在 K\\mathcal{K}K 上执行 lll, 得到答案列表 A\\mathcal{A}A. Retrieve 该模块通过为自然语言问题生成表征的方式计算其与案例库中案例的相似度。在计算时，作者同时希望得到的相似案例具有在关系上的高度相似性，而非实体上的相似性。因此在训练时，作者以一定的比例 pmaskp_\\text{mask}pmask​ 将问题中出现的实体替换为 [BLANK]\\text{[BLANK]}[BLANK]. 两个问题之间的相似度由它们表征向量之间的余弦相似度度量。 在训练时，作者利用训练数据中可访问的逻辑表达式作为远程监督：一对问题之间的损失函数 （负对数似然） 由它们的逻辑表达式中出现的关系重复度加权： L=−∑i,jωi,jlog⁡exp⁡(∼(qi,qj))Σjexp⁡(∼(qi,qj))L = -\\sum_{i, j}\\omega_{i, j}\\log\\frac{\\exp(\\sim(\\mathbf{q}_i, \\mathbf{q}_j))}{\\Sigma_j\\exp(\\sim(\\mathbf{q}_i, \\mathbf{q}_j))} L=−i,j∑​ωi,j​logΣj​exp(∼(qi​,qj​))exp(∼(qi​,qj​))​ 其中，qi∈Rd\\mathbf{q}_i\\in \\mathbb{R}^dqi​∈Rd 表示 qiq_iqi​ 的表征向量，∼(qi,qj)=qiTqj\\sim(\\mathbf{q}_i, \\mathbf{q}_j) = \\mathbf{q}_i^T\\mathbf{q}_j∼(qi​,qj​)=qiT​qj​, ωi,j\\omega_{i, j}ωi,j​ 由 qi,qjq_i, q_jqi​,qj​ 对应的逻辑形式中关系的 F1 计算。 作者预计算案例库中所有问题的表征，然后在推理时返回 qqq 的 top-k 相似的案例。 Reuse 该模块将 retriever 返回的 k 个案例作为输入送入一个预训练的 encoder-decoder transformer 模型，训练模型为新问题生成逻辑表达式。作者选用具有稀疏注意力的 BigBird 作为该模块的 transformer 模型，主要因为完全注意力的 transformer 模型的空间复杂度是输入序列长度的二次方，例如 BART，T5 等只支持最长 512 的输入序列。 作者按如下格式对问题 qqq 和案例 Dq={(q1′,l1′),(q2′,l2′),...,(qk′,lk′)}\\mathcal{D}_q = \\{(q_1&#x27;, l_1&#x27;), (q_2&#x27;, l_2&#x27;), ..., (q_k&#x27;, l_k&#x27;)\\}Dq​={(q1′​,l1′​),(q2′​,l2′​),...,(qk′​,lk′​)} 进行拼接： InputEncoder(q,Dq)=q[SEP]q1′[SEP]l1′...qk′[SEP]lk′\\text{Input}_\\text{Encoder}(q, \\mathcal{D}_q) = q\\text{[SEP]}q_1&#x27;\\text{[SEP]}l_1&#x27;...q_k&#x27;\\text{[SEP]}l_k&#x27; InputEncoder​(q,Dq​)=q[SEP]q1′​[SEP]l1′​...qk′​[SEP]lk′​ 作者采用 BART-BASE 初始化模型，并使用标准的 seq2seq 交叉熵损失训练。除此之外，作者在模型输出层的 softmax 层上添加了一个正则化项：有案例 Dq\\mathcal{D}_qDq​ 时和仅有 qqq 时得到概率分布之间的 KL 散度。记 seq2seq 模型为函数 fff, 有 Dq\\mathcal{D}_qDq​ 时输出的概率分布为 σ=softmax(f(q,Dq))\\sigma = \\text{softmax}(f(q, \\mathcal{D}_q))σ=softmax(f(q,Dq​)), 仅有 qqq 时输出的概率分布为 σ′=softmax(f(q))\\sigma&#x27; = \\text{softmax}(f(q))σ′=softmax(f(q)), 则添加了 KLD 正则化项的损失函数为： L=LCE(f(q,Dq),lq)+λTKLD(σ,σ′)L = L_{CE}(f(q, \\mathcal{D}_q), l_q) + \\lambda_T\\text{KLD}(\\sigma, \\sigma&#x27;) L=LCE​(f(q,Dq​),lq​)+λT​KLD(σ,σ′) 其中 λT\\lambda_TλT​ 是一个超参数，该正则化项的目的是使得模型在有无案例参与的情况下输出的结果相差不要太大。 Revise 在 Reuse 模块中，模型复用案例中的关系生成新的逻辑表达式，但是案例中的关系很可能与新问题的实体不匹配，即知识图谱中的该实体不存在生成的关系。 因此作者提出一个对齐策略，对于问题中的实体，首先将其所有的关系取出，然后计算它们与生成关系之间的相似度，取最为相似的关系替换生成的关系。 关系之间的相似度通过关系表征之间的余弦相似度计算。作者尝试了两种关系表征计算方法：TransE 和 RoBERTa 的句子嵌入。由于关系通常是一个短语，RoBERTa 无法发挥其上下文表征的优势，TransE 的预训练表征在测试时更优。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"}],"author":["Rajarshi Das","Manzil Zaheer","Dung Thai","Ameya Godbole","Ethan Perez","Jay-Yoon Lee","Lizhen Tan","Lazaros Polymenakos","Andrew McCallum"]},{"title":"TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph","slug":"TransferNet-An-Effective-and-Transparent-Framework-for-Multi-hop-Question-Answering-over-Relation-Graph","date":"2022-02-21T16:00:00.000Z","updated":"2023-06-21T06:51:28.924Z","comments":true,"path":"TransferNet-An-Effective-and-Transparent-Framework-for-Multi-hop-Question-Answering-over-Relation-Graph/","link":"","permalink":"https://blog.zhuwenq.cc/TransferNet-An-Effective-and-Transparent-Framework-for-Multi-hop-Question-Answering-over-Relation-Graph/","excerpt":"本文针对多跳 KGQA 问题，提出一种透明的框架提升多跳 KGQA 的效率和可解释性。现存的模型通常是通过预测多跳过程中的关系序列或者通过图神经网络提取知识图谱的隐式特征来解决多跳 KGQA 问题，前者由于推理路径的搜索空间太大而难以优化，后者则缺乏可解释性。本文作者提出的 TransferNet 则通过逐步计算，获取推理过程中每个节点激活的实体和关系解决 KGQA 问题，由于推理过程中的中间结果可以轻易被人类理解，具有较好的可解释性。","text":"本文针对多跳 KGQA 问题，提出一种透明的框架提升多跳 KGQA 的效率和可解释性。现存的模型通常是通过预测多跳过程中的关系序列或者通过图神经网络提取知识图谱的隐式特征来解决多跳 KGQA 问题，前者由于推理路径的搜索空间太大而难以优化，后者则缺乏可解释性。本文作者提出的 TransferNet 则通过逐步计算，获取推理过程中每个节点激活的实体和关系解决 KGQA 问题，由于推理过程中的中间结果可以轻易被人类理解，具有较好的可解释性。 Introduction 文章关注于 关系图谱(Relation Graph) 上的多跳 QA 问题，关系图谱中包含实体和他们之间的关系，其关系有两种形式： 标签形式：即知识图谱的形式，关系是人工构建的受限的谓语 文本形式：关系是从自由文本语料中提取的自由文本。这种关系可以从主客实体同时出现的句子中抽取 标签形式的关系图谱由于需要人工构建和维护，通常成本较高且不完整，文本形式则更加经济且实际。作者提出的 TransferNet 可以在一个统一的框架内解决这两种形式的关系图谱上的多跳 QA 问题。 现存的解决多跳 QA 问题的方法主要有两种思路： 在弱监督的设置下学习推理的关系路径，即根据最终答案学习整个推理中间过程。这种方法由于搜索空间太大通常难以优化，对于文本形式的关系图谱，搜索空间则更大。特别地，对于标签形式关系图谱上的多跳 QA 问题(即 KGQA 问题)，主要的解决方案可以归为两类： 信息提取式：通过学习 KG 和问题文本的表征来计算答案 语义解析式：通过将问题语句解析成可执行的逻辑表达式（如 SQL 语句）来查询答案 通过图神经网络捕捉图中的隐式特征，这种方法可以解决两种形式的关系图谱，但是由于整个推理过程是黑盒状态，其可解释性较差。 TransferNet 则是通过逐步推理，通过注意力机制让模型在每一步关注于问题文本中某些词，来计算该步中应该激活的关系和实体最终获得答案。 Methodology Relation Graph 记 Relation Graph 为 G\\mathcal{G}G, 它的实体和边的集合记为 E\\mathcal{E}E 和 R\\mathcal{R}R. 假设 G\\mathcal{G}G 中实体数量是 nnn, 则 R\\mathcal{R}R 是一个 n×nn\\times nn×n 的矩阵，其元素 ri,jr_{i, j}ri,j​ 表示头实体 eie_iei​ 和尾实体 eje_jej​ 之间的关系，ri,jr_{i, j}ri,j​ 可以是一系列的标签或文本。一个多跳问题 qqq 通常需要从主题实体 exe_xex​ 出发，经过一系列的关系到达答案 Y={ey1,...,ey∣Y∣}Y = \\{e_{y^1}, ..., e_{y^{|Y|}}\\}Y={ey1​,...,ey∣Y∣​}. TransferNet TransferNet 从主题实体 exe_xex​ 出发，经过 TTT 步逐步推理获得答案。每一步中，TransferNet 关注于问题文本的不同部分来决定最合适的关系路径。TransferNet 维护一个评分向量来表示每个实体的激活概率，初始情况下，主题实体的评分初始化为 1，其余实体评分初始化为 0. 每一步中，TransferNet 通过关注问题不同的部分为每个关系计算一个激活概率，然后据此转移实体评分向量，计算要激活的实体。 形式化地，可以将第 ttt 步的实体评分向量记为 at=[0,1]n\\mathbf{a}^t = [0, 1]^nat=[0,1]n, 向量维度为 nnn，每个值都是介于 [0,1][0, 1][0,1] 之间的实数，代表每个实体的激活概率。在第 ttt 步，模型通过关注于问题文本的特殊部分获得一个查询向量 qt∈Rd\\mathbf{q}^t\\in\\mathcal{R}^dqt∈Rd： q,(h1,...,h∣q∣)=Encoder(q;θe)qkt=ft(q;θft)bt=Softmax(qkt⋅[h1;...;h∣q∣]T)qt=∑i=1∣q∣bithi\\begin{aligned} \\mathbf{q}, (\\mathbf{h}_1, ..., \\mathbf{h}_{|q|}) &amp;= \\text{Encoder}(q; \\theta_e)\\\\ \\mathbf{qk}^t &amp;= f^t(\\mathbf{q}; \\theta_{f^t})\\\\ \\mathbf{b}^t &amp;= \\text{Softmax}(\\mathbf{qk^t}\\cdot[\\mathbf{h}_1; ...; \\mathbf{h}_{|q|}]^T)\\\\ \\mathbf{q}^t &amp;= \\sum_{i = 1}^{|q|}b_i^t\\mathbf{h}_i \\end{aligned}q,(h1​,...,h∣q∣​)qktbtqt​=Encoder(q;θe​)=ft(q;θft​)=Softmax(qkt⋅[h1​;...;h∣q∣​]T)=i=1∑∣q∣​bit​hi​​ 其中，q\\mathbf{q}q 是文本嵌入，(h1,...,h∣q∣)(\\mathbf{h}_1, ..., \\mathbf{h}_{|q|})(h1​,...,h∣q∣​) 是问题文本中每个 token 的嵌入。ftf^tft 是一个投影函数，将问题表征 q\\mathbf{q}q 投影为查询向量 qkt\\mathbf{qk}^tqkt, qkt\\mathbf{qk}^tqkt 即第 ttt 步的注意力向量，通过其与每个 token 的嵌入 hi\\mathbf{h}_ihi​ 相乘再经过 Softmax 获取问题文本中每个 token 的注意力分数向量 bt\\mathbf{b}^tbt, qt\\mathbf{q}^tqt 则是 hi\\mathbf{h}_ihi​ 经过注意力分数加权的加权和。 得到问题文本的注意力查询向量 qt\\mathbf{q}^tqt 后，TransferNet 基于此计算关系激活分数 Wt∈[0,1]n×n=g(qt;θg)\\mathbf{W}^t\\in[0, 1]^{n\\times n} = g(\\mathbf{q}^t; \\theta_g)Wt∈[0,1]n×n=g(qt;θg​). 其中的 θg\\theta_gθg​ 是函数 ggg 的可学习参数，对于标签形式和文本形式的关系图谱，将有不同的 ggg 的实现。 随后，TransferNet 模拟推理过程，计算下一步的激活实体：at=at−1Wt\\mathbf{a}^t = \\mathbf{a}^{t-1}\\mathbf{W}^tat=at−1Wt, 即：ajt=∑i=1nait−1×Wi,jta_j^t = \\sum_{i=1}^{n}a_i^{t-1}\\times W_{i, j}^tajt​=∑i=1n​ait−1​×Wi,jt​. 其物理意义为：实体 eie_iei​ 前一步的激活分数乘以边 ri,jr_{i, j}ri,j​ 的关系激活分数得到实体 eje_jej​ 的当前激活分数。 经过 TTT 步上述过程后，可以得到每一步的实体激活向量 a1,a2,...,aT\\mathbf{a}^1, \\mathbf{a}^2, ..., \\mathbf{a}^Ta1,a2,...,aT, 由于模型无法得知输入的问题实际的跳数，作者通过计算所有 TTT 步的实体激活向量的加权和作为最终的答案输出： c=Softmax(MLP(q))a∗=∑t=1Tctat\\begin{aligned} \\mathbf{c} &amp;= \\text{Softmax}(\\text{MLP}(\\mathbf{q}))\\\\ \\mathbf{a}^* &amp;= \\sum_{t=1}^Tc_t\\mathbf{a}^t \\end{aligned}ca∗​=Softmax(MLP(q))=t=1∑T​ct​at​ 其中 c∈[0,1]T\\mathbf{c}\\in[0, 1]^Tc∈[0,1]T 表示输入问题的跳数的概率分布，即 ctc_tct​ 代表该问题是 ttt 跳问题的概率。作者在此是寄希望于多层感知机可以根据问题的嵌入 q\\mathbf{q}q 计算出该问题的跳数，并表现为概率分布的形式。因此可以通过该概率分布与每一步的实体激活向量相乘而自动地处理从 1 跳到 TTT 跳的问题。最终结果 a∗\\mathbf{a}^*a∗ 中得分最高的实体即为最终答案。 通过上述过程可知，TransferNet 计算过程中每一步的激活实体和激活关系都可以被单独取出观察，由此可以轻易观察到模型的推理过程，因此 TransferNet 是具有高度可解释性的。 Training 给定问题的答案集 Y={ey1,...,ey∣Y∣}Y=\\{e_{y^1}, ..., e_{y^{|Y|}}\\}Y={ey1​,...,ey∣Y∣​}, 作者构建目标评分向量 y∈{0,1}n\\mathbf{y}\\in\\{0, 1\\}^ny∈{0,1}n: yi={1,if ei∈Y0,elsey_i = \\begin{cases} 1, \\text{if }e_i\\in Y\\\\ 0, \\text{else} \\end{cases}yi​={1,if ei​∈Y0,else​ 作者取 a∗\\mathbf{a}^*a∗ 与 y\\mathbf{y}y 之间的 L2L2L2 范数作为训练目标函数：L=∥a∗−y∥\\mathcal{L} = \\Vert\\mathbf{a}^*-\\mathbf{y}\\VertL=∥a∗−y∥. 由于 TransferNet 整个计算过程是可微的，通过该简单的训练目标可以学习所有中间结果。 额外模块 为了帮助训练，作者提出两个额外模块： Score Truncation: 根据实体激活分数计算过程，ajta_j^tajt​ 经过一个转移步之后其值可能会超过 1. 太大的激活分数可能会在跳数过多的情况下导致梯度爆炸，因此作者在每个转移步之后进行分数裁剪，保证激活分数落在 [0,1][0, 1][0,1] 之间，同时要保证计算过程的可微性： Trunc(a)=az(a)z(a)={a.detach(),if a&gt;11,else\\begin{aligned} \\text{Trunc}(a) &amp;= \\frac{a}{z(a)}\\\\ z(a) &amp;= \\begin{cases} a.\\text{detach()}, &amp;\\text{if } a &gt; 1\\\\ 1, &amp;\\text{else} \\end{cases} \\end{aligned}Trunc(a)z(a)​=z(a)a​={a.detach(),1,​if a&gt;1else​​ Language Mask: TransferNet 在计算时没有考虑可能会提供提示的语言偏向。例如，对于文本形式的关系图： （Harry Potter, &lt;sub&gt; was published in &lt;obj&gt;, United Kingdom） 和 （Harry Potter, &lt;sub&gt; was published in &lt;obj&gt;, 1997） 之间的关系文本完全相同，但是一个表示出版地另一个表示出版时间，此时对于问题 Where was Harry Potter published , TransferNet 未考虑 Where 对答案的提示。 针对此问题，作者引入一个语言遮罩来结合问题提示：m=Sigmoid(MLP(q))\\mathbf{m} = \\text{Sigmoid}(\\text{MLP}(\\mathbf{q}))m=Sigmoid(MLP(q)), 其中 m∈[0,1]n\\mathbf{m}\\in [0, 1]^nm∈[0,1]n, mim_imi​ 表示实体 eie_iei​ 的遮罩评分。此处作者寄希望于多层感知机能够根据问题嵌入提取其中的提示，从而从该提示的角度为候选实体评分。 根据语言遮罩修正的结果为：a∗^=m⊙a∗\\hat{\\mathbf{a}^*} = \\mathbf{m}\\odot \\mathbf{a}^*a∗^=m⊙a∗. 注意到相同关系文本不同含义的现象仅出现在文本形式的关系图谱中，标签形式图谱不需要该修正。 关系激活分数计算 对于关系分数 Wt=g(qt;θg)\\mathbf{W}^t = g(\\mathbf{q}^t; \\theta_g)Wt=g(qt;θg​), 函数 ggg 对于标签形式和文本形式的关系图谱有不同的实现。 标签形式：关系可以表示为一个固定的谓语集 P\\mathcal{P}P. 作者首先根据 qt\\mathbf{q}^tqt 计算这些谓语的概率，然后根据首尾实体将对应的关系 ri,jr_{i, j}ri,j​ 的概率放入 Wi,jW_{i, j}Wi,j​: pt=Softmax(MLP(qt))\\mathbf{p}^t = \\text{Softmax}(\\text{MLP}(\\mathbf{q}^t))pt=Softmax(MLP(qt)), 如果同时可以激活多个关系，Softmax\\text{Softmax}Softmax 函数可以替换为 Sigmoid\\text{Sigmoid}Sigmoid 函数。 假设一对实体之间的关系数目最多为 bbb, 则记 ri,j={ri,j,1,...,ri,j,b}r_{i, j} = \\{r_{i, j, 1}, ..., r_{i, j, b}\\}ri,j​={ri,j,1​,...,ri,j,b​}, Wi,jW_{i, j}Wi,j​ 可以由所有关系 ri,jr_{i, j}ri,j​ 的概率之和计算：Wi,jt=∑k=1bpri,j,ktW_{i, j}^t = \\sum_{k=1}^bp_{r_{i, j, k}}^tWi,jt​=∑k=1b​pri,j,k​t​ 文本形式：关系是来自文本语料中实体对同时出现的文本，记 ri,j={ri,j,1,...,ri,j,b}r_{i, j} = \\{r_{i, j, 1}, ..., r_{i, j, b}\\}ri,j​={ri,j,1​,...,ri,j,b​}, 其中 ri,j,kr_{i, j, k}ri,j,k​ 代表第 kkk 个关系文本。作者使用关系编码器获取关系文本的嵌入，然后计算其分数： ri,j,k=Encoder(ri,j,k;θr)pri,j,kt=Sigmoid(MLP(ri,j,k⊙qt))Wi,jt=∑k=1bpri,j,kt\\begin{aligned} \\mathbf{r}_{i, j, k} &amp;= \\text{Encoder}(r_{i, j, k}; \\theta_r)\\\\ p_{r_{i, j, k}}^t &amp;= \\text{Sigmoid}(\\text{MLP}(\\mathbf{r}_{i, j, k}\\odot \\mathbf{q}^t))\\\\ W_{i, j}^t &amp;= \\sum_{k=1}^bp_{r_{i, j, k}}^t \\end{aligned}ri,j,k​pri,j,k​t​Wi,jt​​=Encoder(ri,j,k​;θr​)=Sigmoid(MLP(ri,j,k​⊙qt))=k=1∑b​pri,j,k​t​​ 此处作者将关系文本的嵌入与问题文本注意力查询向量相乘，然后寄希望于多层感知机能够根据相乘的结果计算该关系文本的概率分数。 值得注意的是，文本形式的关系图谱中关系文本的数量十分庞大，不可能全部计算其概率分数。因此作者选择在前一步中得分大于一个预定义的阈值 τ\\tauτ 的实体，然后仅考虑从这些实体开始的关系文本。如果数量仍过于庞大，作者根据它们的头实体激活分数对关系文本进行排序，然后仅取前 ω\\omegaω 个关系文本。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"}],"author":["Jiaxin Shi","Shulin Cao","Lei Hou","Juanzi Li","Hanwang Zhang"]},{"title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models","slug":"Large-Scale-Relation-Learning-for-Question-Answering-over-Knowledge-Bases-with-Pre-trained-Language-Models","date":"2022-01-14T16:00:00.000Z","updated":"2023-06-21T06:51:28.800Z","comments":true,"path":"Large-Scale-Relation-Learning-for-Question-Answering-over-Knowledge-Bases-with-Pre-trained-Language-Models/","link":"","permalink":"https://blog.zhuwenq.cc/Large-Scale-Relation-Learning-for-Question-Answering-over-Knowledge-Bases-with-Pre-trained-Language-Models/","excerpt":"KBQA(Question Answering over Knowlegde Bases) 旨在根据结构化的知识图谱回答自然语言问题，该任务在现代的问答系统和信息检索系统中广泛应用。 由于自然语言问题与知识图谱中的推理链的不相容性，最近的 KBQA 方法更关注于图谱中的逻辑关系，忽略了图谱的节点和边上的文本语义信息。同时预训练模型虽然从大规模的语料中学习到了大量开放世界知识，但是这种知识是非结构化的，与结构化的知识图谱之间存在隔阂。为连接预训练语言模型与知识图谱，作者提出了三种关系学习任务来进行关系增强训练。经过这种训练，模型能够将自然语言表述与知识图谱中的关系对齐，同时能够跨过缺失的直接关系进行推理。","text":"KBQA(Question Answering over Knowlegde Bases) 旨在根据结构化的知识图谱回答自然语言问题，该任务在现代的问答系统和信息检索系统中广泛应用。 由于自然语言问题与知识图谱中的推理链的不相容性，最近的 KBQA 方法更关注于图谱中的逻辑关系，忽略了图谱的节点和边上的文本语义信息。同时预训练模型虽然从大规模的语料中学习到了大量开放世界知识，但是这种知识是非结构化的，与结构化的知识图谱之间存在隔阂。为连接预训练语言模型与知识图谱，作者提出了三种关系学习任务来进行关系增强训练。经过这种训练，模型能够将自然语言表述与知识图谱中的关系对齐，同时能够跨过缺失的直接关系进行推理。 Introduction 传统的信息检索系统通常是一个 pipeline 的形式： named entity recongnization entity linking subgraph retrieval entity scoring KBQA 任务对这种 pipeline 仍然具有挑战性，特别是面对多跳场景时： 由于人类语言的复杂性，通常难以将自然语言问题与知识图谱中的关系对齐，模型通常是根据一些 shortcut 特征而不是正确的推理进行预测 实践中，知识图谱往往是不完整的，同时缺乏显式推理训练的模型难以在缺失的关系上推理 GraftNet, PullNet 等通过引入大量额外的文本并使用特殊的架构来融合文档中的信息解决上述问题 EmbedKGQA 通过引入预训练 KB embedding 并训练问题表征以适应关系表征来解决知识图谱不完整问题 本文中，为了获得一个更好的从自然语言问题到知识图谱中推理路径的映射，作者重组了基于检索的 KBQA 任务，将其变成问题文本匹配的形式。 作者提出了三个关系学习的辅助任务： Relation Extraction, RE: 从句子中推断关系 Relation Matching, RM: 判断两个句子是否表达同一个关系 Relation Reasoning, RR: 训练模型在缺失的关系上推理 Approach 基于检索的 KBQA 给定一个输入问题 qqq, 首先标记 qqq 中的命名实体，然后将其与知识图谱中的节点连接，随后使用一些启发式算法(如 pagerank)从知识图谱中检索出一个特定于 qqq 的子图 G={&lt;e,r,e′&gt;∣e,e′∈E,r∈R}\\mathcal{G} = \\{&lt;e, r, e&#x27;&gt; | e, e&#x27; \\in \\mathcal{E}, r\\in\\mathcal{R}\\}G={&lt;e,r,e′&gt;∣e,e′∈E,r∈R}, 其中 E\\mathcal{E}E 是问题 qqq 的候选答案实体的集合，R\\mathcal{R}R 是关系的集合。该任务即为每个候选答案实体 ei∈Ee_i\\in \\mathcal{E}ei​∈E 计算一个分数 sis_isi​ 标示该实体是否是答案实体。 BERT for KBQA 对于每一个问题 qqq, 作者首先利用 entity linking 结果找出 qqq 在知识图谱中的 主题实体 etopice_\\text{topic}etopic​，随后，作者通过以下步骤将问题转化为文本匹配问题： 找出 G\\mathcal{G}G 中所有连接了 etopice_\\text{topic}etopic​ 和候选答案实体 eie_iei​ 的路径，查找时设置一个路径数的最大值以保证可计算性 通过将路径中的节点替换成实体名称，边替换为关系名称，将路径 重组为文本形式 通过将问题文本和重组后的路径文本 p1,p2,...,pnp_1, p_2, ..., p_np1​,p2​,...,pn​ 拼接，获取BERT的输入样本：xi=[CLS]q[SEP]p1[SEP]...pn[SEP].x_i = \\text{[CLS]}q\\text{[SEP]}p_1\\text{[SEP]}...p_n\\text{[SEP]}.xi​=[CLS]q[SEP]p1​[SEP]...pn​[SEP]. 将 xix_ixi​ 送入 BERT，取最终输出中 [CLS]\\text{[CLS]}[CLS] 的表征做二分类，代表了该候选实体 eie_iei​ 是否是问题 qqq 的答案 该过程中，相当于将知识图谱中所有 etopice_\\text{topic}etopic​ 和 eie_iei​ 之间的路径作为问题 qqq 的支撑事实，并使用 BERT 推断 “eie_iei​ 是问题 qqq 的答案” 这一假设在支撑事实条件下是否成立。 二分类损失函数计算过程： si=σ(ωTBERTCLS(xi))Li=−(y⋅log⁡si+(1−y)⋅log⁡(1−si))\\begin{aligned} s_i &amp;= \\sigma(\\mathbf{\\omega}^T \\text{BERT}_\\text{CLS}(x_i))\\\\ \\mathcal{L}_i &amp;= -(y\\cdot\\log s_i + (1-y)\\cdot\\log(1-s_i)) \\end{aligned}si​Li​​=σ(ωTBERTCLS​(xi​))=−(y⋅logsi​+(1−y)⋅log(1−si​))​ 其中 σ\\sigmaσ 代表 sigmoid 函数，yyy 是 ground truth，代表 eie_iei​ 实际上是否是问题 qqq 的答案。 副任务 Relation Extraction (RE) 1-hop: 直接使用 relation extraction 数据集，训练模型从给定的语句中抽取头尾实体之间的关系。在实践方面，作者构建BERT的输入格式：[CLS]s[SEP]h,r,t[SEP]\\text{[CLS]}s\\text{[SEP]}h, r, t\\text{[SEP]}[CLS]s[SEP]h,r,t[SEP], 其中 s,h,r,ts, h, r, ts,h,r,t 分别表示句子，头实体，关系和尾实体 2-hop: 选取2个1-hop的样本组合成2-hop样本：[CLS]s1,s2[SEP]h1,r1,t1(h2),r2,t2[SEP]\\text{[CLS]}s_1, s_2\\text{[SEP]}h_1, r_1, t_1(h_2), r_2, t_2\\text{[SEP]}[CLS]s1​,s2​[SEP]h1​,r1​,t1​(h2​),r2​,t2​[SEP], 其中第一个1-hop样本的尾实体即是第二个1-hop样本的头实体 Relation Matching (RM) 作者假设：表达相同关系的两个句子应该有相似的表征。因此利用BERT预测两个句子是否表达相同的关系，输入样本格式：[CLS]s1[SEP]s2[SEP]\\text{[CLS]}s_1\\text{[SEP]}s_2\\text{[SEP]}[CLS]s1​[SEP]s2​[SEP] Relation Reasoning (RR) BERTRL 提出一种自训练模式，用于 KB 补全任务。他们随机从 KB 中选取一个三元组 (h,r,t)(h, r, t)(h,r,t) 并假设其是缺失的，然后选择从 hhh 到 ttt 的其他多跳连接，然后使用 BERT 根据这些多跳连接预测 (h,r,t)(h, r, t)(h,r,t) 是否存在。 输入样本格式：[CLS]h,r,t[SEP]p1[SEP]...pn[SEP]\\text{[CLS]}h, r, t\\text{[SEP]}p_1\\text{[SEP]}...p_n\\text{[SEP]}[CLS]h,r,t[SEP]p1​[SEP]...pn​[SEP] 注意到以上三种辅助任务都被组织成了二分类任务，且只在输入数据的组织形式上做了改变，因此可以应用在 KBQA 任务之前的预训练过程或者在微调过程中与 KBQA 任务联合训练。 Experiments Dataset KBQA Dataset: 利用 WebQSP 数据集，使用Sun et al.的脚本进行处理，该脚本主要对 WebQSP 做 entity linking 和 subgraph retrieval，同时 entity linking 的结果直接使用 Yih et al. 的。 对于每个问题，都有一些用于子图检索的种子实体。子图检索过程使用 Personalized PageRank 算法。 Relation Extraction Dataset: 对于关系学习任务，作者使用 WebRED 和 FewRel 作为额外资源 Baselines KV-Mem GraftNet PullNet EmbedKGQA NSM BERT Metrics Hits@1: 如果模型给出的得分最高的结果就是答案，则 Hits@1 是1，否则是0 F1: 给定一个阈值，并认为模型输出结果中得分高于阈值的都是答案，然后计算模型输出的答案和GT答案之间的 F1 F1 指标对阈值的选择很敏感，实验中选取 Hits@1 作为主指标","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"}],"author":["Yuanmeng Yan","Rumei Li","Sirui Wang","Hongzhi Zhang","Daoguang Zan","Fuzheng Zhang","Wei Wu","Weiran Xu"]},{"title":"Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings","slug":"Improving-Multi-hop-Question-Answering-over-Knowledge-Graphs-using-Knowledge-Base-Embeddings","date":"2022-01-07T16:00:00.000Z","updated":"2023-06-21T06:51:28.792Z","comments":true,"path":"Improving-Multi-hop-Question-Answering-over-Knowledge-Graphs-using-Knowledge-Base-Embeddings/","link":"","permalink":"https://blog.zhuwenq.cc/Improving-Multi-hop-Question-Answering-over-Knowledge-Graphs-using-Knowledge-Base-Embeddings/","excerpt":"Multi-hop KGQA 要求跨越 Knowledge Graph, KG 的多个边进行推理，而且 KG 往往是不完整的，使 KGQA 更具挑战性。最近的研究有些使用额外的相关领域的语料解决 KG 的稀疏性问题，但是额外的语料是否可获取，以及相关性如何判别都是问题。KG embedding 作为另一个研究方向，被用于缺失连接预测以及 KG 补全，但是没有被直接用于 Multi-hop KGQA 问题。本文作者提出 EmbedKGQA，使用 KG embedding 以及 Question embedding 解决 KGQA 问题。","text":"Multi-hop KGQA 要求跨越 Knowledge Graph, KG 的多个边进行推理，而且 KG 往往是不完整的，使 KGQA 更具挑战性。最近的研究有些使用额外的相关领域的语料解决 KG 的稀疏性问题，但是额外的语料是否可获取，以及相关性如何判别都是问题。KG embedding 作为另一个研究方向，被用于缺失连接预测以及 KG 补全，但是没有被直接用于 Multi-hop KGQA 问题。本文作者提出 EmbedKGQA，使用 KG embedding 以及 Question embedding 解决 KGQA 问题。 Introduction 对于 multi-hop KGQA 问题，主要挑战在于： 系统需要在 KG 的多条边上进行推理 KG 往往是不完整的 为解决 KG 的稀疏性，Sun et al.[1][2] 利用额外的相关领域文本语料来应对。其中 Sun et al. 提出的 PullNet[1:1] 从 KG 中构建一个与问题相关的子图，然后利用额外的文档对其进行增强，随后在增强后的子图上使用图卷积神经网络计算最终答案。这种方法不仅存在相关的额外的文本语料的获取问题，同时也需要一个预定义的邻域范围用于子图的召回。 另一个方向上，Bordes et al.[3], Trouillon et al.[4], Yang et al.[5] 和 Nickel et al.[6] 使用 KG embedding 的方法为 KG 中的实体和关系学习高维嵌入，然后将其用于连接预测以及图谱补全。 Related Work KGQA: TransE[3:1] 等用于单跳场景 Yih et al.[7] 和 Bao et al., 2016[8] 等提出从 KG 中抽取一个特定于问题的子图来回答问题 Bordes et al., 2014a[9] 提出的方法将抽取出的关于头实体的子图编码到高维空间用于QA任务 Bordes et al., 2015[10] 提出使用 Memory Network 为知识图谱中出现的事实训练表征用于QA Bordes et al., 2014b[11] 训练一个问题与三元组的相似度函数，然后在测试时为问题和候选三元组计算相似度分数 Yang et al., 2014b 和 Yang et al., 2015 利用嵌入的方法将自然语言问题转换为逻辑形式 KG embedding: Bordes et al. 提出的 TransE[3:2]，将知识图谱嵌入到高维实空间，将关系看作是头实体和尾实体之间的 translation Sun et al. 提出的 RotatE[12]，将知识图谱中的实体嵌入到复空间，并将关系看作是复空间之间的 rotate Background KG 给定实体集合 E\\mathcal{E}E 和关系集合 R\\mathcal{R}R, 知识图谱 G\\mathcal{G}G 定义为三元组的集合 K⊆E×R×E\\mathcal{K}\\subseteq\\mathcal{E}\\times\\mathcal{R}\\times\\mathcal{E}K⊆E×R×E. 其中每个三元组记作 (h,r,t)(h, r, t)(h,r,t), h,th, th,t 分别代表头实体和尾实体，rrr 代表它们之间的关系。 Link Prediction 给定一个不完整的 KG, Link Predition 的任务是预测缺失的连接。KG Embedding 模型通过一个评分函数 ϕ\\phiϕ 为三元组评分 s=ϕ(h,r,t)s=\\phi(h, r, t)s=ϕ(h,r,t) 指示该连接是否存在。 KG Embedding 对于 e∈Ee\\in\\mathcal{E}e∈E 和 r∈Rr\\in\\mathcal{R}r∈R, KG Embedding 为其生成一个嵌入向量 ee∈Rde,er∈Rdre_e\\in \\mathbb{R}^{d_e}, e_r\\in\\mathbb{R}^{d_r}ee​∈Rde​,er​∈Rdr​. 嵌入通常通过对比学习的方法训练，即采用一个 (eh,er,et)(e_h, e_r, e_t)(eh​,er​,et​) 的评分函数 ϕ\\phiϕ, 训练模型为三元组 (h,r,t)(h, r, t)(h,r,t) 生成嵌入。使得对于 (h,r,t)∈K(h, r, t) \\in \\mathcal{K}(h,r,t)∈K, ϕ(eh,er,et)&gt;0\\phi(e_h, e_r, e_t) &gt; 0ϕ(eh​,er​,et​)&gt;0; 对于 (h′,r′,t′)∉K(h&#x27;, r&#x27;, t&#x27;) \\notin \\mathcal{K}(h′,r′,t′)∈/K, ϕ(eh′,er′,et′)&lt;0\\phi(e_{h&#x27;}, e_{r&#x27;}, e_{t&#x27;}) &lt; 0ϕ(eh′​,er′​,et′​)&lt;0. ComplEx Embedding ComplEx[4:1] 是一种通过张量分解来嵌入实体和关系到复空间的方法。对于 h,t∈Eh, t\\in\\mathcal{E}h,t∈E 和 r∈Rr\\in \\mathcal{R}r∈R, ComplEx 生成嵌入 eh,hr,ht∈Cde_h, h_r, h_t \\in \\mathbb{C}^deh​,hr​,ht​∈Cd, 并且定义评分函数为： ϕ(h,r,t)=Re(⟨eh,er,et⟩)=Re(∑k=1deh(k)er(k)et(k))\\begin{aligned} \\phi(h, r, t) &amp;= \\text{Re}(\\langle e_h, e_r, e_t\\rangle)\\\\ &amp;=\\text{Re}(\\sum_{k=1}^{d}e_h^{(k)}e_r^{(k)}e_t^{(k)}) \\end{aligned}ϕ(h,r,t)​=Re(⟨eh​,er​,et​⟩)=Re(k=1∑d​eh(k)​er(k)​et(k)​)​ 使得对于三元组正例评分大于0，负例评分小于0. EmbedKGQA KGQA: 给定一个自然语言问题 qqq 和一个问题中出现的主题实体 ehe_heh​, KGQA 的任务是从 KG 中抽取出问题 qqq 指向的那个实体 ete_tet​ 作者提出的 EmbedKGQA 模型包含三个模块： KG Embedding 模块，为 KG 中所有实体创建 embedding Question Embedding 模块，为自然语言问题创建 embedding Answer Selection 模块，缩小备选答案的范围并选择最终答案 KG Embedding 模块 KG Embedding 直接使用 ComplEx Embedding 方法，根据训练集中实体对 KG 的覆盖程度，训练出的 embedding 可以选择保持不变或者在后续步骤中微调。 Question Embedding 模块 该模块将自然语言问题 qqq 嵌入到复向量空间 Cd\\mathbb{C}^dCd. 这个过程首先使用一个预训练的 RoBERTa 将 qqq 编码为 768 维的向量，然后使其通过四层带有 ReLU 激活函数的线性全连接层，最终投影到 Cd\\mathbb{C}^dCd. Question Embedding 的训练过程同样采用 ComplEx Embedding，对于自然语言问题 qqq，主题实体 h∈Eh\\in\\mathcal{E}h∈E 和答案集合 A⊆E\\mathcal{A}\\subseteq\\mathcal{E}A⊆E, QE 的训练即使得： ϕ(eh,eq,ea)&gt;0 ∀a∈Aϕ(eh,eq,ea‾)&lt;0 ∀a‾∉A\\begin{aligned} \\phi(e_h, e_q, e_a) &gt; 0 &amp;\\ \\ \\forall a \\in \\mathcal{A}\\\\ \\phi(e_h, e_q, e_{\\overline{a}}) &lt; 0 &amp;\\ \\ \\forall \\overline{a} \\notin \\mathcal{A} \\end{aligned}ϕ(eh​,eq​,ea​)&gt;0ϕ(eh​,eq​,ea​)&lt;0​ ∀a∈A ∀a∈/A​ Answer Selection 模块 在推理时，模型为 (h,q)(h, q)(h,q) 对与所有的候选答案 a′∈Ea&#x27;\\in \\mathcal{E}a′∈E 组成的三元组评分。对于相对较小的 KG，直接选出评分最高的实体作为答案： eans=arg max⁡a′∈Eϕ(eh,eq,ea′)e_{ans} = \\argmax_{a&#x27;\\in\\mathcal{E}}\\phi(e_h, e_q, e_{a&#x27;}) eans​=a′∈Eargmax​ϕ(eh​,eq​,ea′​) 但是当 KG 很大时，适当修剪候选答案集合将会显著提升性能。 Relation Matching 与 PullNet[1:2] 类似，作者通过一个评分函数 S(r,q)S(r, q)S(r,q) 来根据自然语言问题 qqq 排序关系 r∈Rr\\in \\mathcal{R}r∈R. 记 hrh_rhr​ 为关系 rrr 的嵌入，q′=(&lt;s&gt;,w1,...,w∣q∣,&lt;/s&gt;)q&#x27;=(&lt;s&gt;, w_1, ..., w_{|q|}, &lt;/s&gt;)q′=(&lt;s&gt;,w1​,...,w∣q∣​,&lt;/s&gt;) 为问题 qqq 输入 RoBERTa 的序列，则评分函数定义为 RoBERTa 的最终隐藏层输出和 hrh_rhr​ 的点积再经过 sigmoid 函数： hq=RoBERTa(q′)S(r,q)=sigmoid(hqThr)\\begin{aligned} h_q = \\text{RoBERTa}(q&#x27;)\\\\ S(r, q) = \\text{sigmoid}(h_q^Th_r) \\end{aligned}hq​=RoBERTa(q′)S(r,q)=sigmoid(hqT​hr​)​ 在所有关系中，选择使得上述评分函数大于0.5的部分记作 Ra\\mathcal{R}_aRa​, 对于每个候选答案实体 a′∈Ea&#x27;\\in \\mathcal{E}a′∈E，找到从 hhh 到 a′a&#x27;a′ 的最短路径，这个路径由关系构成，将路径上的关系记作 Ra′\\mathcal{R}_{a&#x27;}Ra′​, 则候选实体 a′a&#x27;a′ 的 relation score 定义为 Ra\\mathcal{R}_aRa​ 与 Ra′\\mathcal{R}_{a&#x27;}Ra′​ 的交集大小：RelScorea′=∣Ra∩Ra′∣\\text{RelScore}_{a&#x27;} = |\\mathcal{R}_a\\cap\\mathcal{R}_{a&#x27;}|RelScorea′​=∣Ra​∩Ra′​∣ 最终使用 relation score 和 ComplEx score 的线性组合寻找答案： eans=arg max⁡a′∈Nhϕ(eh,eq,ea′)+γ∗RelScorea′e_{ans} = \\argmax_{a&#x27;\\in\\mathcal{N}_h}\\phi(e_h, e_q, e_{a&#x27;}) + \\gamma*\\text{RelScore}_{a&#x27;} eans​=a′∈Nh​argmax​ϕ(eh​,eq​,ea′​)+γ∗RelScorea′​ Pullnet: Open domain question answering with iterative retrieval on Knowledge bases and text. Haitian Sun, Tania Bedrax-Weiss, William W Cohen. arXiv: 1904.09537 ↩︎ ↩︎ ↩︎ Open domain question answering using early fusion of knowledge bases and text. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, William W Cohen. arXiv: 1809.00782 ↩︎ Translating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko. NIPS 2013 ↩︎ ↩︎ ↩︎ Complex embeddings for simple link prediction. Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, Guillaume Bouchard. ICML 2016 ↩︎ ↩︎ Embedding entities and relations for learning and inference in knowledge bases. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng. arXiv: 1412.6575 ↩︎ A three-way model for collective learning on multi-relational data. Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel. ICML 2011 ↩︎ Semantic parsing via staged query graph generation: Question answering with knowledge base. Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, Jianfeng Gao. ACL 2015 ↩︎ Constraint-based question answering with knowledge graph. Junwei Bao, Nan Duan, Zhao Yan, Ming Zhou, Tiejun Zhao. COLING 2016 ↩︎ Question answering with subgraph embeddings. Antoine Bordes, Sumit Chopra, Jason Weston. arXiv: 1406.3676 ↩︎ Large-scale simple question answering with memory networks. Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston. arXiv: 1506.02075 ↩︎ Open question answering with weakly supervised embedding models. Antoine Bordes, Jason Weston, Nicolas Usunier. arXiv: 1404.4326 ↩︎ Rotate: Knowledge graph embedding by relational rotation in complex space. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, Jian Tang. arXiv: 1902.10197 ↩︎","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"},{"name":"Embedding","slug":"Embedding","permalink":"https://blog.zhuwenq.cc/tags/Embedding/"}],"author":["Apoorv Saxena","Aditay Tripathi","Partha Talukdar"]},{"title":"Layer Normalization","slug":"LayerNormalization","date":"2021-12-02T16:00:00.000Z","updated":"2023-06-21T06:51:28.828Z","comments":true,"path":"LayerNormalization/","link":"","permalink":"https://blog.zhuwenq.cc/LayerNormalization/","excerpt":"Layer Normalization 是针对 Batch Normalization 提出的，两者都是深度神经网络中为解决训练困难而提出的归一化手段。","text":"Layer Normalization 是针对 Batch Normalization 提出的，两者都是深度神经网络中为解决训练困难而提出的归一化手段。 Background 前馈神经网络可以认为是一个将输入 x\\mathbf{x}x 映射到输出向量 yyy 的非线性映射。即下式所示的过程： ail=wilThl,hil+1=f(ail+bil)a_i^{l}={w_i^{l}}^Th^l, h_i^{l+1} = f(a_i^l + b_i^l) ail​=wil​Thl,hil+1​=f(ail​+bil​) 其中 aila_i^lail​ 指第 lll 层第 iii 个隐藏单元的表征输出，wilw_i^lwil​ 指第 lll 层中第 iii 个隐藏单元的权重，hlh^lhl 指输入到第 lll 层的输入表征，f(⋅)f(\\cdot)f(⋅) 是非线性函数，bilb_i^lbil​ 是偏置参数。 这样的深度学习计算过程导致一个层参数的梯度与上一层的输出高度相关，这被称为“协变量偏差”。Batch Normalization 的提出就是为了减弱这种偏差。 Batch Normalization BN 将训练样本在每个隐藏单元上的输入归一化，具体地说，对于第 lll 层上第 iii 个输入，BN 根据训练数据的分布缩放隐藏单元的输入。 a‾il=gilσil(ail−μil),μil=Ex∼P(x)[ail],σil=Ex∼P(x)[(ail−μil)2]\\overline{a}_i^l = \\frac{g_i^l}{\\sigma_i^l}(a_i^l - \\mu_i^l), \\mu_i^l=\\mathbb{E}_{\\mathbf{x}\\sim P(\\mathbf{x})}[a_i^l], \\sigma_i^l = \\sqrt{\\mathbb{E}_{\\mathbf{x}\\sim P(\\mathbf{x})}[(a_i^l - \\mu_i^l)^2]} ail​=σil​gil​​(ail​−μil​),μil​=Ex∼P(x)​[ail​],σil​=Ex∼P(x)​[(ail​−μil​)2]​ 其中 a‾il\\overline{a}_i^lail​ 是第 lll 层第 iii 个隐藏单元的归一化后的输入，gig_igi​ 是增益参数。 上述公式中计算统计参数 μ\\muμ 和 σ\\sigmaσ 是在整个训练集 P(x)P(\\mathbf{x})P(x) 上进行，但是这在实践中不可能完成。因为这需要在某个 Batch 的计算中引入不属于该 Batch 的分布信息，因此统计参数由当前的 mini-batch 中的样本中估计。这客观上限制了 mini-batch 的大小不能太小，并且很难将其应用在 RNN 中 Layer Normalization 作者考虑到，如果固定深度神经网络每一层的输入数据的均值和方差，可以减弱上述的协变量偏移问题。LN 中统计参数的计算过程如下： μl=1H∑i=1Hail,σl=1H∑i=1H(ail−μl)2\\mu^l = \\frac{1}{H}\\sum_{i=1}^{H}a_i^l, \\sigma^l = \\sqrt{\\frac{1}{H}\\sum_{i=1}^H(a_i^l - \\mu^l)^2} μl=H1​i=1∑H​ail​,σl=H1​i=1∑H​(ail​−μl)2​ 其中 HHH 代表第 lll 层(当前层)中隐藏单元的数量。 与 BN 不同的是，LN 中同一层的所有隐藏单元共享相同的统计参数 μ\\muμ 和 σ\\sigmaσ ，不同的训练样本计算出不同的 μ\\muμ 和 σ\\sigmaσ，而 BN 中则是同一 Batch 中所有的训练样本在相同的隐藏单元上共享相同的统计参数，不同的隐藏单元计算出不同的统计参数。这一区别使得 LN 取消了对 mini-batch 尺寸的限制，甚至可以进行 batch-size 为 1 的纯在线训练过程。 LN 在 RNN 中应用","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"ML","slug":"ML","permalink":"https://blog.zhuwenq.cc/tags/ML/"}],"author":["Jimmy Lei Ba","Jamie Ryan Kiros","Geoffrey E. Hinton"]},{"title":"ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning","slug":"ERICA-Improving-Entity-and-Relation-Understanding-for-Pre-trained-Language-Models-via-Contrastive-Learning","date":"2021-11-26T16:00:00.000Z","updated":"2023-06-21T06:51:28.788Z","comments":true,"path":"ERICA-Improving-Entity-and-Relation-Understanding-for-Pre-trained-Language-Models-via-Contrastive-Learning/","link":"","permalink":"https://blog.zhuwenq.cc/ERICA-Improving-Entity-and-Relation-Understanding-for-Pre-trained-Language-Models-via-Contrastive-Learning/","excerpt":"Motivation 预训练语言模型拥有很强的表征能力，可以利用预训练语言模型生成的语言表征高效捕捉文本的语法和语义特征。作者认为目前平凡的预训练目标不能明确建模relational facts，而relational facts对于理解整个文本十分重要。现有的关于建模实体及实体之间关系的研究主要关注于句内实体之间的孤立关系，忽略了在整个文档层面上实体之间的关系。","text":"Motivation 预训练语言模型拥有很强的表征能力，可以利用预训练语言模型生成的语言表征高效捕捉文本的语法和语义特征。作者认为目前平凡的预训练目标不能明确建模relational facts，而relational facts对于理解整个文本十分重要。现有的关于建模实体及实体之间关系的研究主要关注于句内实体之间的孤立关系，忽略了在整个文档层面上实体之间的关系。 Contribution 提出了两个预训练目标，用以更好地捕获in-text facts： Entity Discrimination task(实体区分任务)，用给定的头实体和关系区分出尾实体 Relation Discrimination task(关系区分任务)，区分两个给定的关系是否语义上相似 Methodology 数据构建和形式化描述 ERICA 在大规模无标签的语料上训练，并由外部知识图谱 K\\mathcal{K}K 进行远程监督。 首先将数据以文档为单位分批， D={di}i=1∣D∣\\mathcal{D} = \\{d_i\\}_{i=1}^{|\\mathcal{D}|}D={di​}i=1∣D∣​ 是一个包含若干文档的一个 batch。 Ei={eij}j=1∣Ei∣\\mathcal{E}_i = \\{e_{ij}\\}_{j=1}^{|\\mathcal{E}_i|}Ei​={eij​}j=1∣Ei​∣​ 是文档 did_idi​ 中出现的所有实体的集合，其中 eije_{ij}eij​ 是文档 did_idi​ 中的第 jjj 个实体。 对于文档 did_idi​, 作者枚举出其中的所有实体对 (eij,eik)(e_{ij}, e_{ik})(eij​,eik​), 并将它们以 K\\mathcal{K}K 中的关系 rjkir_{jk}^irjki​ 连接起来，这样就得到了一个元组集 Ti={tjki=(di,eij,rjki,eik)∣j≠k}\\mathcal{T}_i = \\{t_{jk}^i=(d_i, e_{ij}, r_{jk}^i, e_{ik})|j\\neq k\\}Ti​={tjki​=(di​,eij​,rjki​,eik​)∣j=k}. 对于 K\\mathcal{K}K 中不存在关系的实体对(out-of-KG 问题), 将其关系设置为 no_relation. 对于整个批次，将其中每个文档的元组集拼接得到整个批次的元组集 T=Ti∪T2∪...∪T∣D∣\\mathcal{T} = \\mathcal{T}_i\\cup\\mathcal{T}_2\\cup...\\cup\\mathcal{T}_{|\\mathcal{D}|}T=Ti​∪T2​∪...∪T∣D∣​. 进一步，作者通过去除 T\\mathcal{T}T 中所有关系为 no_relation 的元素构建了阳性元组集 T+\\mathcal{T}^+T+. T+\\mathcal{T}^+T+ 中包含句内实体对和句间实体对。 实体和关系的表征方法 对于每个文档 did_idi​ ，作者首先使用 PLM 对齐进行编码，计算出每个 token 的表征 {h1,h2,...,h∣di∣}\\{\\mathbf{h}_1, \\mathbf{h}_2, ..., \\mathbf{h}_{|d_i|}\\}{h1​,h2​,...,h∣di​∣​}, 然后对于文档中的每个实体 eije_{ij}eij​, 作者通过对这个实体所包含的所有 token 的表征进行均值池化(mean pooling) 获得这个实体本次出现的局部表征。由于一个实体在文档中可能多次出现，其第 kkk 次出现的局部表征记为： meijk=MeanPool(hnstartk,...,hnendk)\\mathbf{m}_{e_{ij}}^k = \\text{MeanPool}(\\mathbf{h}_{n_{start}^k}, ..., \\mathbf{h}_{n_{end}^k}) meij​k​=MeanPool(hnstartk​​,...,hnendk​​) 其中，nstartkn_{start}^knstartk​ 和 nendkn_{end}^knendk​ 分别是实体 eije_{ij}eij​ 所包含的 token 在文档 did_idi​ 中第 kkk 次出现的起始位置和终止位置。 同时作者将文档中 eije_{ij}eij​ 的所有局部表征取均值作为其在文档中的全局特征 eij\\mathbf{e}_{ij}eij​，作者认为eij\\mathbf{e}_{ij}eij​中包含该实体在该文档中的全部信息 对于文档中两个实体 eij1,eij2e_{ij_1}, e_{ij_2}eij1​​,eij2​​ 之间的关系，作者将其全局表征拼接起来作为其关系的表征 rj1j2i=[eij1;eij2]\\mathbf{r}_{j_1j_2}^i = [\\mathbf{e}_{ij_1}; \\mathbf{e}_{ij_2}]rj1​j2​i​=[eij1​​;eij2​​] 两个预训练目标 Entity Discrimination, ED 实体辨别任务是根据给定的文档和头实体以及关系推断出尾实体。作者认为，这个任务可以促使PLM通过实体之间的关系理解实体。 实践中，先从 T+\\mathcal{T}^+T+ 中采样一个元组 tjki=(di,eij,rjki,eik)t_{jk}^{i} = (d_i, e_{ij}, r_{jk}^{i}, e_{ik})tjki​=(di​,eij​,rjki​,eik​). 然后使用 PLM 区分GT尾实体和文档 did_idi​ 中的其他实体。实际的数据构造成如下的格式： di∗=&quot;relation_name entity_mention[SEP]di&quot;d_i^*=\\text{&quot;relation\\_name entity\\_mention[SEP]}d_i\\text{&quot;} di∗​=&quot;relation_name entity_mention[SEP]di​&quot; 即将关系 rjkir_{jk}^irjki​ 的名称，头实体 eije_{ij}eij​ 的 mention(文字) 放在文档 did_idi​ 的前面，并用 [SEP]\\text{[SEP]}[SEP] 隔开。 该任务的目标就是最大化如下后验概率： P(eik∣eij,rjki)=softmax(f(eik))\\mathcal{P}(e_{ik}|e_{ij}, r_{jk}^i) = \\text{softmax}(f(\\mathbf{e}_{ik})) P(eik​∣eij​,rjki​)=softmax(f(eik​)) 其中 f(⋅)f(\\cdot)f(⋅) 是一个实体分类器。 作者实验发现直接优化上述后验概率并不能很好地考虑实体之间的关系，因此借鉴对比学习的思想，训练表征使实体正样本(eij,eik)(e_{ij}, e_{ik})(eij​,eik​)之间的余弦距离比负样本更近。ED任务的损失函数设计为： LED=−∑tjki∈T+log⁡exp⁡(cos⁡(eij,eik)/τ)∑l=1,l≠j∣Ei∣exp⁡(cos⁡(eij,eil)/τ)\\mathcal{L}_\\text{ED}=-\\sum_{t_{jk}^i\\in \\mathcal{T}^+}\\log\\frac{\\exp(\\cos(\\mathbf{e}_{ij}, \\mathbf{e}_{ik})/\\tau)}{\\sum_{l=1, l\\neq j}^{|\\mathcal{E}_i|}\\exp(\\cos(\\mathbf{e}_{ij}, \\mathbf{e}_{il})/\\tau)} LED​=−tjki​∈T+∑​log∑l=1,l=j∣Ei​∣​exp(cos(eij​,eil​)/τ)exp(cos(eij​,eik​)/τ)​ 其中的 τ\\tauτ 是一个超参数。 Relation Discrimination, RD 关系辨别任务即辨别两个关系是否语义上相似，与现存的关系增强的PLM相比，本文提出的方法主要是使用了文档级别的关系而不是句内关系的远程监督。作者认为这样可以使PLM学会现实世界中的复杂推理链。作者为实体对之间的关系文本训练表征，并优化使共享相同关系的不同实体对之间的关系表征在语义空间中相近。 实践上，作者从 Ts+\\mathcal{T}_s^+Ts+​ 或 Tc+\\mathcal{T}_c^+Tc+​ 中为每种关系线性采样(每种关系对应的采样率与该关系的数量在挡墙batch中的占比成正比)元组对 tA=(dA,eA1,rA,eAe),tB=(dB,eB1,rB,eB2)t_A=(d_A, e_{A_1}, r_A, e_{A_e}), t_B=(d_B, e_{B_1}, r_B, e_{B_2})tA​=(dA​,eA1​​,rA​,eAe​​),tB​=(dB​,eB1​​,rB​,eB2​​) 其中 rA=rBr_A=r_BrA​=rB​. 然后使用实体和关系的表征方法中提到的方法对 rAr_ArA​ 和 rBr_BrB​ 进行编码，获得 rtA\\mathbf{r}_{t_A}rtA​​ 和 rtB\\mathbf{r}_{t_B}rtB​​ . 然后，与ED类似，作者也使用了对比学习的思想构建RD的损失函数： LRDT1,T2=−∑tA∈T1,tB∈T2log⁡exp⁡(cos⁡(rtA,rrB)/τ)ZZ=∑tC∈T/{tA}Nexp⁡(cos⁡(rtA,rrB)/τ)LRD=LRDTs+,Ts++LRDTs+,Tc++LRDTc+,Ts++LRDTc+,Tc+\\begin{aligned} \\mathcal{L}_\\text{RD}^{\\mathcal{T}_1, \\mathcal{T}_2} &amp;= -\\sum_{t_A\\in \\mathcal{T}_1, t_B\\in \\mathcal{T}_2}\\log\\frac{\\exp(\\cos(\\mathbf{r}_{t_A}, \\mathbf{r}_{r_B})/\\tau)}{\\mathcal{Z}} \\\\ \\mathcal{Z} &amp;= \\sum_{t_C\\in \\mathcal{T}/\\{t_A\\}}^N\\exp(\\cos(\\mathbf{r}_{t_A}, \\mathbf{r}_{r_B})/\\tau) \\\\ \\mathcal{L}_\\text{RD} &amp;= \\mathcal{L}_\\text{RD}^{\\mathcal{T}_s^+, \\mathcal{T}_s^+} + \\mathcal{L}_\\text{RD}^{\\mathcal{T}_s^+, \\mathcal{T}_c^+} + \\mathcal{L}_\\text{RD}^{\\mathcal{T}_c^+, \\mathcal{T}_s^+} + \\mathcal{L}_\\text{RD}^{\\mathcal{T}_c^+, \\mathcal{T}_c^+} \\end{aligned}LRDT1​,T2​​ZLRD​​=−tA​∈T1​,tB​∈T2​∑​logZexp(cos(rtA​​,rrB​​)/τ)​=tC​∈T/{tA​}∑N​exp(cos(rtA​​,rrB​​)/τ)=LRDTs+​,Ts+​​+LRDTs+​,Tc+​​+LRDTc+​,Ts+​​+LRDTc+​,Tc+​​​ 其中 NNN 是超参数。实验中确保 tBt_BtB​ 从 Z\\mathcal{Z}Z 中采样并且从 T\\mathcal{T}T 中构建 N−1N-1N−1 个负样本。 总体训练目标 为避免灾难性遗忘，ERICA 还采用了MLM目标任务，因此总的训练目标是： L=LED+LRD+LMLM\\mathcal{L} = \\mathcal{L}_\\text{ED} + \\mathcal{L}_\\text{RD} + \\mathcal{L}_\\text{MLM} L=LED​+LRD​+LMLM​ 实验 远程监督数据集构建 预训练参数细节 RE 任务上与 CNN，BILSTM，BERT，RoBERTa，HINBERT，CorefBERT，SpanBERT，ERINE，MTB，CP比较。分别在 Document-level 和 Sentence-level 上比较 Multi-Chioce QA 任务上与 FastQA, BiDAF, BERT, RoBERTa, CorefBERT, SpanBERT, MTB 和 CP 比较。Extractive QA 上与 BERT，RoBERTa，MTB，CP 比较 消融实验，探索了 LED\\mathcal{L}_\\text{ED}LED​ 和 LRD\\mathcal{L}_\\text{RD}LRD​ 的作用，分析了预训练数据的 domain，size 和 实体编码的方法对性能的影响","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"Knowledge Embedding","slug":"Knowledge-Embedding","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Embedding/"}]},{"title":"Rigid Formats Controlled Text Generation","slug":"Rigid-Formats-Controlled-Text-Generation","date":"2021-10-19T16:00:00.000Z","updated":"2023-06-21T06:51:28.884Z","comments":true,"path":"Rigid-Formats-Controlled-Text-Generation/","link":"","permalink":"https://blog.zhuwenq.cc/Rigid-Formats-Controlled-Text-Generation/","excerpt":"Motivation 一般的文本生成任务没有严格的格式限制，像宋词，十四行诗，歌词等文本由严格的格式或韵律控制，而关于受控格式的文本生成还未被充分研究过。","text":"Motivation 一般的文本生成任务没有严格的格式限制，像宋词，十四行诗，歌词等文本由严格的格式或韵律控制，而关于受控格式的文本生成还未被充分研究过。 刚性格式控制的文本生成 生成的文本必须： 完全遵守预定义的刚性格式 文本的安排必须满足韵律格式 必须保证句子的完整性 现有的如诗歌写作任务的工作只是将文本的格式和韵律作为一种数据的隐式特征在训练过程中学习。这种模型不具有泛化性，在五言绝句数据上训练的模型不能用于七言绝句的生成。 任务定义 输入一个刚性格式 C∈CC\\in \\mathcal{C}C∈C: C={c0c1c2c3,c0c1c2c3c4c5.}C = \\{c_0c_1c_2c_3, c_0c_1c_2c_3c_4c_5.\\} C={c0​c1​c2​c3​,c0​c1​c2​c3​c4​c5​.} 其中 C\\mathcal{C}C 是所有可能的格式的集合。C\\mathcal{C}C 是一个可以扩展自定义格式的集合，因此 ∣C∣→∞|\\mathcal{C}|\\rightarrow \\infty∣C∣→∞. 其中的 cic_ici​ 表示一个占位符，代表需要生成一个实际的字的位置。CCC 中定义了字的个数和标点符号的位置。 输出的是一个符合格式 CCC 的自然语言语句 Y∈YY\\in \\mathcal{Y}Y∈Y, 例： Y=love is not love,bends with the remover to remove.\\begin{array}{rl} Y = &amp;\\text{love is not love,}\\\\ &amp;\\text{bends with the remover to remove.} \\end{array}Y=​love is not love,bends with the remover to remove.​ 因为 C\\mathcal{C}C 是一个可以扩展的格式集，可以基于生成的结果 YYY 重建一个新的格式 C′C&#x27;C′, 重建的方式为 mask 掉部分的内容： C′={c0c1c2 love,c0c1c2c3c4 remove.}C&#x27; = \\{c_0c_1c_2 \\text{ love}, c_0c_1c_2c_3c_4 \\text{ remove}.\\} C′={c0​c1​c2​ love,c0​c1​c2​c3​c4​ remove.} 即两个子句的最后一个词必须是 love\\text{love}love 和 remove\\text{remove}remove。 然后根据新生成的格式 C′C&#x27;C′ 重新生成文本，此过程称为抛光(polishing) 该任务的目标即找到一个映射函数 GGG 来根据格式生成文本： Y=G(C)Y = G(C) Y=G(C) Methodology 基本框架是一个基于 Transformer 的自回归语言模型，输入可以是整个的宋词或十四行诗的 token 序列。作者设计了几种 embedding 来增强在格式，韵律和句子完整性上的准确率。同时改进了注意力机制来促使模型捕捉格式信息特征。 以一段莎士比亚十四行诗中的句子为例细说: 原始语句：love is not love, bends with the remover to remove.\\text{love is not love, bends with the remover to remove.}love is not love, bends with the remover to remove. 输入语句：&lt;bos&gt; love is not love, &lt;/s&gt;bends with the remover to remove.&lt;/s&gt;\\text{&lt;bos&gt; love is not love, &lt;/s&gt;bends with the remover to remove.&lt;/s&gt;}&lt;bos&gt; love is not love, &lt;/s&gt;bends with the remover to remove.&lt;/s&gt; 输出是输入的左移版本：$$\\begin{array}{c} \\text{love is not love,}\\ \\text{bends with the remover to remove.} \\end{array}$$ 其中的 &lt;/s&gt;&lt;/s&gt;&lt;/s&gt; 是句子分隔符。 新的嵌入 Format and Rhyme Symbols: C={c0,c0,c0,c2,c1,&lt;/s&gt;c0,c0,c0,c0,c0,c2,c1,&lt;/s&gt;,&lt;eos&gt;}\\begin{aligned} C = \\{&amp;c_0, c_0, c_0, c_2, c_1, &lt;/s&gt;\\\\ &amp;c_0, c_0, c_0, c_0, c_0, c_2, c_1, &lt;/s&gt;, &lt;eos&gt;\\} \\end{aligned}C={​c0​,c0​,c0​,c2​,c1​,&lt;/s&gt;c0​,c0​,c0​,c0​,c0​,c2​,c1​,&lt;/s&gt;,&lt;eos&gt;}​ 其中的 c0c_0c0​ 表示普通的 token， c1c_1c1​ 表示标点符号， c2c_2c2​ 表示韵脚位置。 Intra-Position Symbols: P={p4,p3,p2,p1,p0,&lt;/s&gt;p6,p5,p4,p3,p2,p1,p0,&lt;/s&gt;,&lt;eos&gt;}\\begin{aligned} P = \\{&amp;p_4, p_3, p_2, p_1, p_0, &lt;/s&gt;\\\\ &amp;p_6, p_5, p_4, p_3, p_2, p_1, p_0, &lt;/s&gt;, &lt;eos&gt;\\} \\end{aligned}P={​p4​,p3​,p2​,p1​,p0​,&lt;/s&gt;p6​,p5​,p4​,p3​,p2​,p1​,p0​,&lt;/s&gt;,&lt;eos&gt;}​ 其中 pip_ipi​ 表示 token 在分句中的局部位置。作者特意采用递减的顺序进行编码，以便促使该 embedding 更好地捕获分句结束的信息特征(p0p_0p0​ 总是表示标点符号的位置，p1p_1p1​ 总是表示句子的最后一个词) Segment Symbols: S={s0,s0,s0,s0,s0,&lt;/s&gt;s1,s1,s1,s1,s1,s1,s1,&lt;/s&gt;,&lt;eos&gt;}\\begin{aligned} S = \\{&amp;s_0, s_0, s_0, s_0, s_0, &lt;/s&gt;\\\\ &amp;s_1, s_1, s_1, s_1, s_1, s_1, s_1, &lt;/s&gt;, &lt;eos&gt;\\} \\end{aligned}S={​s0​,s0​,s0​,s0​,s0​,&lt;/s&gt;s1​,s1​,s1​,s1​,s1​,s1​,s1​,&lt;/s&gt;,&lt;eos&gt;}​ 其中的 sis_isi​ 表示该位置的 token 属于哪个分句。该 embedding 旨在增强不同位置不同子句之间的交互。 在训练时，上述的 embedding 和 token 的词嵌入以及全局位置嵌入相加作为序列的输入嵌入。 Ht0=Ewt+Ect+Ept+Est+Egt\\mathbf{H}_t^0 = \\mathbf{E}_{w_t} + \\mathbf{E}_{c_t} + \\mathbf{E}_{p_t} + \\mathbf{E}_{s_t} + \\mathbf{E}_{g_t} Ht0​=Ewt​​+Ect​​+Ept​​+Est​​+Egt​​ 其中的 Ht0\\mathbf{H}_t^0Ht0​ 表示第 ttt 个 token 的第 000 层表征，E∗\\mathbf{E}_*E∗​ 表示各种的 embedding，wtw_twt​ 表示 token ttt 的词嵌入，c,p,sc, p, sc,p,s 表示上述的三种自定义 embedding，ggg 表示 token 的全局位置，与 Transformer 的相同。 修改注意力机制(的内容) 模型在自回归生成位置 ttt 的 token 时，还需要一些靠后的位置的信息来掌握全局的动态句子信息。如模型可能想知道当前的 token 是否应该是标点符号或者是否是最后一个 token。为了在预测的过程种引入这些信息，作者引入了另一个注意力参数 F0\\mathbf{F}^0F0: Ft0=Ect+Ept+Est\\mathbf{F}_t^0 = \\mathbf{E}_{c_t} + \\mathbf{E}_{p_t} + \\mathbf{E}_{s_t} Ft0​=Ect​​+Ept​​+Est​​ 作者将注意力计算过程分为两个部分： Masking Multi-Head Self-Attention: K0,V0=H0WK,H0WVQ0=H0WQCt1=LN(SLF-ATT(Qt0,K≤t0,V≤t0)+Ht0)Ct1=LN(FFN(Ct1)+Ct1)\\begin{aligned} \\mathbf{K}^0, \\mathbf{V}^0 &amp;= \\mathbf{H}^0\\mathbf{W}^K, \\mathbf{H}^0\\mathbf{W}^V\\\\ \\mathbf{Q}^0 &amp;= \\mathbf{H}^0\\mathbf{W}^Q\\\\ \\mathbf{C}_t^1 &amp;= \\normalsize\\text{L}\\footnotesize\\text{N}(\\normalsize\\text{S}\\footnotesize\\text{LF-}\\normalsize\\text{A}\\footnotesize\\text{TT}(\\mathbf{Q}_t^0, \\mathbf{K}_{\\leq t}^0, \\mathbf{V}_{\\leq t}^0) + \\mathbf{H}_t^0)\\\\ \\mathbf{C}_t^1 &amp;= \\normalsize\\text{L}\\footnotesize\\text{N}(\\normalsize\\text{F}\\footnotesize\\text{FN}(\\mathbf{C}_t^1) + \\mathbf{C}_t^1) \\end{aligned}K0,V0Q0Ct1​Ct1​​=H0WK,H0WV=H0WQ=LN(SLF-ATT(Qt0​,K≤t0​,V≤t0​)+Ht0​)=LN(FFN(Ct1​)+Ct1​)​ 该过程是标准的 Masking Multi-Head Self-Attention 过程，在进行自注意力运算时只 attend to ≤t\\leq t≤t 的文本表征(K,V\\mathbf{K, V}K,V), 即将 &gt;t&gt; t&gt;t 的位置 mask 成 −∞-\\infty−∞ Global Multi-Head Attention: K1,V1=F0WK,F0WVQ1=C1WQHt1=LN(GLOBAL-ATT(Qt1,K1,V1)+Ct1)Ht1=LN(FFN(Ht1)+Ht1)\\begin{aligned} \\mathbf{K}^1, \\mathbf{V}^1 &amp;= \\mathbf{F}^0\\mathbf{W}^K, \\mathbf{F}^0\\mathbf{W}^V\\\\ \\mathbf{Q}^1 &amp;= \\mathbf{C}^1\\mathbf{W}^Q\\\\ \\mathbf{H}_t^1 &amp;= \\normalsize\\text{L}\\footnotesize\\text{N}(\\normalsize\\text{G}\\footnotesize\\text{LOBAL-}\\normalsize\\text{A}\\footnotesize\\text{TT}(\\mathbf{Q}_t^1, \\mathbf{K}^1, \\mathbf{V}^1) + \\mathbf{C}_t^1)\\\\ \\mathbf{H}_t^1 &amp;= \\normalsize\\text{L}\\footnotesize\\text{N}(\\normalsize\\text{F}\\footnotesize\\text{FN}(\\mathbf{H}_t^1) + \\mathbf{H}_t^1) \\end{aligned}K1,V1Q1Ht1​Ht1​​=F0WK,F0WV=C1WQ=LN(GLOBAL-ATT(Qt1​,K1,V1)+Ct1​)=LN(FFN(Ht1​)+Ht1​)​ 在该过程中，所有上下文的 F0\\mathbf{F}^0F0 都参与计算， F0\\mathbf{F}^0F0 中只有输入的格式，韵律和分段以及相对位置信息，并不包含词本身的信息。 上述两个注意力计算块重复 LLL 层就获得了最终表征 HL\\mathbf{H}^LHL. LLL 层的计算中，HlH^lHl 每层都会更新，而 F0\\mathbf{F}^0F0 是全局固定的。 训练目标是最小化整个序列的负对数似然： Lnll=−∑t=1nlog⁡P(yt∣y&lt;t)\\mathcal{L}^{\\text{nll}} = -\\sum_{t = 1}^n\\log P(\\mathbf{y}_t|\\mathbf{y}_{&lt;t}) Lnll=−t=1∑n​logP(yt​∣y&lt;t​) 一些与刚性文本格式(宋词，十四行诗)相关的评价指标等细节","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Text Generation","slug":"Text-Generation","permalink":"https://blog.zhuwenq.cc/tags/Text-Generation/"}]},{"title":"Translating Embeddings for Modeling Multi-relational Data","slug":"Translating-Embeddings-for-Modeling-Multi-relational-Data","date":"2021-10-13T16:00:00.000Z","updated":"2023-06-21T06:51:28.932Z","comments":true,"path":"Translating-Embeddings-for-Modeling-Multi-relational-Data/","link":"","permalink":"https://blog.zhuwenq.cc/Translating-Embeddings-for-Modeling-Multi-relational-Data/","excerpt":"Motivation 本文主要针对在低维向量空间中嵌入多关系数据(Multi-relational data)的实体和关系信息的问题。 Multi-relational Data, 指节点形如 (head,label,tail)(head, label, tail)(head,label,tail) 的有向图，节点中的三元组分别代表头实体(head entity), 尾实体(tail entity)和头尾实体之间的关系(label)。本文的工作主要就关注于如何建模 Multi-relation data 的问题。 Modeling multi-relational data 多关系数据建模过程可以归结为实体之间的局部或全局关系模式的抽取过程，并根据观察到的关系模式泛化到所有实体之间进行预测。单一关系的局部性可能是纯粹结构性的，但也与实体类型相关。一般的关系数据则更加复杂，同时涉及关系和实体的类型。因此关系数据是异构的，需要更加泛化性的方法建模。 现存的方法大多基于隐式属性学习的框架，即通过学习实体和关系数据的隐藏表征进行建模。这些工作集中于增加模型的表现力和通用性，一般采用贝叶斯聚类框架或者基于能量的框架在低维向量空间学习实体的表征。这些模型的表现力的提升是以模型复杂度的增加作为代价，这导致其建模假设难以解释，同时带来计算成本的提高。","text":"Motivation 本文主要针对在低维向量空间中嵌入多关系数据(Multi-relational data)的实体和关系信息的问题。 Multi-relational Data, 指节点形如 (head,label,tail)(head, label, tail)(head,label,tail) 的有向图，节点中的三元组分别代表头实体(head entity), 尾实体(tail entity)和头尾实体之间的关系(label)。本文的工作主要就关注于如何建模 Multi-relation data 的问题。 Modeling multi-relational data 多关系数据建模过程可以归结为实体之间的局部或全局关系模式的抽取过程，并根据观察到的关系模式泛化到所有实体之间进行预测。单一关系的局部性可能是纯粹结构性的，但也与实体类型相关。一般的关系数据则更加复杂，同时涉及关系和实体的类型。因此关系数据是异构的，需要更加泛化性的方法建模。 现存的方法大多基于隐式属性学习的框架，即通过学习实体和关系数据的隐藏表征进行建模。这些工作集中于增加模型的表现力和通用性，一般采用贝叶斯聚类框架或者基于能量的框架在低维向量空间学习实体的表征。这些模型的表现力的提升是以模型复杂度的增加作为代价，这导致其建模假设难以解释，同时带来计算成本的提高。 Methodology 本文提出 TransE 模型，它是一个用于实体低维嵌入学习的基于能量的模型。 TransE 将关系视为嵌入空间中的转移，即：如果关系 (h,l,t)(h, l, t)(h,l,t) 存在，那么尾实体 ttt 的 embedding 应该与头实体 hhh 的 embedding 和一些与关系 lll 相关的向量的加和相近。 Translation-based model 定义训练集 SSS, SSS 中的元素为三元组 (h,l,t)(h, l, t)(h,l,t). 定义实体集 EEE, 关系集 LLL, 训练集中三元组的 h,l∈Eh, l \\in Eh,l∈E 和 lll 分别为 EEE 中的实体和 LLL 中的关系。模型为实体和关系学习 kkk 维嵌入向量 h,l,t∈Rk\\mathbf{h, l, t} \\in \\mathbb{R}^kh,l,t∈Rk. 模型的基本思路是：当关系 (h,l,t)(h, l, t)(h,l,t) 存在时，使关系 h+l≈t\\mathbf{h} + \\mathbf{l} \\approx \\mathbf{t}h+l≈t 也成立( t\\mathbf{t}t 应该是距离 h+l\\mathbf{h + l}h+l 最近的向量 )。 根据基于能量的框架(Energy-based Framework), 一个三元组的能量等于 d(h+l,t)d(\\mathbf{h} + \\mathbf{l}, \\mathbf{t})d(h+l,t), 其中 ddd 是某种不相似性评价函数( L1L_1L1​ 或 L2L_2L2​ 范数 )。 训练时，在训练集上最小化下面基于边际的排序标准(margin-based ranking criterion): L=∑(h,l,t)∈S∑(h′,l,t′)∈S(h,l,t)′[γ+d(h+l,t)−d(h′+l,t′)]+\\mathcal{L} = \\sum_{(h, l, t) \\in S}\\sum_{(h&#x27;, l, t&#x27;) \\in S&#x27;_{(h, l, t)}}[\\gamma + d(\\mathbf{h} + \\mathbf{l}, \\mathbf{t}) - d(\\mathbf{h&#x27;} + \\mathbf{l}, \\mathbf{t&#x27;})]_+ L=(h,l,t)∈S∑​(h′,l,t′)∈S(h,l,t)′​∑​[γ+d(h+l,t)−d(h′+l,t′)]+​ 其中的 [x]+[x]_+[x]+​ 表示 xxx 的正值部分，γ&gt;0\\gamma &gt; 0γ&gt;0 是一个超参数， S′S&#x27;S′ 定义如下： S(h,l,t)′={(h′,l,t)∣h′∈E}∪{(h,l,t′)∣t′∈E}S&#x27;_{(h, l, t)} = \\{(h&#x27;, l, t)|h&#x27; \\in E\\} \\cup \\{(h, l, t&#x27;)|t&#x27; \\in E\\} S(h,l,t)′​={(h′,l,t)∣h′∈E}∪{(h,l,t′)∣t′∈E} S′S&#x27;S′ 由训练集中的三元组随机替换一个头实体或尾实体(同时只替换一个)组成。 优化的过程通过在所有可能的 (h,l,t)(h, l, t)(h,l,t) 上进行随机梯度下降算法完成(minibatch mode), 一个附加限制是学习到的所有实体的嵌入其 L2L_2L2​ 范数等于 111(关系的嵌入不受此限制) ，这是为了防止训练过程通过减小实体的嵌入向量来最小化损失函数。 TransE 的训练过程如下： 所有实体和关系的嵌入向量首先由随机过程初始化，在每次迭代过程中，首先归一化实体的嵌入向量(归一化限制), 然后从训练集中采样一个小的子集作为训练的 minibatch 。对于每一个训练三元组，为其采样一个受损样本，然后在恒定学习率下根据梯度更新参数。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"Knowledge Embedding","slug":"Knowledge-Embedding","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Embedding/"},{"name":"TransE","slug":"TransE","permalink":"https://blog.zhuwenq.cc/tags/TransE/"}]},{"title":"ERNIE: Enhanced Language Representation with Informative Entities","slug":"ERNIE-Enhanced-Language-Representation-with-Informative-Entities","date":"2021-10-09T16:00:00.000Z","updated":"2023-06-21T06:51:28.788Z","comments":true,"path":"ERNIE-Enhanced-Language-Representation-with-Informative-Entities/","link":"","permalink":"https://blog.zhuwenq.cc/ERNIE-Enhanced-Language-Representation-with-Informative-Entities/","excerpt":"Motivition BERT 等在大规模语料库上训练的语言表征模型可以提取到丰富的语义信息，作者认为结合知识图谱中的实体信息可以引入额外知识来增强语言表征。 BERT 等语言表征模型虽然已经作为许多 NLP 任务的一部分取得了很好的结果，但是由于没有引入先验知识，在语言理解类任务中存在劣势。例如对于自然语言语句：Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004.\\text{Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004.}Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004., 如果不知道 Blowin’ in the Wind\\text{Blowin’ in the Wind}Blowin’ in the Wind 和 Chronicles: Volume One\\text{Chronicles: Volume One}Chronicles: Volume One 是歌曲，在实体分类任务中就无法分辨 Bob Dylan\\text{Bob Dylan}Bob Dylan 究竟是作家还是歌手。同时也几乎无法提取细粒度关系。因此，引入引入额外的知识信息可以在许多基于知识的任务中受益，例如实体分类和关系识别任务。","text":"Motivition BERT 等在大规模语料库上训练的语言表征模型可以提取到丰富的语义信息，作者认为结合知识图谱中的实体信息可以引入额外知识来增强语言表征。 BERT 等语言表征模型虽然已经作为许多 NLP 任务的一部分取得了很好的结果，但是由于没有引入先验知识，在语言理解类任务中存在劣势。例如对于自然语言语句：Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004.\\text{Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004.}Bob Dylan wrote Blowin’ in the Wind in 1962, and wrote Chronicles: Volume One in 2004., 如果不知道 Blowin’ in the Wind\\text{Blowin’ in the Wind}Blowin’ in the Wind 和 Chronicles: Volume One\\text{Chronicles: Volume One}Chronicles: Volume One 是歌曲，在实体分类任务中就无法分辨 Bob Dylan\\text{Bob Dylan}Bob Dylan 究竟是作家还是歌手。同时也几乎无法提取细粒度关系。因此，引入引入额外的知识信息可以在许多基于知识的任务中受益，例如实体分类和关系识别任务。 Challenge 为了在语言表征中引入额外知识，有以下两个挑战： 结构化知识编码：即如何从知识图谱中提取和编码与输入文本有关的知识信息 异构信息融合：即如何设计一种特殊的预训练目标，以融合语义，句法和知识信息 Contribution 针对结构化知识编码问题：作者首先识别出输入文本中提及的实体，然后将其与其在知识图谱中相应的实体对齐(提取)。在编码方面，作者使用知识嵌入算法（如 TransE）对知识图谱的图结构进行编码，并将编码的知识嵌入作为 ERNIE 的输入。基于文本和知识实体之间的对齐， ERINE 将知识模块中的实体表征集成到了语义模块的底层 与 BERT 类似，作者使用了 MLM 和 NSP 作为预训练目标。除此之外，作者设计了一种新的预训练目标以更好地融合文本和知识特征：随机遮住输入文本中的一些命名实体对齐，并让模型从知识图谱中选择合适的实体来补全遮住的部分。这使得模型必须结合上下文信息和知识信息才能预测被遮住的 tokens 和 entities，从而实现了一个知识性的语言表征模型 Methodology 输入序列记作 {w1,...,wn}\\{w_1, ..., w_n\\}{w1​,...,wn​}, 其中 nnn 是序列的 token 长度。输入序列中的实体序列记作 {e1,...,em}\\{e_1, ..., e_m\\}{e1​,...,em​}, 其中 mmm 是实体序列的长度。将包含所有 token 的词典记作 V\\mathcal{V}V, 包括知识图谱中所有实体的实体列表记作 E\\mathcal{E}E。如果一个 token w∈Vw \\in \\mathcal{V}w∈V 属于一个实体 e∈Ee \\in \\mathcal{E}e∈E, 则定义它们的对齐为：f(w)=ef(w) = ef(w)=e。本文中，将所有的实体对齐到实体短语的第一个 token。 模型架构 ERNIE 的架构包含两个模块： 基础的文本编码器 (T-Encoder\\text{T-Encoder}T-Encoder), 负责捕捉基础的语义和语法信息。（BERT） 上层的知识编码器 (K-Encoder\\text{K-Encoder}K-Encoder), 负责将额外的 token 导向的知识信息融合到文本信息的底层。 T-Encoder,K-Encoder\\text{T-Encoder}, \\text{K-Encoder}T-Encoder,K-Encoder 的层数分别为 N,MN, MN,M 如图中所示，对于一个给定的 token 序列 {w1,...,wn}\\{w_1, ..., w_n\\}{w1​,...,wn​} 和对应的实体序列 {e1,...,em}\\{e_1, ..., e_m\\}{e1​,...,em​}, 文本编码器首先将对每个 token 计算输入 embedding：token embedding + segment embedding + positional embedding. 然后计算语法语义特征(BERT 的过程)： {w1,...,wn}=T-Encoder({w1,...,wn})\\{\\mathbf{w}_1, ..., \\mathbf{w}_n\\} = \\text{T-Encoder}(\\{w_1, ..., w_n\\}) {w1​,...,wn​}=T-Encoder({w1​,...,wn​}) 其中的 T-Encoder(⋅)\\text{T-Encoder}(\\cdot)T-Encoder(⋅) 是一个多层双向 Transformer 编码器，与 BERT 相同。 之后，ERNIE 使用一个知识编码器 K-Encoder\\text{K-Encoder}K-Encoder 来向语言表征插入知识信息。具体地说，首先使用预训练的知识嵌入模型 TransE [[Translating-Embeddings-for-Modeling-Multi-relational-Data]] 计算实体序列 {e1,...,em}\\{e_1, ..., e_m\\}{e1​,...,em​} 的嵌入 {e1,...,em}\\{\\mathbf{e}_1, ..., \\mathbf{e}_m\\}{e1​,...,em​}, 然后将 {w1,...,wn}\\{\\mathbf{w}_1, ..., \\mathbf{w}_n\\}{w1​,...,wn​} 和 {e1,...,em}\\{\\mathbf{e}_1, ..., \\mathbf{e}_m\\}{e1​,...,em​} 送入 K-Encoder\\text{K-Encoder}K-Encoder 进行异构信息融合并计算最终的输出嵌入： {w1o,...,wno},{e1o,...,emo}=K-Encoder({w1,...,wn},{e1,...,em})\\{\\mathbf{w}_1^o, ..., \\mathbf{w}_n^o\\}, \\{\\mathbf{e}_1^o, ..., \\mathbf{e}_m^o\\} = \\text{K-Encoder}(\\{\\mathbf{w}_1, ..., \\mathbf{w}_n\\}, \\{\\mathbf{e}_1, ..., \\mathbf{e}_m\\}) {w1o​,...,wno​},{e1o​,...,emo​}=K-Encoder({w1​,...,wn​},{e1​,...,em​}) {w1o,...,wno},{e1o,...,emo}\\{\\mathbf{w}_1^o, ..., \\mathbf{w}_n^o\\}, \\{\\mathbf{e}_1^o, ..., \\mathbf{e}_m^o\\}{w1o​,...,wno​},{e1o​,...,emo​} 将被用于具体的任务。 知识编码器 K-Encoder\\text{K-Encoder}K-Encoder 中的 Multi-Head Attention\\text{Multi-Head Attention}Multi-Head Attention 部分即正常的自注意力过程: {w~1(i),...,w~n(i)}=MH-ATT({w1(i−1),...,wn(i−1)}){e~1(i),...,e~m(i)}=MH-ATT({e1(i−1),...,em(i−1)})\\begin{array}{rl} \\{\\widetilde{\\mathbf{w}}_1^{(i)}, ..., \\widetilde{\\mathbf{w}}_n^{(i)}\\} &amp;= \\text{MH-ATT}(\\{\\mathbf{w}_1^{(i-1)}, ..., \\mathbf{w}_n^{(i-1)}\\})\\\\ \\{\\widetilde{\\mathbf{e}}_1^{(i)}, ..., \\widetilde{\\mathbf{e}}_m^{(i)}\\} &amp;= \\text{MH-ATT}(\\{\\mathbf{e}_1^{(i-1)}, ..., \\mathbf{e}_m^{(i-1)}\\}) \\end{array}{w1(i)​,...,wn(i)​}{e1(i)​,...,em(i)​}​=MH-ATT({w1(i−1)​,...,wn(i−1)​})=MH-ATT({e1(i−1)​,...,em(i−1)​})​ 经过自注意之后，第 iii 个聚合器用于将 token 和 entity 序列相互融合，并为每个 token 和 entity 计算一个输出 embedding。对于 token wjw_jwj​ 和它对应的实体 ek=f(wj)e_k = f(w_j)ek​=f(wj​), 信息融合过程如下： hj=σ(W~t(i)w~j(i)+W~eie~k(i)+b~(i))wj(i)=σ(Wt(i)hj+bt(i))ek(i)=σ(W~e(i)hj+be(i))\\begin{array}{rl} \\mathbf{h}_j &amp;= \\sigma(\\widetilde{\\mathbf{W}}_t^{(i)}\\widetilde{\\mathbf{w}}_j^{(i)} + \\widetilde{\\mathbf{W}}_e^{i}\\widetilde{\\mathbf{e}}_k^{(i)} + \\widetilde{\\mathbf{b}}^{(i)})\\\\ \\mathbf{w}_j^{(i)} &amp;= \\sigma(\\mathbf{W}_t^{(i)}\\mathbf{h}_j + \\mathbf{b}_t^{(i)})\\\\ \\mathbf{e}_k^{(i)} &amp;= \\sigma(\\widetilde{\\mathbf{W}}_e^{(i)}\\mathbf{h}_j + \\mathbf{b}_e^{(i)}) \\end{array}hj​wj(i)​ek(i)​​=σ(Wt(i)​wj(i)​+Wei​ek(i)​+b(i))=σ(Wt(i)​hj​+bt(i)​)=σ(We(i)​hj​+be(i)​)​ 其中的 hj\\mathbf{h}_jhj​ 是聚合了 token 和 entity 信息的内部隐藏状态。σ(⋅)\\sigma(\\cdot)σ(⋅) 是非线性激活函数，通常是 GELU 函数。对于不属于实体短语的 tokens, 信息融合层计算其输出 embedding 的过程如下： hj=σ(W~t(i)w~j(i)+b~(i))wj(i)=σ(Wt(i)hj+bt(i))\\begin{array}{rl} \\mathbf{h}_j &amp;= \\sigma(\\widetilde{\\mathbf{W}}_t^{(i)}\\widetilde{\\mathbf{w}}_j^{(i)} + \\widetilde{\\mathbf{b}}^{(i)})\\\\ \\mathbf{w}_j^{(i)} &amp;= \\sigma(\\mathbf{W}_t^{(i)}\\mathbf{h}_j + \\mathbf{b}_t^{(i)}) \\end{array}hj​wj(i)​​=σ(Wt(i)​wj(i)​+b(i))=σ(Wt(i)​hj​+bt(i)​)​ 顶部的聚合器输出的 token 和 entity 嵌入被用作 K-Encoder\\text{K-Encoder}K-Encoder 的最终输出嵌入 预训练 去噪实体自动编码器(denoision entity auto-encoder, dEA)：随机遮住一些 token-entity alignments, 并要求系统预测所有对应的实体。 考虑到实体列表 E\\mathcal{E}E 可能很大，仅要求系统从给定的实体列表的子集中预测实体。 给定 token 序列 {w1,...,wn}\\{w_1, ..., w_n\\}{w1​,...,wn​} 和对应的实体序列 {e1,...,em}\\{e_1, ..., e_m\\}{e1​,...,em​}, 定义 token wiw_iwi​ 的实体分布为： p(ej∣wi)=exp⁡(linear(wio)⋅ej)Σk=1mexp⁡(linear(wio)⋅ek)p(e_j|w_i) = \\frac{\\exp(\\text{linear}(\\mathbf{w}_i^o)\\cdot \\mathbf{e}_j)}{\\Sigma_{k = 1}^{m}\\exp(\\text{linear}(\\mathbf{w}_i^o)\\cdot \\mathbf{e}_k)} p(ej​∣wi​)=Σk=1m​exp(linear(wio​)⋅ek​)exp(linear(wio​)⋅ej​)​ 其中的 linear(⋅)\\text{linear}(\\cdot)linear(⋅) 是一个线性层，上式被用来计算 dEA 目标的交叉熵损失函数。 微调 与 BERT 类似，ERNIE 输入序列的第一个 token 也是特殊的 token [CLS]\\text{[CLS]}[CLS]，用于特殊的任务。 对于关系分类任务，系统需要根据上下文来为给定的实体对分类。本文提出的方法是，修改输入序列，在序列中的实体两端添加特殊的 mark token ，这些 mark token 在关系分类模型中起到类似位置嵌入的作用。如下图所示，分别使用 [HD]\\text{[HD]}[HD] 和 [TL]\\text{[TL]}[TL] 来标记头部的实体和尾部的实体。 对于实体分类任务，其微调过程是关系分类任务的简化版本。早前的模型同时利用了上下文嵌入(context embedding) 和实体提及嵌入(entity mention embedding)，作者认为修改输入序列在实体提及的两端加上特殊的 token [ENT]\\text{[ENT]}[ENT] 可以引导 ERNIE 结合上下文信息和实体提及信息。 实验 预训练 预训练时的 token 编码器(T-Encoder)直接应用 BERT 的参数。预训练的过程是一个多任务(MLM, NSP, dEA)的过程，作者使用英文维基作为语料库。总共约有 4500M 个子词和 140M 个实体。 预训练之前，先利用由维基数据训练的知识嵌入 TransE 作为实体的输入嵌入。具体地说，作者采样了维基数据中的 5040986 个实体和 24267796 个三元组。实体的知识嵌入在训练过程中固定，实体编码模块参数随机初始化。 实体分类 分别在 FIGER 和 Open Entity 数据集上与 NFGEC, UEFT 和 BERT 作比较。证明了 预训练语言模型比传统的实体分类模型更具表征能力，能够充分利用训练数据中的信息。 与 BERT 相比，ERNIE 由于引入了额外知识提高了分类精度。 关系分类 在数据集 FewRel, TACRED 上与模型 CNN, PA-LSTM, C-GCN 和 BERT 对比。结果证明： 预训练语言模型在关系分类方面可以提供更多的信息 ERNIE 在两个数据集上关系分类表现都优于 BERT，尤其在小规模数据集上。证明额外知识信息帮助模型更好地利用小的训练数据集 GLUE 在通用自然语言理解中的8个数据集上与 BERT 进行了对比。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"Knowledge Embedding","slug":"Knowledge-Embedding","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Embedding/"}]},{"title":"ERNIE: Enhanced Representation through Knowledge Integration","slug":"ERNIE-Enhanced-Representation-through-Knowledge-Integration","date":"2021-10-09T16:00:00.000Z","updated":"2023-06-21T06:51:28.788Z","comments":true,"path":"ERNIE-Enhanced-Representation-through-Knowledge-Integration/","link":"","permalink":"https://blog.zhuwenq.cc/ERNIE-Enhanced-Representation-through-Knowledge-Integration/","excerpt":"百度 ERNIE, 与清华 ERNIE (ERNIE-Enhanced-Language-Representation-with-Informative-Entities) 同名","text":"百度 ERNIE, 与清华 ERNIE (ERNIE-Enhanced-Language-Representation-with-Informative-Entities) 同名 Motivation 现有的表征学习方法在学习 token embedding 时没有考虑语句中的先验知识，作者认为如果模型能够学习到先验知识，则可以获得更可靠的语言表征。 Methodology ERNIE 使用 knowledge masking 策略隐式地学习先验知识，除了标准的随机 masking 方法之外，ERNIE 引入了两种特殊的 maksing 策略：phrase-level 和 entity-level，即分别将一个短语或一个实体作为一个基本单元进行 masking。 BERT as Backbone 语言表征的基础编码器采用和 BERT 等模型相同的多层 Transformer 结构。针对中文语料，作者将每个字用空格分开，并采用 WordPiece 进行 tokenize。每个 token 的输入 embedding 由 token embedding, segment embedding 和 position embedding 相加构成(与 BERT 相同)，每个输入序列的第一个 token 是特殊的分类 token ([CLS][CLS][CLS])(与 BERT 相同) multi-stage knowledge masking strategy Base-Level Masking 第一阶段将输入语句视为基本的语言单元的序列(对于英文，基本单元是词；对于中文，基本单元是字)。这一阶段直接使用 BERT 的 Masking 方法。作者认为此阶段难以对高阶语义知识建模。 Phrase-Level Masking 第二阶段将语句中的短语视为 Mask 的基本单元。随机选择句子中的一部分短语进行 Masking ，然后预测被 Masking 的短语包含的所有基本语言单元。 Entity-Level Masking 第三个阶段从语句中分析出命名实体的存在(使用额外工具)，然后对实体进行 Masking 并预测实体所包含的所有语言单元。 实验 对于对话语言模型任务(DLM)，ERINE 使用 dialogue embedding 来标示对话中的角色。另外还构建了假样本用于训练模型分辨一组多轮对话数据是真的对话还是假的对话。","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Knowledge Embedding","slug":"Knowledge-Embedding","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Embedding/"}]},{"title":"Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction","slug":"Tail-to-Tail-Non-Autoregressive-Sequence-Prediction-for-Chinese-Grammatical-Error-Correction","date":"2021-09-30T16:00:00.000Z","updated":"2023-06-21T06:51:28.920Z","comments":true,"path":"Tail-to-Tail-Non-Autoregressive-Sequence-Prediction-for-Chinese-Grammatical-Error-Correction/","link":"","permalink":"https://blog.zhuwenq.cc/Tail-to-Tail-Non-Autoregressive-Sequence-Prediction-for-Chinese-Grammatical-Error-Correction/","excerpt":"Motivition CGEC 中文语法纠错，旨在自动检测和纠正中文语句中的语法错误。 中文语法纠错需要的操作种类： 替换，中文中常常出现同音字(拼音输入法)和错别字的错误。可以通过替换对应的字纠正 删除和插入，分别应对文字的冗余和遗漏问题 本地释义，有时语法错误要求通过重新排列等方式重新释义序列中的部分词 其中插入删除和本地释义是变长操作，替换是定长操作","text":"Motivition CGEC 中文语法纠错，旨在自动检测和纠正中文语句中的语法错误。 中文语法纠错需要的操作种类： 替换，中文中常常出现同音字(拼音输入法)和错别字的错误。可以通过替换对应的字纠正 删除和插入，分别应对文字的冗余和遗漏问题 本地释义，有时语法错误要求通过重新排列等方式重新释义序列中的部分词 其中插入删除和本地释义是变长操作，替换是定长操作 Pervious Works 序列翻译(sequence translation)和标记(sequence tagging)是常用于文本语法纠错的方法。 对于序列翻译方法：虽然这种模式可以应对任意类型的语法错误，但是由于exposure bias issue和phenomenon of hallucination问题的存在，即使引入复制机制也无法保证结果的可信性。 对于tagging的方法，该模式使用标记来指导纠错操作： 需要将词典扩展到原先的约三倍 需要对序列进行多遍预测，直到预测不出新的错误为止 计算开销过大。 最新的工作尝试微调 BERT 来解决 CGEC 问题，但是由于 BERT 模型结构的限制，大多数只能解决定长纠错的场景 CGEC 中大多数 tokens 都没有错误，应该被直接复制。这造成在序列标记和序列翻译任务中常用的参数学习方法 MLE 受到标签不平衡的影响。之前的工作没有探索这种不平衡问题。 Contribution 提出一个名为 tail-to-tail non-autoregressive sequence prediction(TtT) 的解决 CGEC 问题的结构 使用 BERT 编码器和一个 CRF 层作为主干框架，同时可以进行替换，插入，删除和本地释义操作 考虑到句子中大多数 tokens 都不需要操作的不平衡性，采用 Focal Loss 惩罚策略缓解 建立了一个变长 CGEC 数据集，并在上面实验，证明了方法的效果 Method 模型的输入是一个带有语法错误的句子 X=(x1,x2,...,xT)X = (x_1, x_2, ..., x_T)X=(x1​,x2​,...,xT​), 其中 xix_ixi​ 是句子中的 token，TTT 是句子的长度。模型的输出是经过纠正的句子 Y=(y1,y2,...,yT′)Y = (y_1, y_2, ..., y_{T&#x27;})Y=(y1​,y2​,...,yT′​), 其中 T′T&#x27;T′ 与 TTT 并不必须相等。 变长场景 假设输入是 X=(x1,x2,x3,...,&lt;eos&gt;)X = (x_1, x_2, x_3, ..., &lt;eos&gt;)X=(x1​,x2​,x3​,...,&lt;eos&gt;), 长度是 TTT；输出是 Y=(y1,y2,y3,...,&lt;eos&gt;)Y = (y_1, y_2, y_3, ..., &lt;eos&gt;)Y=(y1​,y2​,y3​,...,&lt;eos&gt;), 长度是 T′T&#x27;T′ T = T’，不需要额外的处理 T &gt; T’，即从输入序列中删除了部分 token，可以在输出序列的末尾补充 T−T′T - T&#x27;T−T′ 个特殊的 token &lt;pad&gt;&lt;pad&gt;&lt;pad&gt; 来使输入输出序列长度相等。Y=(y1,y2,...,&lt;eos&gt;,&lt;pad&gt;)Y = (y_1, y_2, ..., &lt;eos&gt;, &lt;pad&gt;)Y=(y1​,y2​,...,&lt;eos&gt;,&lt;pad&gt;) T &lt; T’，即在输入序列中插入部分 token，可以在输入序列的末尾补充特殊的 token &lt;mask&gt;&lt;mask&gt;&lt;mask&gt; 使输入输出相等。X=(x1,x2,...,&lt;eos&gt;,&lt;mask&gt;)X = (x_1, x_2, ..., &lt;eos&gt;, &lt;mask&gt;)X=(x1​,x2​,...,&lt;eos&gt;,&lt;mask&gt;) 双向语义建模 即标准的 BERT 过程： Ht0=Ewt+EptQ0,K0,V0=H0WQ,H0WK,H0WVHt1=LN(SELF-ATT(Qt0,K0,V0)+Ht0)Ht1=LN(FFN(Ht1)+Ht1)\\begin{array}{rl} \\mathbf{H}_t^0 &amp;= \\mathbf{E}_{w_t}+\\mathbf{E}_{p_t}\\\\ \\mathbf{Q}^0, \\mathbf{K}^0, \\mathbf{V}^0 &amp;= \\mathbf{H}^0\\mathbf{W}^Q, \\mathbf{H}^0\\mathbf{W}^K, \\mathbf{H}^0\\mathbf{W}^V\\\\ \\mathbf{H}_t^1 &amp;= \\text{LN}(\\text{SELF-ATT}(\\mathbf{Q}_t^0, \\mathbf{K}^0, \\mathbf{V}^0) + \\mathbf{H}_t^0)\\\\ \\mathbf{H}_t^1 &amp;= \\text{LN}(\\text{FFN}(\\mathbf{H}_t^1) + \\mathbf{H}_t^1) \\end{array}Ht0​Q0,K0,V0Ht1​Ht1​​=Ewt​​+Ept​​=H0WQ,H0WK,H0WV=LN(SELF-ATT(Qt0​,K0,V0)+Ht0​)=LN(FFN(Ht1​)+Ht1​)​ 该部分即一个 BERT 编码器，经过 L 层的编码后，最终输出的语言表征向量为 HL∈Rmax(T,T′)×d\\mathbf{H}^L \\in \\mathbb{R}^{\\text{max}(T, T&#x27;)\\times d}HL∈Rmax(T,T′)×d 非自回归序列预测 直接预测。对于经过 BERT 编码器得到的语言表征，可以直接在表征向量上应用 softmax 操作进行预测。具体地说，首先在输出的语言表征上进行一次线性投影： st=htTWs+bs\\mathbf{s}_t = \\mathbf{h}_t^T\\mathbf{W}_s + \\mathbf{b}_s st​=htT​Ws​+bs​ 其中 bt∈Rd\\mathbf{b}_t \\in \\mathbb{R}^dbt​∈Rd 表示输入序列中第 ttt 个 token 的表征，来自 HL\\mathbf{H}^LHL, Ws∈Rd×∣V∣,bs∈R∣V∣\\mathbf{W}_s \\in \\mathbb{R}^{d\\times |\\mathcal{V}|}, \\mathbf{b}_s \\in \\mathbb{R}^{|\\mathcal{V}|}Ws​∈Rd×∣V∣,bs​∈R∣V∣ 是投影参数。如此得到 token 的表征向量投影到词典维度的向量 st\\mathbf{s}_tst​ 然后对 st\\mathbf{s}_tst​ 进行 softmax 操作，获得该 token 在目标词典上的概率分布： Pdp(yt)=softmax(st)P_{dp}(y_t) = \\text{softmax}(\\mathbf{s}_t) Pdp​(yt​)=softmax(st​) 这种直接的预测可以解决定长的语法纠错问题，但是只能进行替换操作，对于其他种类的操作无能为力。 非自回归的序列预测方法还有忽略掉了邻居 token 之间的依赖关系的问题。 通过 CRF 进行依赖建模。对于给定的长度为 TTT 的输入序列 XXX， 输出的长度为 T′T&#x27;T′ 的序列 YYY 的似然函数为： Pcrf(Y∣X)=1Z(X)exp⁡(∑t=1T′s(yt)+∑t=2T′t(yt−1,yt))P_{\\text{crf}}(Y|X) = \\frac{1}{Z(X)}\\exp \\left(\\sum_{t = 1}^{T&#x27;}s(y_t) + \\sum_{t = 2}^{T&#x27;}t(y_{t-1}, y_t)\\right) Pcrf​(Y∣X)=Z(X)1​exp⎝⎛​t=1∑T′​s(yt​)+t=2∑T′​t(yt−1​,yt​)⎠⎞​ 其中 Z(X)Z(X)Z(X) 是归一化参数，s(yt)s(y_t)s(yt​) 即 token yty_tyt​ 的标签分数，来自第 ttt 个 token 的表征经过投影的向量 st\\mathbf{s}_tst​, 相当于 st(Vyt)s_t(\\mathcal{V}^{y_t})st​(Vyt​), Vyt\\mathcal{V}^{y_t}Vyt​ 是 token yty_tyt​ 在词典中的索引。 t(yt−1,yt)=Myt−1,ytt(y_{t-1}, y_t) = \\mathbf{M}_{y_{t-1}, y_t}t(yt−1​,yt​)=Myt−1​,yt​​, 代表从 token yt−1y_{t-1}yt−1​ 到 yty_tyt​ 的转移分数。其中 M∈R∣V∣×∣V∣\\mathbf{M}\\in \\mathbb{R}^{|\\mathcal{V}|\\times |\\mathcal{V}|}M∈R∣V∣×∣V∣ 是转移矩阵。 通常情况下，M\\mathbf{M}M 可以通过作为神经网络的参数通过端到端学习得到，但是由于维度过大(词典的维度)，无法有效获取。 作者使用低秩分解的方法定义两个低秩矩阵 E1,E2∈R∣V∣×dm\\mathbf{E}_1, \\mathbf{E}_2 \\in \\mathbb{R}^{|\\mathcal{V}|\\times d_m}E1​,E2​∈R∣V∣×dm​ 来逼近 M\\mathbf{M}M: M=E1E2T\\mathbf{M} = \\mathbf{E}_1\\mathbf{E}_2^T M=E1​E2T​ 对于归一化参数 Z(X)Z(X)Z(X), 使用维特比算法进行搜索。 训练 在训练时： 将解决定长问题的直接预测和解决变长问题的CRF 依赖建模结合 使用最大似然估计作为参数学习方法，负对数似然作为损失函数 直接预测的损失函数(优化目标)是： Ldp=−∑t=1T′log⁡Pdp(yt∣X)\\mathcal{L}_\\text{dp} = -\\sum_{t = 1}^{T&#x27;}\\log P_{dp}(y_t|X) Ldp​=−t=1∑T′​logPdp​(yt​∣X) CRF 依赖建模的损失函数是： Lcrf=−logPcrf(Y∣X)\\mathcal{L}_\\text{crf} = -log P_\\text{crf}(Y|X) Lcrf​=−logPcrf​(Y∣X) 总的损失函数：L=Ldp+Lcrf\\mathcal{L} = \\mathcal{L}_\\text{dp} + \\mathcal{L}_\\text{crf}L=Ldp​+Lcrf​ 为缓解样本不平衡(大多数 token 不需要纠正)问题，使用 Focal Loss： Ldpfl=−∑t=1T′(1−Pdp(yt∣X))γlog⁡Pdp(yt∣X)Lcrffl=−(1−Pcrf(Y∣X))γlog⁡Pcrf(Y∣X)Lfl=Ldpfl+Lcrffl\\begin{array}{rl} \\mathcal{L}_\\text{dp}^\\text{fl} &amp;= -\\sum_{t = 1}^{T&#x27;}(1 - P_\\text{dp}(y_t|X))^\\gamma \\log P_\\text{dp}(y_t|X)\\\\ \\mathcal{L}_\\text{crf}^\\text{fl} &amp;= -(1 - P_\\text{crf}(Y|X))^\\gamma \\log P_\\text{crf}(Y|X)\\\\ \\mathcal{L}^{fl} &amp;= \\mathcal{L}_\\text{dp}^\\text{fl} + \\mathcal{L}_\\text{crf}^\\text{fl} \\end{array} Ldpfl​Lcrffl​Lfl​=−∑t=1T′​(1−Pdp​(yt​∣X))γlogPdp​(yt​∣X)=−(1−Pcrf​(Y∣X))γlogPcrf​(Y∣X)=Ldpfl​+Lcrffl​​","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Sequence Prediction","slug":"Sequence-Prediction","permalink":"https://blog.zhuwenq.cc/tags/Sequence-Prediction/"},{"name":"CGEC","slug":"CGEC","permalink":"https://blog.zhuwenq.cc/tags/CGEC/"}]},{"title":"Xlnet: Generalized autoregressive pretraining for language understanding","slug":"XLNet","date":"2021-09-25T16:00:00.000Z","updated":"2023-06-21T06:51:28.932Z","comments":true,"path":"XLNet/","link":"","permalink":"https://blog.zhuwenq.cc/XLNet/","excerpt":"提出了双流注意力模型","text":"提出了双流注意力模型 标准语言模型(Autoregressive)和 BERT(MLM) BERT 是一种自编码模型，它使用 MLM 训练目标，试图从被遮罩的语句恢复被遮住的 token。而自回归模型是根据序列中靠前位置的 token 预测下一个 token，对于给定的文本序列 x=[x1,...,xT]\\mathbf{x} = [x_1, ..., x_T]x=[x1​,...,xT​], 自回归语言模型的目标是调整参数使得训练数据上的似然函数最大： max⁡θlog⁡pθ(x)=∑t=1Tlog⁡pθ(xt∣x&lt;t)=∑t=1Tlog⁡exp⁡(hθ(x1:t−1)Te(xt))Σx′exp⁡(hθ(x1:t−1)Te(x′))\\max_\\theta \\log p_\\theta(\\mathbf{x}) = \\sum_{t=1}^T\\log p_\\theta(x_t|\\mathbf{x}_{&lt;t}) = \\sum_{t=1}^T\\log \\frac{\\exp(h_\\theta(\\mathbf{x}_{1:t-1})^Te(x_t))}{\\Sigma_{x&#x27;}\\exp(h_\\theta(\\mathbf{x}_{1:t-1})^Te(x&#x27;))} θmax​logpθ​(x)=t=1∑T​logpθ​(xt​∣x&lt;t​)=t=1∑T​logΣx′​exp(hθ​(x1:t−1​)Te(x′))exp(hθ​(x1:t−1​)Te(xt​))​ 其中 x&lt;t\\mathbf{x}_{&lt; t}x&lt;t​ 表示 x\\mathbf{x}x 中位置 ttt 之前的所有 xxx，即 x1:t−1\\mathbf{x}_{1:t-1}x1:t−1​。hθ(x1:t−1)h_\\theta(\\mathbf{x}_{1:t-1})hθ​(x1:t−1​) 是 RNN 或 Transformer 编码的隐藏状态。e(x)e(x)e(x) 是 token xxx 的 embedding。 而 BERT 是去噪自编码的方法，对于序列 x\\mathbf{x}x, BERT 经过随机挑选 token 进行遮罩将其变成带有噪声的 x^\\hat{\\mathbf{x}}x^, 假设被遮罩的 token 原始值是 x‾\\overline{\\mathbf{x}}x, BERT 根据上下文恢复除被遮罩的 token 的原始值，即： max⁡θlog⁡pθ(x‾∣x^)≈∑t=1Tmtlog⁡pθ(xt∣x^)=∑t=1Tmtlog⁡exp⁡(Hθ(x)tTe(xt))Σx′exp⁡(Hθ(x)tTe(x′))\\max_\\theta \\log p_\\theta(\\overline{\\mathbf{x}}|\\hat{\\mathbf{x}})\\approx\\sum_{t = 1}^{T}m_t\\log p_\\theta(x_t|\\hat{\\mathbf{x}}) = \\sum_{t = 1}^{T}m_t\\log \\frac{\\exp(H_\\theta(\\mathbf{x})_t^Te(x_t))}{\\Sigma_{x&#x27;}\\exp(H_\\theta(\\mathbf{x})_t^Te(x&#x27;))} θmax​logpθ​(x∣x^)≈t=1∑T​mt​logpθ​(xt​∣x^)=t=1∑T​mt​logΣx′​exp(Hθ​(x)tT​e(x′))exp(Hθ​(x)tT​e(xt​))​ 式中 mt=1m_t = 1mt​=1 表示位置 ttt 是一个 MASK，需要被恢复。HθH_\\thetaHθ​ 是一个 Transformer，它将长度为 TTT 的序列 x\\mathbf{x}x 映射为隐藏状态序列 Hθ(x)=[Hθ(x)1,...,Hθ(x)T]H_\\theta(\\mathbf{x}) = [H_\\theta(\\mathbf{x})_1, ..., H_\\theta(\\mathbf{x})_T]Hθ​(x)=[Hθ​(x)1​,...,Hθ​(x)T​]。（这里的 BERT 可以注意到整个序列所有的 token，因此记作 Hθ(x)H_\\theta(\\mathbf{x})Hθ​(x), 前面的自回归模型只能注意到位置 ttt 之前的 token，因此记作 hθ(x1:t−1)h_\\theta(\\mathbf{x}_{1:t-1})hθ​(x1:t−1​)） 对于 BERT，其缺陷有： 独立假设 注意到 BERT 的训练目标表达式中使用了 ≈\\approx≈, 意为在给定的遮罩信息 x^\\hat{\\mathbf{x}}x^ 的条件下，被遮罩的 token 之间是独立的，这个假设显然不成立。例如语句 New York is a city\\text{New York is a city}New York is a city ，假设被遮住的是 New\\text{New}New 和 York\\text{York}York，那么在给定 is a city\\text{is a city}is a city 的条件下两个被遮住的 token 并不独立。 预训练与微调的割裂 BERT 在预训练时会有特殊的 token MASK，这种 token 在下游的微调过程中不会出现 对于标准语言模型，其缺陷有： 双向上下文 标准语言模型只能利用一个方向的上下文 排列语言模型(Permutation Language Model) XLNet 使用排列语言模型实现了结合上下文信息并且不引入 [MASK] 标记，同时类似自回归的预测过程避开了独立假设的限制。 语序分解 对于给定长度 TTT 的序列 x\\mathbf{x}x，总共有 T!T!T! 种排列顺序，也就是说在自回归时有 T!T!T! 种链式分解方法。假设 x=[x1,x2,x3]\\mathbf{x} = [x_1, x_2, x_3]x=[x1​,x2​,x3​], 则总共有 3!=63! = 63!=6 种分解方式： p(x)=p(x1)p(x2∣x1)p(x3∣x1x2)⇒1→2→3p(x)=p(x1)p(x2∣x1x3)p(x3∣x1)⇒1→3→2p(x)=p(x1∣x2)p(x2)p(x3∣x1x2)⇒2→1→3p(x)=p(x1∣x2x3)p(x2)p(x3∣x2)⇒2→3→1p(x)=p(x1∣x3)p(x2∣x1x3)p(x3)⇒3→1→2p(x)=p(x1∣x2x3)p(x2∣x3)p(x3)⇒3→2→1\\begin{array}{c} p(\\mathbf{x}) = p(x_1)p(x_2|x_1)p(x_3|x_1x_2)\\Rightarrow 1\\rightarrow 2\\rightarrow 3\\\\ p(\\mathbf{x}) = p(x_1)p(x_2|x_1x_3)p(x_3|x_1)\\Rightarrow 1\\rightarrow 3\\rightarrow 2\\\\ p(\\mathbf{x}) = p(x_1|x_2)p(x_2)p(x_3|x_1x_2)\\Rightarrow 2\\rightarrow 1\\rightarrow 3\\\\ p(\\mathbf{x}) = p(x_1|x_2x_3)p(x_2)p(x_3|x_2)\\Rightarrow 2\\rightarrow 3\\rightarrow 1\\\\ p(\\mathbf{x}) = p(x_1|x_3)p(x_2|x_1x_3)p(x_3)\\Rightarrow 3\\rightarrow 1\\rightarrow 2\\\\ p(\\mathbf{x}) = p(x_1|x_2x_3)p(x_2|x_3)p(x_3)\\Rightarrow 3\\rightarrow 2\\rightarrow 1 \\end{array}p(x)=p(x1​)p(x2​∣x1​)p(x3​∣x1​x2​)⇒1→2→3p(x)=p(x1​)p(x2​∣x1​x3​)p(x3​∣x1​)⇒1→3→2p(x)=p(x1​∣x2​)p(x2​)p(x3​∣x1​x2​)⇒2→1→3p(x)=p(x1​∣x2​x3​)p(x2​)p(x3​∣x2​)⇒2→3→1p(x)=p(x1​∣x3​)p(x2​∣x1​x3​)p(x3​)⇒3→1→2p(x)=p(x1​∣x2​x3​)p(x2​∣x3​)p(x3​)⇒3→2→1​ 值得注意的是，排序语言模型中的语序并没有改变，只是预测的顺序发生了改变。例如 p(x2∣x1x3)p(x_2|x_1x_3)p(x2​∣x1​x3​) 指的是第一个 token 是 x1x_1x1​，第三个 token 是 x3x_3x3​ 的情况下第二个词是 x2x_2x2​ 的概率，而不是第一个 token 是 x1x_1x1​，第二个 token 是 x3x_3x3​ 的情况下第三个 token 是 x2x_2x2​ 的概率。 对于这 T!T!T! 种排序方法，如果模型可以遍历所有的情况，并且参数是共享的，则这个模型就能学习到各种的上下文。但是这种策略的计算开销非常大，实际上只能随机采样部分排列方式。用数学语言描述排列语言模型的目标为： max⁡θEz∼ZT[∑t=1Tlog⁡pθ(xzt∣xz&lt;t)]\\max_\\theta \\mathbb{E}_{z\\sim\\mathcal{Z}_T}[\\sum_{t = 1}^{T}\\log p_\\theta(x_{z_t|\\mathbf{x}_{z_{&lt;t}}})] θmax​Ez∼ZT​​[t=1∑T​logpθ​(xzt​∣xz&lt;t​​​)] 即调整参数使得似然概率最大。其中 ZT\\mathcal{Z}_TZT​ 表示长度为 TTT 的序列中所有的排列方式的集合，z∈ZTz\\in \\mathcal{Z}_Tz∈ZT​ 表示其中的一种排列方法。ztz_tzt​ 表示第 ttt 种排列，z&lt;tz_{&lt;t}z&lt;t​ 表示 zzz 的第 1∼t−11\\sim t - 11∼t−1 个元素。 在实际的实现中，XLNet 是通过 Attention 的 Mask 来实现不同的排序方式。例如 p(x1∣x3)p(x2∣x1x3)p(x3)p(x_1|x_3)p(x_2|x_1x_3)p(x_3)p(x1​∣x3​)p(x2​∣x1​x3​)p(x3​) ,可以在 Transformer 编码 x1x_1x1​ 的时候 attend to x3x_3x3​, 同时把 x2x_2x2​ Mask 掉，就实现了 p(x1∣x3)p(x_1|x_3)p(x1​∣x3​)。 双流注意力机制(Two-Stream Self-Attention for Target-Aware Repersentations) 位置信息问题 上述的排序语言模型存在位置信息缺失的问题。例如输入的句子是 I like New York\\text{I like New York}I like New York, 并且采样到的一种排序为 z=[1,3,4,2]z = [1, 3, 4, 2]z=[1,3,4,2], 在预测 z3=4z_3 = 4z3​=4 位置时，根据公式： pθ(Xz3=x∣xz1z2)=pθ(X4=x∣x1x3)=exp⁡(e(x)Thθ(x1x3))Σx′exp⁡(e(x′)Thθ(x1x3))p_\\theta(X_{z_3} = x|x_{z_1z_2}) = p_\\theta(X_4 = x|x_1x_3) = \\frac{\\exp(e(x)^Th_\\theta(x_1x_3))}{\\Sigma_{x&#x27;}\\exp(e(x&#x27;)^Th_\\theta(x_1x_3))} pθ​(Xz3​​=x∣xz1​z2​​)=pθ​(X4​=x∣x1​x3​)=Σx′​exp(e(x′)Thθ​(x1​x3​))exp(e(x)Thθ​(x1​x3​))​ 上式用自然语言描述为，第一个词是 I\\text{I}I, 第三个词是 New\\text{New}New 的条件下第四个词是 York\\text{York}York 的概率。 另外再看另一种排列 z′=[1,3,2,4]z&#x27; = [1, 3, 2, 4]z′=[1,3,2,4], 在预测 z3=2z_3 = 2z3​=2 时： pθ(Xz3=x∣xz1z2)=pθ(X2=x∣x1x3)=exp⁡(e(x)Thθ(x1x3))Σx′exp⁡(e(x′)Thθ(x1x3))p_\\theta(X_{z_3} = x|x_{z_1z_2}) = p_\\theta(X_2 = x|x_1x_3) = \\frac{\\exp(e(x)^Th_\\theta(x_1x_3))}{\\Sigma_{x&#x27;}\\exp(e(x&#x27;)^Th_\\theta(x_1x_3))} pθ​(Xz3​​=x∣xz1​z2​​)=pθ​(X2​=x∣x1​x3​)=Σx′​exp(e(x′)Thθ​(x1​x3​))exp(e(x)Thθ​(x1​x3​))​ 表示第一个词是 I\\text{I}I，第三个词是 New\\text{New}New 的条件下第二个词是 York\\text{York}York 的概率。对比发现上面两个概率是相同的，这不符合日常经验。问题的关键就在于模型无法获知要预测的词在原始语句中的位置。 双流注意力 在 Transformer 中，token 的位置信息使用 positional embedding 编码并加到 embedding 中，然后随 token 输入到模型中。这使得模型只能获取注意到的 token 的位置信息，不能获取要预测的 token 的位置信息。例如 p(X4=x∣x1x3)p(X_4 = x|x_1x_3)p(X4​=x∣x1​x3​) 中模型 attend to x1,x3x_1, x_3x1​,x3​, 则模型在获得 x1,x3x_1, x_3x1​,x3​ 的 embedding 的同时可以获得其位置信息，但是对于要预测的 token X4X_4X4​, 则不可能获取其 embedding，也就无法获取位置信息。 因此必须显示地告诉模型要预测的词处于序列中的位置。对于给定的排列 zzz，要计算 pθ(Xzt=x∣xz&lt;t)p_\\theta(X_{z_t} = x|\\mathbf{x}_{z_{&lt;t}})pθ​(Xzt​​=x∣xz&lt;t​​) 时，朴素的 Transformer 计算过程如下： pθ(Xzt=x∣xz&lt;t)=exp⁡(e(x)Thθ(xz&lt;t))Σx′exp⁡(e(x′)Thθ(xz&lt;t))p_\\theta(X_{z_t} = x|\\mathbf{x}_{z_{&lt;t}}) = \\frac{\\exp(e(x)^Th_\\theta(\\mathbf{x}_{z_{&lt;t}}))}{\\Sigma_{x&#x27;}\\exp(e(x&#x27;)^Th_\\theta(\\mathbf{x}_{z_{&lt;t}}))} pθ​(Xzt​​=x∣xz&lt;t​​)=Σx′​exp(e(x′)Thθ​(xz&lt;t​​))exp(e(x)Thθ​(xz&lt;t​​))​ 这样对于相同的上下文和相同的词，其处在不同位置的概率是相同的。为解决这一问题，XLNet 将要预测的位置 ztz_tzt​ 放在模型中： pθ(Xzt=x∣xz&lt;t)=exp⁡(e(x)Tgθ(xz&lt;t,zt))Σx′exp⁡(e(x′)Tgθ(xz&lt;t,zt))p_\\theta(X_{z_t} = x|\\mathbf{x}_{z_{&lt;t}}) = \\frac{\\exp(e(x)^Tg_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t))}{\\Sigma_{x&#x27;}\\exp(e(x&#x27;)^Tg_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t))} pθ​(Xzt​​=x∣xz&lt;t​​)=Σx′​exp(e(x′)Tgθ​(xz&lt;t​​,zt​))exp(e(x)Tgθ​(xz&lt;t​​,zt​))​ 要找到一个模型来实现 gθ(xz&lt;t,zt)g_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t)gθ​(xz&lt;t​​,zt​) 的功能，那么这个函数需要满足两点要求： 为了预测 xztx_{z_t}xzt​​, gθ(xz&lt;t,zt)g_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t)gθ​(xz&lt;t​​,zt​) 只能使用位置信息 ztz_tzt​ 而不能使用 xztx_{z_t}xzt​​(否则模型直接看到了要预测的词) 为了预测 ztz_tzt​ 之后的词，gθ(xz&lt;t,zt)g_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t)gθ​(xz&lt;t​​,zt​) 必须编码 xztx_{z_t}xzt​​ 的语义信息 这两点对于朴素的 Transformer 来说是矛盾的，例如在第一层 Attention 编码 xz1x_{z_1}xz1​​ 时，如果利用了 xz1x_{z_1}xz1​​ 的信息，那么可以获得其位置信息，但同时它本身的语义信息也泄露了。如果不利用 xz1x_{z_1}xz1​​ 的语义信息，只利用其位置信息，那么在下一层的 Attention 编码 xz2x_{z_2}xz2​​ 时，编码的依赖条件之一 h10(xz1)h_1^0(x_{z_1})h10​(xz1​​) 中没有语义信息，相当于凭空对 xz2x_{z_2}xz2​​ 编码。 因此需要引入双流注意力，即两个隐藏状态： 内容隐藏状态 hθ(xz&lt;t)h_\\theta(\\mathbf{x}_{z_{&lt;t}})hθ​(xz&lt;t​​), 简写为 hzth_{z_t}hzt​​, 和标准的 Transformer 中的隐藏状态一样，即编码上下文也编码 xztx_{z_t}xzt​​ 本身的内容 查询隐藏状态 gθ(xz&lt;t,zt)g_\\theta(\\mathbf{x}_{z_{&lt;t}}, z_t)gθ​(xz&lt;t​​,zt​), 简写为 gztg_{z_t}gzt​​, 它只编码上下文和要预测的位置 ztz_tzt​，不包含 xztx_{z_t}xzt​​ 的语义信息 在实际计算时，第0层的两种隐藏状态 gi0,hi0g_i^0, h_i^0gi0​,hi0​ 分别被初始化为随机变量 www 和词的 embedding e(xi)e(x_i)e(xi​)。接着向高层传递时，按照下面的过程逐层传递： gztm←Attention(Q=gztm−1,KV=hz&lt;tm−1;θ)hztm←Attention(Q=hztm−1,KV=hz≤tm−1;θ)\\begin{array}{c} g_{z_t}^m\\leftarrow Attention(Q = g_{z_t}^{m-1}, KV = h_{z_{&lt;t}}^{m-1}; \\theta)\\\\ h_{z_t}^m\\leftarrow Attention(Q = h_{z_t}^{m-1}, KV = h_{z_{\\leq t}}^{m-1}; \\theta) \\end{array}gzt​m​←Attention(Q=gzt​m−1​,KV=hz&lt;t​m−1​;θ)hzt​m​←Attention(Q=hzt​m−1​,KV=hz≤t​m−1​;θ)​ 上述两个流分别是 Query 流和 Content 流。其中 Query 流中的 QQQ 来自上一层的 gztg_{z_t}gzt​​, Content 流的 QQQ 来自上一层的 hzth_{z_t}hzt​​, 而两个流的 K,VK, VK,V 都来自上一层的 hhh, 其中 Query 流的 K,VK, VK,V 来自 hz&lt;th_{z_{&lt;t}}hz&lt;t​​, 代表其不能访问当前 token 的信息，Content 流的 K,VK, VK,V 来自 hz≤th_{z_{\\leq t}}hz≤t​​, 包含当前 token 的语义信息。 如图是双流注意力模型的示意图。 (a) 图所示是 Content 流的计算过程，假设排列为 3→2→4→13\\rightarrow 2\\rightarrow 4\\rightarrow 13→2→4→1, 并且当前正在编码的是第一个词。Content 流可以参考所有 token 的信息，因此 KV=[h10,h20,h30,h40]KV = [h_1^0, h_2^0, h_3^0, h_4^0]KV=[h10​,h20​,h30​,h40​], 而 Q=h10Q = h_1^0Q=h10​。 (b) 图所示是 Query 流的计算过程，因为 Query 流不能参考自己的内容，因此 KV=[h20,h30,h40]KV = [h_2^0, h_3^0, h_4^0]KV=[h20​,h30​,h40​], 而 Q=g10Q = g_1^0Q=g10​ © 图是完整的计算过程，按照从下而上的顺序，首先 h0,g0h^0, g^0h0,g0 分别被初始化为词嵌入 e(xi)e(x_i)e(xi​) 和随机变量 www，然后 Content Mask 和 Query Mask 计算第一层的输出 h1,g1h^1, g^1h1,g1, 逐层传递。图中的 Attention Mask 中，纵坐标从上到下表示当前预测的 token 编号，横坐标从左到右表示可以 Attend to 的 token 编号，其中红点表示可以 Attend to, 白点表示不能 Attend to。因此， Content Mask 第一行表示：预测第一个 token 时可以 Attend to 所有的 token，第三行表示预测第三个 token 时只能 Attend to 自己。Query Mask 的对角线全白表示 Query Stream 不能 Attend to 自身。 部分预测 排列语言模型可以利用每种排列中的每一个 token，由于计算量的原因，只在所有排列中随机抽取部分排列，同时由于对所有的 token 进行预测模型难以收敛，XLNet 只选择排列的最后 1K\\frac{1}{K}K1​ token 进行预测。 可以将一个排列 zzz 分成两个子序列 z≤cz_{\\leq c}z≤c​ 和 z&gt;cz_{&gt;c}z&gt;c​, 分别叫做 non-target 序列和 target 序列，ccc 是分割点。因此有：∣z∣−c∣z∣=1K\\frac{|z| - c}{|z|} = \\frac{1}{K}∣z∣∣z∣−c​=K1​ 结合 Transformer-XL 的思想 相对位置编码 cache 隐状态 假设有两个从原始序列 s\\mathbf{s}s 中抽取的 segment，x~=s1:T\\widetilde{x} = s_{1:T}x=s1:T​ 和 x=sT+1:2Tx = s_{T+1:2T}x=sT+1:2T​。同时假设 z~,z\\widetilde{z}, zz,z 分别是 [1,...,T],[T+1,...,2T][1, ..., T], [T+1, ..., 2T][1,...,T],[T+1,...,2T] 的一个排列。然后根据排列 z~\\widetilde{z}z 的链式分解计算第一个 segment，并且把 Content 流的隐状态 h~\\widetilde{h}h 缓存下来，那么第二个 segment 的 Content 流计算方法如下 hztm←Attention(Q=hztm−1,KV=[h~m−1,hz≤tm−1];θ)h_{z_t}^m \\leftarrow Attention(Q = h_{z_t}^{m-1}, KV = [\\widetilde{h}^{m-1}, h_{z\\leq t}^{m-1}]; \\theta) hzt​m​←Attention(Q=hzt​m−1​,KV=[hm−1,hz≤tm−1​];θ) 用自然语言描述为：计算 ztz_tzt​ 第 mmm 层的隐藏状态时，Query 是上一层的隐藏状态 hztm−1h_{z_t}^{m - 1}hzt​m−1​, 而 Key 和 Value 除了 z1,...,ztz_1, ..., z_tz1​,...,zt​ 第 m−1m-1m−1 层的隐状态，还要 Attend to 缓存的上一个 segment 的所有位置的隐藏状态。 与 BERT 对比 BERT 使用 MLM，因此只能预测部分词。XLNet 理论上可以预测所有词，但出于性能考虑只预测部分词。除此之外，BERT 由于独立性假设的原因，信息的利用率弱于 XLNet。 例如假设输入是 [New, York, is, a, city]\\text{[New, York, is, a, city]}[New, York, is, a, city], 并且 XLNet 和 BERT 都选中了使用 [is, a, city]\\text{[is, a, city]}[is, a, city] 来预测 New\\text{New}New 和 York\\text{York}York。同时假设 XLNet 使用的排列顺序是 is, a, city, New, York\\text{is, a, city, New, York}is, a, city, New, York。那么两个模型要优化的目标函数分别为： JBERT=log⁡p(New|is a city)+log⁡p(York|is a city)JXLNet=log⁡p(New|is a city)+log⁡p(York|New is a city)\\begin{array}{c} \\mathcal{J}_\\text{BERT} = \\log p(\\text{New|is a city}) + \\log p(\\text{York|is a city})\\\\ \\mathcal{J}_\\text{XLNet} = \\log p(\\text{New|is a city}) + \\log p(\\text{York|\\color{red}{New} \\color{normal}{is a city}}) \\end{array} JBERT​=logp(New|is a city)+logp(York|is a city)JXLNet​=logp(New|is a city)+logp(York|New is a city)​ 参考 http://papers.nips.cc/paper/8812-xlnet-generalizedautoregressive-pretraining-for-language-understanding.pdf http://fancyerii.github.io/2019/06/30/xlnet-theory/ https://zhuanlan.zhihu.com/p/70257427","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Transformer","slug":"Transformer","permalink":"https://blog.zhuwenq.cc/tags/Transformer/"},{"name":"XLNet","slug":"XLNet","permalink":"https://blog.zhuwenq.cc/tags/XLNet/"}]},{"title":"RoBERTa: A Robustly Optimized BERT Pre-training Approach","slug":"RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach","date":"2021-09-10T16:00:00.000Z","updated":"2023-06-21T06:51:28.884Z","comments":true,"path":"RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/","link":"","permalink":"https://blog.zhuwenq.cc/RoBERTa-A-Robustly-Optimized-BERT-Pretraining-Approach/","excerpt":"Introduction RoBERTa是一种新的BERT训练方法，超过了所有的post-BERT方法的性能。 本文通过重复BERT的研究过程，发现BERT是严重欠训练的。本文旨在仔细评估超参数和训练集的大小对模型性能的影响。 RoBERTa对BERT做出的修改如下： 总体上，本文的贡献为： 展示了一系列BERT设计和训练的选择和策略，介绍了可以提高下游任务性能的选择 使用了一个新的数据集：CC-NEWS，确认了使用更多数据进行预训练能够进一步提高下游任务上的表现 实验证明了MLM在正确的设计下优于最近新提出的所有方法","text":"Introduction RoBERTa是一种新的BERT训练方法，超过了所有的post-BERT方法的性能。 本文通过重复BERT的研究过程，发现BERT是严重欠训练的。本文旨在仔细评估超参数和训练集的大小对模型性能的影响。 RoBERTa对BERT做出的修改如下： 总体上，本文的贡献为： 展示了一系列BERT设计和训练的选择和策略，介绍了可以提高下游任务性能的选择 使用了一个新的数据集：CC-NEWS，确认了使用更多数据进行预训练能够进一步提高下游任务上的表现 实验证明了MLM在正确的设计下优于最近新提出的所有方法 Background BERT的介绍 [[BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding]] Experimental Setup 实现BERT 使用原始BERT除了学习率和热身步数的优化超参数实现。在输入序列长度上，使用长度 T=512T = 512T=512的设定。不随机插入短序列，仅仅在全长度序列上训练(BERT在前90%的更新上使用缩短的序列) 数据 语言模型预训练严重依赖大型数据集，很多工作使用的数据集不能公开，本文收集整理了超过160GB的未压缩文本，其中包括以下的文本语料库： BookCorpus+EnglishWikipediaBook Corpus+ English WikipediaBookCorpus+EnglishWikipedia: 原始BERT预训练使用的数据（16GB） CC−NewsCC-NewsCC−News: 收集自CommonCraw新闻数据集的英文部分。包含2016年9月到2019年2月之间爬取的6千3百万篇英文新闻文章（过滤后76GB） OpenWebTextOpen Web TextOpenWebText: WebText语料库的开源实现，来自Reddit上至少有三个赞的web内容 StoriesStoriesStories: CommonCrawl数据集的符合Winograd架构故事的子集 评估 在以下benchmarks上评估预训练模型的性能： GLUE：我们将模型在GLUE的每个单独的任务数据上进行微调，然后进行评估 SQuAD：分别在V1.1和V2.0两个版本上进行评估。在V1.1上，使用与BERT一样的范围预测方法，在V2.0上，我们添加了一个额外的二分类层用于预测这个问题是否可回答(答案是否在给定的文本内)，然后将这个二分类层与范围预测的Loss相加来联合训练。在评估时，仅仅对二分类层输出为可以回答的问题进行预测。 RACE：RACE数据集中对于每个问题的相关文本相比其他数据集更长，且数据集中需要推理的问题占比很大 benchmark的介绍见： [[Data-Set-and-Benchmark]] 训练过程分析 以下的实验和分析旨在探索和量化在预训练BERT的过程中重要的参数选择 静态/动态遮罩 原始的BERT使用static masking，在数据处理时生成静态的mask，然后在训练时使用。为了避免句子在不同的epoch中mask相同，将每个句子复制10次，生成10种不同的mask，然后在40个epoch种使用。 dynamic masking即每个序列在送入模型之前才随机生成mask，这种方法在我们的实验中有轻微优势 模型的输入格式和NSP BERT的实验观察到移除NSP会对模型性能产生影响。但是最近的工作(XLNet, SpanBERT)质疑NSP的必要性。本文通过下面的实验研究NSP的作用 SEGMENT-PAIR+NSP\\text{SEGMENT-PAIR+NSP}SEGMENT-PAIR+NSP: 原始BERT中使用的设定。输入包含两个文本段，每个段可能包含若干个句子，总的token数不超过512。两个文本段等概率地从同一文档或不同文档中采样。 SENTENCE-PAIR+NSP\\text{SENTENCE-PAIR+NSP}SENTENCE-PAIR+NSP: 输入中包含两个自然语言句子，两个句子从同一文档中连续采集或从不同文档分别采集的概率相等。因为句子中的token数显著少于512，我们增加了batch size以保证每个batch中的token数与 SEGMENT-PAIR + NSP\\text{SEGMENT-PAIR + NSP}SEGMENT-PAIR + NSP的相近 FULL-SENTENCES\\text{FULL-SENTENCES}FULL-SENTENCES: 输入从一个或多个文档中连续采集，使得每个输入的token长度都是512。输入可能跨越文档的边界，在文档分界出添加额外的分隔符。移除了NSP Loss DOC-SENTENCES\\text{DOC-SENTENCES}DOC-SENTENCES: 与 FULL-SENTENCES\\text{FULL-SENTENCES}FULL-SENTENCES 相似，但是输入不再跨文档采集。当采集到文档末尾时，输入中的token数量可能会少于512。这种情况下我们通过动态改变batch size来使token数目达到差不多的水平。移除了NSP Loss 实验结果： 使用句子作为输入会降低模型在下游任务的性能。这可能是因为使用句子而不是文本段使模型无法学习长距离依赖。 移除NSP Loss不影响甚至略有提高在下游任务的性能。这与BERT的实验结果相反，可能是因为BERT的实验中仅仅去除了NSP Loss，但是其输入格式仍然保持 SEGMENT-PAIR\\text{SEGMENT-PAIR}SEGMENT-PAIR 的格式(两个分段可能从不同文档采集) 让输入序列从同一个文档中采集( DOC-SENTENCES\\text{DOC-SENTENCES}DOC-SENTENCES )相比将不同文档的句子打包在一起 ( FULL-SENTENCES\\text{FULL-SENTENCES}FULL-SENTENCES )有轻微的优势。但是由于 DOC-SENTENCES\\text{DOC-SENTENCES}DOC-SENTENCES 需要动态改变batch size，因此后续的实验中使用 FULL-SENTENCES\\text{FULL-SENTENCES}FULL-SENTENCES 使用更大的batch训练 原始的 BERTBASE\\text{BERT}_\\text{BASE}BERTBASE​ 使用 batch size = 256个序列训练了1M 步，这在计算量上相当于通过梯度累积在 batch size = 2K 个序列训练 125K 步，或者 batch size = 8K 个序列训练 31K 步 后续的实验采用 batch size = 8K 的设置 文本编码 原始的BERT使用字符级的BPE，本文使用字节级的BPE。两者在性能上相差无几，但是字节级的BPE更具通用性。 Byte-Pair Encoding, BPE: RoBERTa 将以上提到的改进汇总，称之为Robustly optimized BERT approach, RoBERTa。具体地说，包括以下改进： 动态遮罩 FULL-SENTENCES\\text{FULL-SENTENCES}FULL-SENTENCES with out NSP 更大的batch size 字节级BPE 预训练数据 训练的次数 对于前四点改进，我们使用与原始BERT的训练数据相当的数据(16GB)训练RoBERTa，验证改进的有效性 然后结合额外的数据(160GB)，相同的训练步数（100K）进行预训练，取得了性能的提高，证明了数据大小和多元化的重要性 最后将训练步数提高到300K和500K进行预训练，观察到显著的提升。说明训练量的重要性。并且观察到最长的训练都没有导致过拟合。 最后将RoBERTa分别在GLUE，SQuAD，RACE上进行评估","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"BERT","slug":"BERT","permalink":"https://blog.zhuwenq.cc/tags/BERT/"}]},{"title":"BART: Bidirectional and Auto-Regressive Transformers","slug":"BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension","date":"2021-09-06T16:00:00.000Z","updated":"2023-06-21T06:51:28.776Z","comments":true,"path":"BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension/","link":"","permalink":"https://blog.zhuwenq.cc/BART-Denoising-Sequence-to-Sequence-Pre-training-for-Natural-Language-Generation-Translation-and-Comprehension/","excerpt":"Introduction BART是一种用于预训练seq2seq模型的降噪自动编码器。BART使用标准的Transformer架构，训练时使用任意噪声函数对文本加噪，然后试图重建受损的文本。 BART的一个关键优势在于它的预训练过程使用任意的噪声函数，包括打乱原始句子的顺序，对任意长度的文本片段进行掩码替换等。这迫使模型对句子的整体长度进行更多推理，并对输入进行更长范围的转换。 BART格外适合微调到生成任务，在阅读理解，抽象对话，问答和总结任务中也有很好的效果。本文还基于BART介绍了一种新的机器翻译的模式，即在BART之上添加一些额外的Transformer层来将外语转换为带有噪声的目标语言，从而使用BART作为目标端语言模型进行去噪。","text":"Introduction BART是一种用于预训练seq2seq模型的降噪自动编码器。BART使用标准的Transformer架构，训练时使用任意噪声函数对文本加噪，然后试图重建受损的文本。 BART的一个关键优势在于它的预训练过程使用任意的噪声函数，包括打乱原始句子的顺序，对任意长度的文本片段进行掩码替换等。这迫使模型对句子的整体长度进行更多推理，并对输入进行更长范围的转换。 BART格外适合微调到生成任务，在阅读理解，抽象对话，问答和总结任务中也有很好的效果。本文还基于BART介绍了一种新的机器翻译的模式，即在BART之上添加一些额外的Transformer层来将外语转换为带有噪声的目标语言，从而使用BART作为目标端语言模型进行去噪。 Background 早期的语言模型使用L2R(GPT)或者L2R和R2L叠加(ELMo)的方法，这样只能基于一侧的信息或者无法提取两侧信息的交互，在一些任务上有问题。（自回归模型，在生成任务上表现较好，但在分类任务上较差） BERT使用MLM，允许在预训练过程中学习两侧信息的交互，但是由于预测过程不是自回归的，在生成任务上不是最优的。 Model BART是一个标准的seq2seq模型，其编码器是双向的，解码器是L2R自回归的。在预训练时我们优化重建文本和原始文本的负对数似然(negative log likehood) Architecture BART使用标准的Transformer架构，但是将ReLu激活函数替换为GeLus，并且从分布 N(0,0.02)N(0, 0.02)N(0,0.02) 中初始化参数。 BART的模型结构与BERT相似，有以下不同： decoder的每一层额外的在encoder的最终隐藏层上做交叉注意 BERT在执行单词预测之前还有一层前馈网络，但BART没有 Pre-training BART BART基于受损文本重建任务进行预训练，重建损失即decoder的输出和原始文本之间的交叉熵。 BART允许使用任意类型的噪声。实验中我们使用了以下几种： Token Masking: 与BERT使用的相同，即随机采样一些tokens并使用[MASK]替换 Token Deletion: 随机删去一些tokens，与Token Masking相比，这种方法迫使模型预测被删除的位置 Text Infilling: 随机采样一些长度符合泊松分布( λ=3\\lambda = 3λ=3 ) 的文本片段，并用遮罩 [MASK] 替换。对于长度为0的文本片段，相当于插入了[MASK]。这种噪声迫使模型预测被替换的文本片段的长度。 Sentence Permutation: 将文档按照句号分割成不同的句子，然后随机排列句子的顺序。这种噪声迫使模型学习同一文档中句子的顺序。 Document Rotation: 随机选择一个token，然后将文本旋转到以这个token为开头的状态。这种噪声训练模型识别文本开头的能力。 Fine-tuning BART 序列分类任务 对于序列分类任务，将输入同时送进encoder和decoder，与BERT的[CLS] token类似，BART在decoder输出的末尾位置添加一个token专门用于分类任务，对应于这个token的最终隐藏层输出经过一个线性分类器得到输入序列的分类 在末尾添加的原因是这样decoder中这个token的表征可以注意到整个输入在decoder中的信息。 Token分类任务 对于Token分类任务，将完整的输入文本送入encoder和decoder，然后使用decoder的顶部隐藏状态(top hidden state)作为每个词的表征，这个表征用于token的分类(即每个词属于某种token)。 序列生成任务 BART的decoder是自回归的，因此它可以直接微调到如抽取式问答和总结类的生成任务。在这类任务中，信息直接从输入中抽取并修改，这与BART的去噪预训练目标吻合。 机器翻译任务 对于翻译任务，BART通过将encoder的embedding层替换为少量随机初始化的encoder层，利用这个新的encoder将外语编码成有噪声的目标语言的编码。然后将该有噪声的编码作为BART的输入，由BART将其降噪成高质量的翻译结果。即，BART作为目标语言端解码器，新添加的encoder作为外语端编码器，组成了一个新的seq2seq机器翻译模型。 具体地说，微调该翻译任务分为两步： 保持大多数的BART参数不变，仅更新新添加的encoder，BART的位置嵌入层和BART的encoder中第一层自注意力层的投影矩阵( WQ,WK,WVW_Q, W_K, W_VWQ​,WK​,WV​ ) 在少量迭代下更新所有的参数 数据/任务集 SQuAD, MNLI, ELI5, XSum。详情参考： [[Data-Set-and-Benchmark]] 实验说明的一些结论 实验结果： 预训练方法的性能因任务而异 Token Masking是十分重要的预训练目标 L2R的预训练能够提高生成任务的性能 对于SQuAD任务，双向编码器十分重要(在分类任务中，结合token之后的信息十分关键) 预训练目标不是唯一关键的因素","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Transformer","slug":"Transformer","permalink":"https://blog.zhuwenq.cc/tags/Transformer/"},{"name":"Autoregressive","slug":"Autoregressive","permalink":"https://blog.zhuwenq.cc/tags/Autoregressive/"}]},{"title":"Attention is all you need","slug":"Attention-is-all-you-need","date":"2021-09-01T16:00:00.000Z","updated":"2023-06-21T06:51:28.776Z","comments":true,"path":"Attention-is-all-you-need/","link":"","permalink":"https://blog.zhuwenq.cc/Attention-is-all-you-need/","excerpt":"Introduction 序列转导模型的编码器和解码器主要基于复杂的递归或卷积神经网络，其中表现最好的模型在编码器和解码器之间通过一个注意力机制层相连。我们提出一种简单的模型架构：Transformer。仅仅基于注意力机制，完全去除递归和卷积。同时Transformer可以很好地推广到其他任务中。","text":"Introduction 序列转导模型的编码器和解码器主要基于复杂的递归或卷积神经网络，其中表现最好的模型在编码器和解码器之间通过一个注意力机制层相连。我们提出一种简单的模型架构：Transformer。仅仅基于注意力机制，完全去除递归和卷积。同时Transformer可以很好地推广到其他任务中。 Background 对于递归模型，其通常沿着输入和输出序列的符号位置进行因数计算，将位置与计算时间中的步骤对其， ttt 时刻的隐藏状态 hth_tht​ 作为下一时刻 t+1t+1t+1 计算的输入。这种固有的顺序限制了训练时的并行化。本文提出的Transformer完全依靠注意力机制来计算输入和输出之间的全局关系。 对于使用卷积神经网络减少顺序计算的模型，将来自不同位置的输入或输出信号关联起来所需的计算操作随着信号之间的距离的增加而增加。这使得学习远距离位置信息之间的依赖关系变得困难。在Transformer中，这种操作的数量被控制为一个恒定的常数（尽管由于平均注意力加权位置而降低了有效分辨率，Transformer使用Multi-Head Attention来解决）。 Self-attention，有时称为intra-attention。是一种注意力机制，将单个序列的不同位置联系起来，以计算序列的表示。 Model Architecture Transformer基于encoder-decoder 架构。其中，encoder将由符号表征的输入序列 (x1,...,xn)(x_1, ..., x_n)(x1​,...,xn​) 映射为连续的序列表征 z=(z1,...,zn)\\mathbf{z} = (z_1, ..., z_n)z=(z1​,...,zn​)。对于给定的 z\\mathbf{z}z ，decoder利用其生成一个输出的符号序列 (y1,...,yn)(y_1, ..., y_n)(y1​,...,yn​) 。在encoder和decoder中，使用堆栈式自注意力和逐点的全连接层。 Encoder 和 Decoder Encoder : Encoder 由 N=6N = 6N=6 个相同的层组成，每个层都有两个子层。第一个子层是多头自注意力机制(multi-head self-attention mechanism)，第二个子层是一个逐位置的全连接前馈网络(position-wise fully connected feed-forward network) 每个子层之后都有一个Layer normalization层，并通过残差连接相连。即，每个子层的输出是： LayerNorm(x+Sublayer(x))\\text{LayerNorm}(x + \\text{Sublayer}(x))LayerNorm(x+Sublayer(x))，其中的 Sublayer(x)\\text{Sublayer}(x)Sublayer(x) 表示子层所实现的操作。为了应用残差连接，模型中的所有子层，包括嵌入层，输出维度都是 dmodel=512d_{model} = 512dmodel​=512 Decoder : Decoder 也是由 N=6N = 6N=6 个相同的层组成，每个层中除了与encoder相同的两个子层外，decoder还插入了第三个子层，用于对encoder stack的输出进行multi-head attention。 与encoder类似，decoder中也使用了残差连接的layer normalization层。decoder中修改了stack中的自注意力层(Masked Multi-head Attention)，防止对某位置的解码过程中涉及后续位置的信息。这种遮蔽式的修改，配合输出的embedding偏移一个位置的设置，确保了对 iii 位置的预测只依赖小于 iii 位置处的已知输出。 Attention 注意力函数 (attention function) 就是将一个 query 和一系列 key-value 对映射到一个输出(output)的过程，其中的 query, key, value和output都是向量。output由values的加权和计算，其中分配给每个value的权重由querry和相应的key经过一个函数计算。 这里提到的 querries，keys和values是由输入 XXX 计算得到的 Scaled Dot-Product Attention 我们使用的Attention称为 Scaled Dot-Product Attention。它的输入包括维度为 dkd_kdk​ 的queries和keys，以及维度为 dvd_vdv​ 的values。我们计算每个 querry 与所有 keys 的点乘，并除以 dk\\sqrt{d_k}dk​​ ，然后经过一个 softmax 获取所有 values 的权重。 实践中，我们通过将querries打包进同一个矩阵 QQQ 来并行化attention function的计算。keys和values也被分别打包为矩阵 KKK 和 VVV，计算过程为： Attention(Q,K,V)=softmax(QKTdk)V\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})VAttention(Q,K,V)=softmax(dk​​QKT​)V 我们的attention function除了乘以一个缩放因子 1dk\\frac{1}{\\sqrt{d_k}}dk​​1​ 之外与 dot-product attention相同。缩放的原因是：[我们怀疑]对于较大的维度值 dkd_kdk​ ，点积运算结果的增长幅度较大，会将 softmax函数的梯度推向非常小的区域。 Multi-Head Attention 与在一个单独的attention function上对 dmodeld_{model}dmodel​ 维度的 querries, keys 和 values进行计算相比，将 querries，keys 和 values 以不同的，可学习的线性投影分别投射到 hhh 个通道上并行进行 attention function更有好处，在每个通道上，querries, keys和values分别为 dk,dk和dvd_k, d_k和d_vdk​,dk​和dv​维度。在 hhh 个通道上并行进行的attention function最终得到 hhh 个输出矩阵，将其连接起来再经过一个投影矩阵，得到最终的结果。这种机制允许模型同时关注来自不同位置的不同表征子空间的信息。在只有一个attention head时，加权平均抑制了这种特性。 Multi-Head Attention的数学过程为： MultiHead(Q,K,V)=Concat(head1,...,headh)WO\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^OMultiHead(Q,K,V)=Concat(head1​,...,headh​)WO, 其中 headi=Attention(QWiQ,KWiK,VWiV)head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)headi​=Attention(QWiQ​,KWiK​,VWiV​) 其中的 WiQ∈Rdmodel×dk,WiK∈Rdmodel×dk,WiV∈Rdmodel×dvW_i^Q \\in \\mathbb{R}^{d_{model}\\times d_k}, W_i^K \\in \\mathbb{R}^{d_{model}\\times d_k}, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}WiQ​∈Rdmodel​×dk​,WiK​∈Rdmodel​×dk​,WiV​∈Rdmodel​×dv​即是分别用在 Q,K,VQ, K, VQ,K,V 上的投影矩阵， WO∈Rhdv×dmodelW^O\\in \\mathbb{R}^{hd_v\\times d_{model}}WO∈Rhdv​×dmodel​是用作连接后的结果矩阵的投影矩阵。 在本文的工作中，我们使用了 h=8h = 8h=8 个并行attention层(heads)，对于每一个并行层，使用 dk=dv=dmodel/h=64d_k = d_v = d_{model}/h = 64dk​=dv​=dmodel​/h=64 Applications of Attention in our Model 在encoder-decoder attention层（decoder的第二个multi-attention层），querries来自上一层decoder的输出，keys和values则由encoder的输出计算得到。这种机制使decoder在每一个位置上都可以注意到输入所有位置上的信息。 在encoder的self-attention层中，所有的querries，keys和values都来自上一层encoder的输出，encoder的每一个位置都可以注意上一层encoder输出的所有位置 decoder中的self-attention层（第一个Multi-attention层）允许每个位置注意到这个位置之前包括这个位置的信息，但是禁止左侧位置参考右侧的信息，以保持模型的自回归特性。我们通过将softmax的输入中所有非法连接的值设置为 −inf⁡-\\inf−inf 来实现这一点。 逐位置前馈网络(Position-wise Feed-Forward Networks) 在encoder和decoder的每一层中，都包含一个全连接的前馈网络层，对所有位置分别进行相同的操作。由两个全连接层组成，第一个全连接层的激活函数是ReLu，第二个不需要激活函数：FFN(x)=max(0,xW1+b1)W2+b2\\text{FFN}(x) = max(0, xW_1+b_1)W_2 + b_2FFN(x)=max(0,xW1​+b1​)W2​+b2​ 每一层不同位置上的线性变换相同，但不同层上的参数不同。另一种描述这种过程的方式是使用两个size为1的卷积核进行两次卷积，其中输入和输出维度为 dmodel=512d_{model}=512dmodel​=512，内层维度为 dff=2048d_{ff} = 2048dff​=2048 位置编码(Positional Encoding) Transformer模型中没有使用递归和卷积，因此必须在输入中插入关于token在序列中位置的信息，从而使模型能够利用序列的顺序信息。我们将与embedding相同维度的positional encoding与embedding相加作为encoder和decoder的输入。本文使用的是cos函数的调频编码： {PE(pos,2i)=sin⁡(pos/100002i/dmodel)PE(pos,2i+1)=cos⁡(pos/100002i/dmodel)\\begin{cases}PE_{(pos, 2i)} &amp;= \\sin(pos/10000^{2i/d_{model}}) \\\\PE_{(pos, 2i+1)} &amp;= \\cos(pos/10000^{2i/d_{model}})\\end{cases}{PE(pos,2i)​PE(pos,2i+1)​​=sin(pos/100002i/dmodel​)=cos(pos/100002i/dmodel​)​ 其中， pospospos 是token的位置， iii 是维度。即，每个维度的位置编码对应着一个正弦波，不同维度的正弦波的波长形成 2π−10000⋅2π2\\pi - 10000\\cdot 2\\pi2π−10000⋅2π范围的几何级数。我们选择这种编码是因为对于任意的offset kkk， PEpos+KPE_{pos+K}PEpos+K​可以表示为 PEposPE_{pos}PEpos​的线性变换，因此我们假设这种编码方法可以使模型轻易学习到注意相对位置特征。 Why Self-Attention self-attention层与递归和卷积层相比在序列转导模型中的优势： 每层的总计算复杂度 可以被并行化的计算量，由必要的最小顺序化操作数量衡量 在网络中计算long-range依赖需要的计算路径长度。即为了学习long-range依赖，信号在网络中必须要经过的路径长度，这个长度越短，模型就越容易学习long-range依赖。 训练 优化器 Adam optimizer，参数设置为： β1=0.9,β2=0.98,ϵ=10−9\\beta_1 = 0.9, \\beta_2 = 0.98, \\epsilon = 10^{-9}β1​=0.9,β2​=0.98,ϵ=10−9。学习率在训练过程中按照以下公式变化： lrate=dmodel−0.5⋅min(step_num−0.5,step_num⋅warmup_steps−1.5)lrate = d_{model}^{-0.5}\\cdot \\text{min}(step\\_num^{-0.5}, step\\_num\\cdot warmup\\_steps^{-1.5})lrate=dmodel−0.5​⋅min(step_num−0.5,step_num⋅warmup_steps−1.5) 其中 warmup_steps=4000warmup\\_steps = 4000warmup_steps=4000 正则化 Residual Dropout 1. 在每个子层的输出位置，残差连接之前。2. 在encoder及decoder的embedding和positional encoding相加的位置。参数为 pdrop=0.1p_{drop} = 0.1pdrop​=0.1 Label Smoothing 训练时使用了 ϵls=0.1\\epsilon_{ls} = 0.1ϵls​=0.1的标签平滑","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Transformer","slug":"Transformer","permalink":"https://blog.zhuwenq.cc/tags/Transformer/"}]},{"title":"Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","slug":"Pre-train-Prompt-and-Predict","date":"2021-08-29T16:00:00.000Z","updated":"2023-06-21T06:51:28.832Z","comments":true,"path":"Pre-train-Prompt-and-Predict/","link":"","permalink":"https://blog.zhuwenq.cc/Pre-train-Prompt-and-Predict/","excerpt":"刘鹏飞博士著 Prompt 范式综述","text":"刘鹏飞博士著 Prompt 范式综述 NLP中的两次巨变 feature/architecture engineering to objective engineering 早期的NLP主要采用完全监督学习方法，用于特定任务的模型仅在特定的数据集上训练。由于完全监督的方法需要大量高质量的有标签数据集，早期的NLP模型十分依赖研究者的专业领域知识以便从原始数据中定义和抽取特征，并且给模型以恰当的归纳偏置(inductive bias)，这被称为特征工程(feature engineering) 随着神经网络的发展，模型可以在训练中学习特征。研究者主要通过设计有利于学习某类特征的特殊的网络架构来提供归纳偏差，这被称为架构工程(architecture engineering) 第一个巨变发生在2017-2019年，NLP的方向转为预训练和微调的模式，完全监督的方法从此式微。这种模式下，一个固定架构的预训练模型从大型数据集中抽取语言的通用特征，然后通过引入额外的参数并使用特定任务的目标函数对它们微调来适应不同的下游任务。这种模式下，NLP的主要任务是设计在预训练和微调时的目标函数，称为目标工程(objective engineering) objective engineering to promot engineering 2021年正处于第二次巨变中，“预训练-微调”的过程被我们称为“预训练-提示-预测”的过程替代。这种模式下，下游任务通过重组来适配语言模型，而不是语言模型通过微调来适配下游任务。这种模式的优点是，一个完全无监督预训练出的模型在恰当的提示下可以解决多种问题。此时NLP的主要任务就变成了设计出恰当的提示，称为提示工程(promot engineering) Prompting 的正式描述 传统监督学习的NLP 传统的监督学习的NLP过程中，模型 P(y∣x;θ)P(y|x;\\theta)P(y∣x;θ) 对于输入xxx预测出输出yyy。为了学习参数θ\\thetaθ，通常需要成组的输入和输出数据集(有标签数据)来进行训练以对条件概率进行预测。在很多任务背景下这样的数据集数量不够 Prompting 的基本过程和概念 基于Promopting的学习方法通过计算文本xxx本身的概率而不是条件概率来预测输出yyy减少甚至消除对大规模监督数据集的需求 下表是Prompting的几个基本概念： 基于Prompting的方法分三步来预测输出y^\\widehat{y}y​ Prompting function 第一步通过形如 fprompt(⋅)f_{prompt}(\\cdot)fprompt​(⋅) 的prompting function来修改输入的文本xxx为 x′=fprompt(x)x&#x27;=f_{prompt}(x)x′=fprompt​(x)。这个函数包括两个步骤： 使用一个文本模板，该模板有两个插槽：输入插槽 [X][X][X] 用来放输入xxx, 答案插槽[Z][Z][Z]用来放模型生成的答案zzz 把输入文本xxx放进输入插槽[X][X][X] 注意： 上述的Prompts有一个待填充的空插槽[Z][Z][Z]，插槽位于Prompts中间的称为cloze prompt， 插槽位于末尾的称为prefix prompt 许多情况下，模板并不必须由自然语言标记组成，可以嵌入虚拟的字(如数字id)，甚至某些prompting方法可以生成连续的向量 输入插槽[X][X][X]和答案插槽[Z][Z][Z]的数量可以根据任务的需要灵活调整 答案最大化搜索 经过 prompting function 之后，输入文本 xxx 变为带有答案插槽的 x′x&#x27;x′，随后我们搜索获得最高分数的答案文本 z^\\widehat{z}z。定义 Z\\mathcal{Z}Z 为 zzz 所有可能值的集合，例如在文本生成任务中， Z\\mathcal{Z}Z 可能包括所有语言的范围，而在文本识别的任务中， Z\\mathcal{Z}Z 只是一个小的子集。 定义函数 ffill(x′,z)f_{fill}(x&#x27;, z)ffill​(x′,z) 用于将可能的答案 zzz 填充进 prompt x′x&#x27;x′ 的插槽 [X][X][X] 中，将经过这一步骤的 prompt 称为 filled prompt ，如果填充的是正确答案，称为 answered prompt 。 搜索的过程实际是：对于 Z\\mathcal{Z}Z 中每一个可能的答案 zzz ，利用预训练的 LM: P(⋅;θ)P(\\cdot; \\theta)P(⋅;θ) 来计算其概率，并取其中概率最大的答案： z^=searchz∈ZP(ffill(x′,z);θ)\\widehat{z} = \\underset{z\\in \\mathcal{Z}}{\\text{search}} P(f_{\\text{fill}}(x&#x27;, z); \\theta) z=z∈Zsearch​P(ffill​(x′,z);θ) 答案映射 最后，还需要将最高分的答案 z^\\widehat{z}z 映射为最高分的输出 y^\\widehat{y}y​，在某些应用中，答案 z^\\widehat{z}z 就是需要的输出 y^\\widehat{y}y​，所以这一步骤并不一定是必须的 设计Prompting的几个要点 预训练模型的选择 构建prompt函数 fprompt(x)f_{prompt}(x)fprompt​(x) 的方法 可能的答案集 Z\\mathcal{Z}Z 的设计方法和答案映射的方法 对于特定的任务对prompt模式的扩展 prompt 模型的训练策略 预训练语言模型 Training Objectives（损失函数） Standard Language Model, SLM 标准语言模型通过训练来优化对文本的预测概率 P(x)P(x)P(x)，通常是自回归(autoregressive) 的方式对文本序列中从左到右的 tokens 进行预测 较为流行的用于替换 SLM 的 Objective 是 noise objective ，通过噪声函数 x^=fnoise(x)\\widehat{x}=f_{noise}(x)x=fnoise​(x) 向输入文本序列添加噪声，然后根据噪声序列预测原始文本序列 P(x∣x^)P(x|\\widehat{x})P(x∣x)。 noise objective 有以下两种主要的形式： Corrupted Text Reconstruction, CTR 这种目标通过计算文本中受损的loss来恢复处理后的文本 Full Text Reconstruction, FTR 这种目标通过计算整个文本序列的loss来恢复处理后的文本 ## 噪声函数 在基于重建的训练目标中，不同的噪声函数会影响训练的效果，另外，可以通过控制噪声的种类引入先验知识。例如，使用专注于句子中实体的噪声函数，预训练出的语言模型就会在实体上有较高的预测性能 Masking: 用一个MASK替换文本中的一个或多个token Replacement: 用一段文本替换一个或多个token, 而不是固定的MASK Deletion: 删除一个或多个token，通常和FTR Loss一起使用 Permutation: 把文本分成不同的块，然后重新组合 下表是这几种噪声函数的例子： ## Directionality of Representations Left-to-Right: 每个词的表示由词本身和句子中前面的所有词计算，常用于SLM和FTR Bidirectional: 每个词的表示由句子中所有词计算 混合的模式: 将以上两种策略混用，注意力机制 下图是表示顺序的模式示意图： ## 典型的预训练方法 由不同的损失函数，噪声函数和方向性可以组合出不同的预训练方法： Left-to-Right Language Model, L2R LMs 自回归语言模型的一种，用于预测即将出现的词或给出一组词的概率 P(x),x=x1,x2,...,xnP(\\mathbf{x}), \\mathbb{x} = x_1, x_2, ..., x_nP(x),x=x1​,x2​,...,xn​。 P(x)P(\\mathbf{x})P(x) 通常由链式法则计算： P(x)=P(x1)×...P(xn∣x1...xn−1)P(\\mathbf{x}) = P(x_1)\\times ... P(x_n|x_1...x_{n - 1})P(x)=P(x1​)×...P(xn​∣x1​...xn−1​) L2R LM 由马尔可夫在1913年提出并被广泛使用，具有代表性的现代L2R LM有：GPT-3, GPT-Neo Masked Language Models, Mask L2R LM 是建模文本概率的利器，但是它要求必须按照从左到右的顺序来计算表示。这种顺序的表示对于某些下游任务（如分类）可能不是最合适的。 在 representation learning 中广泛使用的一种双向目标函数Masked Language Model用于根据周围的文本预测被Masked的文本片段。即： P(xi∣x1,...xi−1,xi+1,...,xn)P(x_i|x_1,...x_{i - 1}, x_{i+1}, ... ,x_n)P(xi​∣x1​,...xi−1​,xi+1​,...,xn​) 代表词 xix_ixi​ 被给定上下文包围的概率 使用MLM的代表性预训练模型有：BERT， ERNIE。MLM通常非常适合如 文本分类 等自然语言理解和分析的任务。 Prefix and Encoder-Decoder 对于条件文本生成任务例如翻译和文本摘要任务，即给定输入文本 x=x1,..,xn\\mathbf{x} = x_1,..,x_nx=x1​,..,xn​ ，要求生成目标文本 y\\mathbf{y}y，需要一个既能够编码输入文本又能够解码输出文本的模型。 对此有两种流行的架构，它们都是下面的流程：1. 使用具有完全连接掩码的编码器首先对源 x\\mathbf{x}x 进行编码，然后2. 自动回归从左到右解码 y\\mathbf{y}y Prefix Language Model: PLM是一个从左到右的LM，它以前缀序列 x\\mathbf{x}x 为条件解码 y\\mathbf{y}y。编码和解码的模型参数相同 代表性模型：UniLM 1-2, ERNIE-M Encoder-decoder: 使用 L2R LM来解码 y\\mathbf{y}y ，但是编码 x\\mathbf{x}x 的编码器是独立的 代表性模型：BART, MASS 常用于文本生成任务，在恰当的 prompts 下也可以用于信息提取，问答和文本生成评估任务 下面是几种LM的结构示意图和不同函数的的组合表： # Prompt Engineering Prompt Engineering 就是为下游任务构建最佳prompt函数 fprompt(x)f_{prompt}(\\mathbf{x})fprompt​(x) 的过程，需要考虑两个关键点： prompt shape 和 manual/automated 在现有的工作中，Prompt Engineering是一个寻找最佳prompt template的过程，即prompt template engineering ## Prompt Shape cloze prompts: 更适合于使用Masked LM解决的任务 prefix prompts: 生成任务，或使用标准自回归LM解决的任务更适合 ## Manual Template Engineering 根据人类的知识和经验手动创建直观的模板，代表性工作： cloze prompt: Language Models as Knowledge Bases? prefix prompt: Language Models are Few-Shot Learners ## Automated Template Learning 手动创建的模板具有一定的准确性，但是：1. 需要一定的经验和时间，2. 有经验的设计者也无法保证设计出最佳的prompts 针对这些问题，提出了自动的模板设计方法，可以分为 discrete prompts 和 continuous prompts ### Discrete Prompts 离散的prompts，又名hard prompts，这类prompts是一串实际的文本。它的自动设计过程就是在一个离散空间（通常是自然语言短语）中搜索最佳的模板。 下面是几种具体的方法： Prompt Mining: 挖掘方法基于一组给定的输入 xxx 和输出 yyy 自动寻找最佳的模板，该方法为包含 xxx 和 yyy 的字符串抓取一个大型语料库，然后寻找输入和输出之间的中间词或依赖路径，频繁出现的中间词或依赖路径可以作为模板。 How Can We Know What Language Models Know? Prompt Paraphrasing: 通过一个现有的种子prompt来变换出一组候选prompts，然后选择其中获得目标任务最高训练准确性的。 paraphrasing 有很多种具体方法，例如： 翻译成别的语言再翻译回来 How Can We Know What Language Models Know? 使用同义词替换部分词 BARTScore: Evaluating Generated Text as Text Generation 使用专门优化过的神经网络重写prompts BERTese: Learning to Speak to BERT 还可以在输入文本 xxx 放进种子模板后再执行paraphrasing，这样可以为每个输入生成一个不同的paraphrase： BERTese: Learning to Speak to BERT Gradient-based Search:对一个prompt中的所有token进行梯度迭代搜索。 Universal Adversarial Triggers for Attacking and Analyzing NLP AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts Prompt Generation : 把生成prompts的任务看作是一个文本生成任务，使用 标准自然语言生成模型 进行生成 Making Pre-trained Language Models Better Few-shot Learners PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains Prompt Scoring : Commonsense Knowledge Mining from Pretrained Models ### Continuous Prompts prompts没有必要限制在自然语言的范围内，连续的prompts可以直接存在于模型的嵌入空间内，这类prompts被称为continuous prompts或soft prompts。具体来说，连续的prompts去除了两条限制：1. prompts的内容必须是自然语言， 2. 模板必须被预训练模型的参数参数化。模板有自己的可训练参数可以使用下游任务的数据训练。 下面是几种具体的方法： Prefix Tuning : 保持LM的参数不变，将一系列与任务强相关的向量添加到输入中。数学上来讲，这包括在给定可训练的前缀矩阵 MϕM_{\\phi}Mϕ​ 和参数为 θ\\thetaθ 的LM的情况下优化以下类对数目标： maxϕlogP(y∣x;θ;ϕ)=maxϕΣyilogP(yi∣h&lt;i;θ;ϕ)max_{\\phi}log P(\\mathbf{y}|\\mathbf{x}; \\theta; \\phi) = max_{\\phi}\\Sigma_{y_i}log P(y_i|h_{&lt;i}; \\theta; \\phi) maxϕ​logP(y∣x;θ;ϕ)=maxϕ​Σyi​​logP(yi​∣h&lt;i​;θ;ϕ) Prefix-Tuning: Optimizing Continuous Prompts for Generation The Power of Scale for Parameter-Efficient Prompt Tuning Tuning Initialized with Discrete Prompts : 使用离散方法生成的prompts来初始化搜索。 使用离散方法生成虚拟token: Factual Probing Is [MASK]: Learning vs. Learning to Recall 使用离散方法生成初始模板集： Learning How to Ask: Querying LMs with Mixtures of Soft Prompts 根据手动构建模板的结构生成连续模板： WARP: Word-level Adversarial ReProgramming Hard-Soft Prompt Hybrid Tuning : 混合的方法，在固定的hard prompt template中插入以下可变参数。除此之外： 在模板中插入任务相关的锚记： 使用人工构建的子模板来构建模板，实现在逻辑规则下构建模板： PTR: Prompt Tuning with Rules for Text Classification # Answer Engineering Answer Engineering用于搜索答案空间 Z\\mathcal{Z}Z ，并将答案映射为输出 Y\\mathcal{Y}Y。主要需要考虑的是 answer shape 和 answer design method ## Answer Shape Answer Shape即答案的粒度，主要有三种： Tokens : 预训练语言模型的词汇表中的一个词或一个子集 Span : A short multi-token span, 通常和cloze prompts共同使用 Sentence : 一个句子或文档，通常和prefix prompts共同使用 Answer Shape的选择通常跟任务相关， Tokens 和 Span 常用于分类任务或关系抽取，命名实体识别任务。更长的 Sentence 常用于语言生成任务或多选择问答任务。 ## Answer Space Design Methods 即设计合适的答案空间 Z\\mathcal{Z}Z 以及合适的映射到输出 Y\\mathcal{Y}Y 的方法 ### Manual Design Unconstrained Spaces : 无约束空间。在许多情况下， Z\\mathcal{Z}Z 是一个所有token，固定长度的span或token 序列的空间 Constrained Spaces : 约束空间。文本分类，实体识别等任务下可能的输出被限制在一个范围内 ### Auto Design Discrete Answer Search Answer Paraphrasing : 给定一个初始答案空间 Z′\\mathcal{Z}&#x27;Z′ ，然后利用paraphrasing方法来扩展这个空间 How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering Prune-then-Search : Making Pre-trained Language Models Better Few-shot Learners Label Decomposition : AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction Continuous Answer Search WARP: Word-level Adversarial ReProgramming # Multi-Prompt Learning 在某些问题中使用多个Prompt能够提高有效性。 ## Prompt-ensembling 推理时对于同一个输入使用多个未解答的prompts来进行预测，这种综合策略可以： 利用不同提示之间的互补 因为选择一个效果最好的提示非常有挑战性，这样可以降低 prompt engineering 的成本 稳定下游任务的性能表现 目前主要的几个方向： Uniform averaging : 取不同提示的概率平均值，是最为直观的方法。具体来说，概率是： P(z∣x):=1KΣiKP(z∣fprompt,i(x))P(\\mathbb{z}|\\mathbb{x}):=\\frac{1}{K}\\Sigma_i^KP(\\mathbb{z}|f_{prompt, i}(\\mathbb{x}))P(z∣x):=K1​ΣiK​P(z∣fprompt,i​(x))，其中 fprompt,i(⋅)f_{prompt, i}(\\cdot)fprompt,i​(⋅) 是第 iii 个prompt Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference BARTScore: Evaluating Generated Text as Text Generation Weighted averaging : 平均的方法可能是次优的，有一些研究使用加权平均的prompt ensembling Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference Majority voting : 对于分类任务，可以结合投票的方法 The Power of Scale for Parameter-Efficient Prompt Tuning WARP: Word-level Adversarial ReProgramming Knowledge distillation （知识蒸馏）: 使用训练好的模型标记数据，用于训练另一个模型(蒸馏知识) Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners Prompt ensembling for text generation : 这方面的工作较少，一个简单的方法是使用标准方法基于下一个词概率来生成输出： P(zt∣x,z&lt;t):=1KΣiKP(zt∣fprompt,i(x),z&lt;t)P(z_t|\\mathbb{x}, z_{&lt;t}):=\\frac{1}{K}\\Sigma_i^KP(z_t|f_{prompt, i(\\mathbb{x})}, z_{&lt;t})P(zt​∣x,z&lt;t​):=K1​ΣiK​P(zt​∣fprompt,i(x)​,z&lt;t​) Few-Shot Text Generation with Pattern-Exploiting Training ## Prompt Augmentation 提示增强，有时被称为示范学习(demonstration learning)，即在输入中提供几种已经回答的提示(answered prompts)来向LM示范如何为输入 x\\mathbb{x}x 生成输出。这种少量的示范利用了强大的语言模型学习重复模式的能力。 prompt augmentation的思想很简单，但是有两个方面需要探索：1. Sample Selection 2. Sample Prdering Sample Selection : 根据示范用例选择的不同，模型的表现可能是乱猜和state-of-art的区别。 Making Pre-trained Language Models Better Few-shot Learners What Makes Good In-Context Examples for GPT-333? Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions Sample Ordering : 提示用例的顺序对模型的表现也十分重要 Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity Reordering Examples Helps during Priming-based Few-Shot Learning ## Prompt Composition 对于可组合的任务，即可以由更基本的子任务组合而成的任务，可以使用提示重组。即使用多个子提示，每个子提示针对一个子任务，然后基于子提示定义一个组合提示。例如对于关系提取任务，可以重组为识别实体特征任务和关系分类任务。 PTR: Prompt Tuning with Rules for Text Classification ## Prompt Decomposition 对于如序列标记的任务，需要对同一个样本做多次预测。此时较难给输入直接定义一个整体的提示。一个可行的方法是将整体分解为不同的子提示，然后分别回答每个子提示。如在NER任务中： Template-Based Named Entity Recognition Using BART 上面提到的四种多提示方法示意图如下： # 训练策略 ## Training Settings 许多情况下，提示方法可以在没有额外训练的条件下使用，这种情况称为零样本 (zero-shot)。 除 zero-shot 外，还有 full-data learning 和 few-shot learning 。特别是在 few-shot learning 中，prompt方法十分适用 ## 参数更新方法 在基于提示的下游任务学习中，通常有两种参数：预训练模型的参数和提示参数。根据 1. 预训练模型的参数是否更新，2. 是否引入提示参数和3. 提示参数是否更新，可以把参数参数更新方法分为以下五类： **Promptless Fine-tuning：**不使用prompt的模式，仅更新语言模型的参数。 优点：简单，不需要设计提示，更容易适配较大的训练集 模型可能会过拟合或者在小型数据集上不够稳定。可能发生灾难性遗忘(catastrophic forgetting)，即微调后的模型无法做到微调前可以做到的事 Tuning-free Prompting: 即不进行额外训练，直接利用预训练模型配合prompt实现功能 优点：高效，没有参数更新的过程，不会发生灾难性遗忘，适合零样本场景 缺点：prompt是提供给模型的唯一与任务相关的信息，因此需要繁重的prompt engineering来保证准确性 Fixed-LM Prompt Tuning: 引入额外的prompt-relevant 参数，然后在训练时仅更新该参数，保持预训练模型不变 优点：可以维持预训练模型中的知识，不会发生灾难性遗忘，更适合小样本场景。通常精确性会优于tuning-free prompting 缺点：不适合零样本场景，在大量数据的场景下受表征能力的限制。需要进行prompting engineering。prompt通常不是人类可读的 Fixed-prompt LM Tuning: 即在promptless fine-tuning模式下增加固定的prompt参数，在小样本场景下可能带来提升 优点：prompt/answer engineering更加全面地描述特定任务，可以进行更有效的学习 缺点：仍然需要prompt/answer engineering，在一个下游任务上的微调可能对另一个任务无效 Prompt+LM Tuning: 既引入了prompt-relevant参数，又同时更新语言模型和引入的参数（部分或全部的） 优点：表征能力最强的方法，适合多样本场景 缺点：需要训练和储存所有的参数。在小数据集上容易过拟合","categories":[{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"}],"tags":[{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"Prompt","slug":"Prompt","permalink":"https://blog.zhuwenq.cc/tags/Prompt/"},{"name":"Survey","slug":"Survey","permalink":"https://blog.zhuwenq.cc/tags/Survey/"}]},{"title":"Ubuntu(WSL2) 编译安装Qt-5.15.2","slug":"Ubuntu-WSL2-编译安装Qt-5-15-2","date":"2021-08-02T07:24:43.000Z","updated":"2023-06-21T06:51:28.932Z","comments":true,"path":"Ubuntu-WSL2-编译安装Qt-5-15-2/","link":"","permalink":"https://blog.zhuwenq.cc/Ubuntu-WSL2-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Qt-5-15-2/","excerpt":"以往都是通过离线安装包或者在线安装器安装Qt，自动Qt不提供离线安装包以后，在某些无法联网的场景下只能通过源码编译安装。本次在 WSL 的环境下虽然可以联网，但权当一次体验编译安装的机会。","text":"以往都是通过离线安装包或者在线安装器安装Qt，自动Qt不提供离线安装包以后，在某些无法联网的场景下只能通过源码编译安装。本次在 WSL 的环境下虽然可以联网，但权当一次体验编译安装的机会。 首先从qt的IO网站或者清华镜像站或者其他任何托管有qt源码的镜像站下载源代码（以5.15.2为例）。然后需要准备编译环境，这里建议参考Qt的编译指导来安装编译所需要的环境。 安装完成后，就可以开始配置构建选项。这里建议在源代码的平行目录中进行，即假设源代码的目录是：~/qt-src，那么建议在类似 ~/qt-build 的目录中进行。在构建目录中，运行 ./configure 脚本可以进行编译前的配置，configure 脚本有很多可选的参数，具体的列表可以打开源码目录下的 qtbase/config_help.txt 查看。 configure 脚本执行完毕后，如果脚本的输出没有提示环境的不支持，就可以执行 make 开始编译。这里因该需要编译一天左右，我的机器上就从下午三点半编译到第二天的十二点四十，大约21小时。 编译完成后就可以执行 make install 进行安装，默认的安装目录是 /usr/local/Qt-5.15.2。此时执行 qmake 是找不到目标的，需要手动添加到 PATH 或者使用 qtchooser 工具。以 qtchooser 为例，它所默认的qt路径应该是 /usr/lib/x86_64-linux-gnu/qt5。因此需要更改它的配置文件，打开文件 /usr/lib/x86_64-linux-gnu/qt-default/qtchooser/default.conf，将内容改成： 12/usr/local/Qt-5.15.2/bin/usr/local/Qt-5.15.2/lib 就完成了。","categories":[{"name":"记录备忘","slug":"记录备忘","permalink":"https://blog.zhuwenq.cc/categories/%E8%AE%B0%E5%BD%95%E5%A4%87%E5%BF%98/"}],"tags":[{"name":"Qt","slug":"Qt","permalink":"https://blog.zhuwenq.cc/tags/Qt/"}]},{"title":"IEEE754浮点数","slug":"IEEE754浮点数","date":"2021-07-31T12:10:57.000Z","updated":"2023-06-21T06:51:28.792Z","comments":true,"path":"IEEE754浮点数/","link":"","permalink":"https://blog.zhuwenq.cc/IEEE754%E6%B5%AE%E7%82%B9%E6%95%B0/","excerpt":"二进制小数 考虑十进制下的小数表示：[dmdm−1…d1d0⋅d−1d−2…d−n][d_m d_{m−1}…d_1 d_0\\cdot d_{−1} d_{−2}…d_{−n}][dm​dm−1​…d1​d0​⋅d−1​d−2​…d−n​]，其中每个十进制数 did_idi​ 取值范围是 0 90~90 9。这个表达式所描述的数值 ddd 的定义为：d=Σi=−nm10i×did= \\Sigma^m_{i = -n}10^i\\times d_id=Σi=−nm​10i×di​ 。数字权的定义与小数点相关，在小数点的左侧，数字的权值是10的正幂，得到整数值。而小数点的右侧数字权值是10的负幂，得到小数值。如 12.341012.34_{10}12.3410​ 表示数字 1×101+2×100+3×10−1+4×10−2=12341001 × 10^1 + 2 × 10^0 + 3 × 10^{−1} + 4 × 10^{−2} = 12\\frac{34}{100}1×101+2×100+3×10−1+4×10−2=1210034​ 。 类似地，考虑一个形如 [bmbm−1…b1b0⋅b−1b−2…b−n][b_m b_{m−1}…b_1 b_0\\cdot b_{−1} b_{−2}…b_{−n}][bm​bm−1​…b1​b0​⋅b−1​b−2​…b−n​] 的表达式，其中的每个二进制数字 bib_ibi​ 的取值范围是 000 和 111，这种表示方法表示的数定义为：b=Σi=−nm2i×bib=\\Sigma_{i=−n}^m 2^i × b_ib=Σi=−nm​2i×bi​ 。同样的，小数点的左侧数字权值为 222 的正幂，右侧数字权值为 222 的负幂。如 101.112101.11_2101.112​ 表示数字 1×22+0×21+1×20+1×2−1+1×2−2=5341 × 2^2 + 0 × 2^1 + 1 × 2^0 + 1 × 2^{−1} + 1 × 2^{−2} = 5\\frac{3}{4}1×22+0×21+1×20+1×2−1+1×2−2=543​ 。这种表示法称为浮点数的定点表示法。 如果仅考虑有限长度的编码，那么上述十进制小数无法准确表示如 1/3,5/71/3,5/71/3,5/7 这样的数。类似地，小数的二进制表示法也智能表示具有 x×2yx×2^yx×2y 形式的数，其他的值只能近似表示。","text":"二进制小数 考虑十进制下的小数表示：[dmdm−1…d1d0⋅d−1d−2…d−n][d_m d_{m−1}…d_1 d_0\\cdot d_{−1} d_{−2}…d_{−n}][dm​dm−1​…d1​d0​⋅d−1​d−2​…d−n​]，其中每个十进制数 did_idi​ 取值范围是 0 90~90 9。这个表达式所描述的数值 ddd 的定义为：d=Σi=−nm10i×did= \\Sigma^m_{i = -n}10^i\\times d_id=Σi=−nm​10i×di​ 。数字权的定义与小数点相关，在小数点的左侧，数字的权值是10的正幂，得到整数值。而小数点的右侧数字权值是10的负幂，得到小数值。如 12.341012.34_{10}12.3410​ 表示数字 1×101+2×100+3×10−1+4×10−2=12341001 × 10^1 + 2 × 10^0 + 3 × 10^{−1} + 4 × 10^{−2} = 12\\frac{34}{100}1×101+2×100+3×10−1+4×10−2=1210034​ 。 类似地，考虑一个形如 [bmbm−1…b1b0⋅b−1b−2…b−n][b_m b_{m−1}…b_1 b_0\\cdot b_{−1} b_{−2}…b_{−n}][bm​bm−1​…b1​b0​⋅b−1​b−2​…b−n​] 的表达式，其中的每个二进制数字 bib_ibi​ 的取值范围是 000 和 111，这种表示方法表示的数定义为：b=Σi=−nm2i×bib=\\Sigma_{i=−n}^m 2^i × b_ib=Σi=−nm​2i×bi​ 。同样的，小数点的左侧数字权值为 222 的正幂，右侧数字权值为 222 的负幂。如 101.112101.11_2101.112​ 表示数字 1×22+0×21+1×20+1×2−1+1×2−2=5341 × 2^2 + 0 × 2^1 + 1 × 2^0 + 1 × 2^{−1} + 1 × 2^{−2} = 5\\frac{3}{4}1×22+0×21+1×20+1×2−1+1×2−2=543​ 。这种表示法称为浮点数的定点表示法。 如果仅考虑有限长度的编码，那么上述十进制小数无法准确表示如 1/3,5/71/3,5/71/3,5/7 这样的数。类似地，小数的二进制表示法也智能表示具有 x×2yx×2^yx×2y 形式的数，其他的值只能近似表示。 IEEE浮点表示 定点表示法无法有效表示非常大的数字。例如表达式 5×21005×2^1005×2100 是用 101101101 后面跟随 100100100 个零的位模式来表示。 IEEE浮点标准用 V=(−1)s×M×2EV=(−1)^s×M×2^EV=(−1)s×M×2E 的形式来表示一个数： 符号(sign)s决定这个数是正数 (s=0)(s=0)(s=0) 还是负数 (s=1)(s=1)(s=1)，数值为000时符号位的解释作为特殊情况处理 尾数(significand)M是一个二进制小数，它的范围是 1∼2−ϵ1\\thicksim 2−ϵ1∼2−ϵ，或是 01∼ϵ01\\thicksim ϵ01∼ϵ 阶码(exponent)E的作用是对浮点数加权，这个权重是 222 的 EEE (可能是负数)次幂 将浮点数的位表示划分为三个字段，分别对这些值进行编码： 一个单独的符号位 s 直接编码符号 s k位的阶码字段 exp=ek−1…e1e0exp=e_{k−1}…e_1 e_0exp=ek−1​…e1​e0​ 编码阶码 E n位小数字段 frac=fn−1…f1f0frac=f_{n−1}…f_1 f_0frac=fn−1​…f1​f0​ 编码尾数 M，该字段的解释依赖于阶码字段的值是否为0 下图所示是这三个字段在字中的两种最常见格式，对于单精度浮点数，s，exp 和 frac字段分别为 1，8，23 位；对于双精度浮点数，s，exp 和 frac 字段分别为 1，11，52 位。 对于一个给定的浮点数位表示，对其的解释根据阶码字段的值可以分成三种情况(最后一种情况有两个变种)： 情况1：规格化的值 这是最普遍的情况。当 exp的位模式既不全为零(数值0)，也不全为1 (单精度情况下数值为255，双精度数值为2047)时，属于这种情况。 此时阶码字段被解释为以 偏置(biased) 形式的有符号整数，即：E=e−BiasE = e - BiasE=e−Bias，其中 eee 是阶码字段编码的无符号数，BiasBiasBias 是一个等于 2k−1−12^{k−1}−12k−1−1(单精度是 127127127，双精度是 102310231023 )的偏置值 。此时产生的指数范围，对于单精度是 −126∼+127−126\\thicksim +127−126∼+127，对于双精度是 −1022∼+1023−1022\\thicksim +1023−1022∼+1023。 小数字段 fracfracfrac 被解释为小数值 fff，其中 0≤f&lt;10≤f&lt;10≤f&lt;1，其二进制表示为：$ 0.f_{n−1}…f_1 f_0$ ，即所有位的权都是2的负幂。尾数 MMM 定义为 M=1+fM=1+fM=1+f 。这种方式叫做隐含的以1开头的(implied leading 1)表示，可以将位数的位表示看作 1.fn−1…f1f01.f_{n−1}…f_1 f_01.fn−1​…f1​f0​。由于总是可以通过调整阶码 EEE，使得尾数 MMM 保持在范围 1≤M&lt;21≤M&lt;21≤M&lt;2 中，那么这种方法可以无代价的获得一个额外的精度位。 情况2：非规格化的值 当阶码域为全0时，所表示的数是非规格化的值。此时阶码值是E=1−BiasE = 1-BiasE=1−Bias，尾数的值是M=fM = fM=f，不包含隐含开头的111 非规格化的表示有两个用途： 它提供了一种表示000的方法，在规格化的情况下，MMM总是大于等于111的，因此无法表示000。实际上，+0.0+0.0+0.0的位模式为全000，符号位，阶码和小数域全为000；而当符号位为111，阶码和小数域为000时，该值被解释为−0.0-0.0−0.0 非规格化数的第二个功能是表示非常接近0.00.00.0的数。它提供了一种称为逐渐下溢(gradual underflow)的特性，让数值的分布均匀地接近0.00.00.0 Note: 使阶码值为1-Bias而不是简单的-Bias是因为这样可以补偿尾数值由M=f+1变为M=f后造成的数值突变，使数值从非规格数到规格数能够更加平滑的变化 情况3：特殊值 最后一类数值是当阶码全为1时出现的。 当小数域全为 000 时，得到的值表示无穷，当 s=0s=0s=0 时表示正无穷，或者当 s=1s=1s=1 时是负无穷。当把两个非常大的数相乘，或者除以 000 时，无穷能够表示溢出的结果。 当小数域非0时，结果被称为&quot;NaN&quot;，即&quot;不是一个数&quot;。如 −1\\sqrt{-1}−1​ 或 ]∞−∞]\\infin−\\infin]∞−∞的结果是NaN。 Note: 假如将从0到最大值排列的一组无符号数的位模式按照相同字长的浮点数解释，则浮点数结果也是递增的。IEEE格式设计点之一就是为了浮点数能够直接使用整数的排序函数。 对于有k位阶码和n位小数域的浮点数表示，其具有以下属性： 描述 expexpexp fracfracfrac 值（单精度） 十进制（单精度） 值（双精度） 十进制（双精度） 000 00…0000…0000…00 0…000…000…00 000 0.00.00.0 000 0.00.00.0 最小非规格化正数 00…0000…0000…00 0…010…010…01 2−23×2−1262^{−23}×2^{−126}2−23×2−126 1.4×10−451.4×10^{−45}1.4×10−45 2−52×2−10222^{−52}×2^{−1022}2−52×2−1022 4.9×10−3244.9×10^{−324}4.9×10−324 最大非规格化数 00…0000…0000…00 1…111…111…11 (1−ϵ)×2−126(1−ϵ)×2^{−126}(1−ϵ)×2−126 1.2×10−381.2×10^{−38}1.2×10−38 (1−ϵ)×2−1022(1−ϵ)×2^{−1022}(1−ϵ)×2−1022 2.2×10−3082.2×10^{−308}2.2×10−308 最小规格化数 00…0100…0100…01 0…000…000…00 1×2−1261×2^{−126}1×2−126 1.2×10−381.2×10^{−38}1.2×10−38 1×2−10221×2^{−1022}1×2−1022 2.2×10−3082.2×10^{−308}2.2×10−308 111 01…1101…1101…11 0…000…000…00 1×201×2^01×20 1.01.01.0 1×201×2^01×20 1.01.01.0 最大规格化数 11…1011…1011…10 1…111…111…11 (2−ϵ)×2127(2−ϵ)×2^{127}(2−ϵ)×2127 3.4×10383.4×10^{38}3.4×1038 (2−ϵ)×21023(2−ϵ)×2^{1023}(2−ϵ)×21023 1.8×103081.8×10^{308}1.8×10308 值+0.0+0.0+0.0 总有一个全0的表示 最小的正非规格化的值的位表示，是由最低有效位为1而其他所有位为000构成的。它具有小数(此时即尾数)值M=f=2−nM=f=2^{−n}M=f=2−n 和阶码值E=−2k−1+2E=−2^{k−1}+2E=−2k−1+2。因此该表示的数值为V=2−n−2k−1+2V=2^{−n−2^{k−1}+2}V=2−n−2k−1+2 最大的非规格化值的位模式是由全为000的阶码字段和全为111的小数字段组成的。它具有小数（此时即尾数）值M=f=1−2−nM=f=1−2^{−n}M=f=1−2−n (写成1−ϵ1−ϵ1−ϵ)和阶码值E=−2k−1+2E=−2^{k−1}+2E=−2k−1+2，因此数值V=(1−2−n)×2−2k−1+2V=(1−2^{−n})×2^{−2^{k−1}+2}V=(1−2−n)×2−2k−1+2 最小的正规格化值位模式的阶码字段最低有效位为111，其他位全为000。它的尾数值M=1M=1M=1，而阶码值E=−2k−1+2E=−2^{k−1}+2E=−2k−1+2。因此数值V=2−2k−1+2V=2^{−2^{k−1}+2}V=2−2k−1+2 。值得注意的是，这之比最大的非规格化值大一点。 值1.01.01.0 的位表示阶码字段除了最高有效位为000之外，所有其他位都为111。它的尾数M=1M=1M=1，它的阶码值E=0E=0E=0 最大的规格化值的位表示的符号位为000，阶码的最低有效位为000，其他位均为111。它的小数值f=1−2−nf=1−2^{−n}f=1−2−n，尾数值M=2−2−nM=2−2^{−n}M=2−2−n (写作2−ϵ2−ϵ2−ϵ)。阶码值E=2k−1−1E=2^{k−1}−1E=2k−1−1，得到数值V=(2−2−n)×22k−1−1V=(2−2^{−n} )×2^{2^{k−1}−1}V=(2−2−n)×22k−1−1 舍入 对于无法准确表示的值，浮点数采用 向偶数舍入(round to even) 的规则来获得一个近似值。 向偶数舍入也称向 最近值舍入(round to nearest) ，它试图找到一个最接近的匹配值。例如，它将 1.41.41.4 舍入成111，而将1.61.61.6舍入成222，因为它们是最接近的值。对于处于两个可能值正中间的值，向偶数舍入采取的策略是使得结果的最低有效数字是偶数。因此这种方法将1.51.51.5和2.52.52.5都舍入成222。 浮点数采用这种舍入规则的原因是，对于一组数据而言，如果都采用向上舍入或向下舍入，会不可避免的带来统计偏差，而向偶数舍入可以在大多数情况下避免这种偏差。 对于二进制小数来说，将最低有效位的值000视为偶数，值111视为奇数。对于处于两种可能值正中间的值来说，舍入后的结果要保证最低有效位为000。如10.111002(278)10.11100_2 (2 \\frac{7}{8})10.111002​(287​)被舍入为11.002(3)11.00_2 (3)11.002​(3) ，10.101002(258)10.10100_2 (2 \\frac{5}{8})10.101002​(285​)被舍入为10.102(212)10.10_2 (2 \\frac{1}{2})10.102​(221​)。 浮点运算 IEEE规定的浮点数运算中，把浮点数xxx和yyy看作是实数，浮点数x,yx,yx,y运算的结果实际上是实数x,yx,yx,y精确运算后舍入的结果。 考虑到舍入的影响，可以将浮点数加法x+fyx+^f yx+fy定义为Round(x+y)Round(x+y)Round(x+y)，即对实数结果舍入。对于所有的浮点数，该运算是可交换的，即x+fy=y+fxx+^f y=y+^f xx+fy=y+fx，另一方面，该运算是不可结合的，即x+f(y+fz)≠(x+fy)+fzx+^f (y+^f z)\\neq (x+^f y) +^f zx+f(y+fz)=(x+fy)+fz。例如在单精度浮点数中，(3.14+1e10)−1e10=0.0(3.14+1e10)−1e10=0.0(3.14+1e10)−1e10=0.0，这是由于舍入，在1e101e101e10的大小尺度下，3.143.143.14被舍去了，同时3.14+(1e10−1e10)=3.143.14+(1e10−1e10)=3.143.14+(1e10−1e10)=3.14 对于乘法浮点数也有类似的属性：浮点数乘法是可交换的，即x×fy=y×fxx×^f y=y×^f xx×fy=y×fx。但同时是不可结合的，例如在单精度条件下：(1e20∗1e20)∗1e−20=+∞(1e20∗1e20)∗1e−20=+∞(1e20∗1e20)∗1e−20=+∞，而1e20∗(1e20∗1e−20)=1e201e20∗(1e20∗1e−20)=1e201e20∗(1e20∗1e−20)=1e20。另外，浮点数乘法不具分配性，例如单精度条件下：1e20∗(1e20−1e20)=0.01e20∗(1e20−1e20)=0.01e20∗(1e20−1e20)=0.0，而1e20∗1e20−1e20∗1e20=NaN1e20∗1e20−1e20∗1e20=NaN1e20∗1e20−1e20∗1e20=NaN。 浮点数运算还具有如下的性质，对于任意a,b,c≠NaNa, b ,c\\neq NaNa,b,c=NaN: {a⩾b,c⩾0⇒a×fc⩾b×fca⩾b,c⩽0⇒a×fc⩽b×fca×fa⩾0\\begin{cases} a \\geqslant b, c \\geqslant 0 \\Rightarrow a \\times^f c \\geqslant b \\times^f c \\\\ a \\geqslant b, c \\leqslant 0 \\Rightarrow a \\times^f c \\leqslant b \\times^f c \\\\ a \\times^f a \\geqslant 0 \\end{cases}⎩⎪⎪⎨⎪⎪⎧​a⩾b,c⩾0⇒a×fc⩾b×fca⩾b,c⩽0⇒a×fc⩽b×fca×fa⩾0​ Note: C语言标准中并未要求机器使用IEEE浮点数","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"浮点数","slug":"浮点数","permalink":"https://blog.zhuwenq.cc/tags/%E6%B5%AE%E7%82%B9%E6%95%B0/"},{"name":"位级表示","slug":"位级表示","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E7%BA%A7%E8%A1%A8%E7%A4%BA/"}]},{"title":"CS:APP Lab1: DataLab","slug":"CS-APP-Lab1-DataLab","date":"2021-07-28T13:06:15.000Z","updated":"2023-06-21T06:51:28.784Z","comments":true,"path":"CS-APP-Lab1-DataLab/","link":"","permalink":"https://blog.zhuwenq.cc/CS-APP-Lab1-DataLab/","excerpt":"0x00 简介 一直在学习CS:APP这本书，本以为大名鼎鼎的Lab会出现在书本上，没想到一直看到第三章也没看见。在网上查了以下才知道是在线的。想想也十分合理，毕竟可能会经常有更新。这里就记录完成第一个Lab——DataLab的过程。 DataLab是使用受限的C语言运算子集来实现逻辑运算/补码运算/浮点运算的函数，例如，可能会要求仅使用位级运算和线性的过程来实现求绝对值的函数。这个Lab可以帮助理解C数据类型的位级表示和数据操作的位级行为。","text":"0x00 简介 一直在学习CS:APP这本书，本以为大名鼎鼎的Lab会出现在书本上，没想到一直看到第三章也没看见。在网上查了以下才知道是在线的。想想也十分合理，毕竟可能会经常有更新。这里就记录完成第一个Lab——DataLab的过程。 DataLab是使用受限的C语言运算子集来实现逻辑运算/补码运算/浮点运算的函数，例如，可能会要求仅使用位级运算和线性的过程来实现求绝对值的函数。这个Lab可以帮助理解C数据类型的位级表示和数据操作的位级行为。 要求 这个Lab主要是在一个C语言文件中实现函数，在实现时对于使用的运算种类和数量都有严格的要求。同时Lab提供了一组工具用于检查代码中使用的运算是否符合要求以及用于测试代码正确与否。还提供了一个小工具可以打印整形数据和浮点数据的16进制或10禁止表示。 具体的编码要求附在文末，同时每道题可能还有自己的更加严格的要求。 概览 总共有不同分值的整数和浮点数函数共13个： 函数 描述 分值 bitXor 计算异或 1 tmin 返回最小补码值 1 isTmax 判断是否最大补码值 2 allOddBits 判断是否奇数位置都为1 2 negate 求相反数 2 isAsciiDigit 判断是否Ascii数字 3 conditional 实现条件分支 3 isLessOrEqual 实现小于等于 3 logicalNeg 实现逻辑非(!) 4 howManyBits 计算能表达数据的最小长度 4 floatScale2 实现位级浮点数*2 4 floatFloat2Int 实现位级float转int 4 floatPower2 实现位级2.0^x 4 0x01 bitXor 第一道题目： 12345678910/* * bitXor - x^y using only ~ and &amp; * Example: bitXor(4, 5) = 1 * Legal ops: ~ &amp; * Max ops: 14 * Rating: 1 */int bitXor(int x, int y) &#123; return 2;&#125; 只使用 ~ 和 &amp; 运算符实现 ^ 运算。最多可以使用14个运算符。 考虑位级异或运算的真值表： x y x ^ y 0 0 0 0 1 1 1 0 1 1 1 0 可以看到令 x ^ y = 1 的情况分别是 x = 0, y = 1 和 x = 1, y = 0，根据卡诺图化简的方法和逻辑代数的表达方式，这个真值表可以化简为：X'Y + XY'，即：x ^ y = (x &amp; ~y) | (~x &amp; y)。 由于题目要求只能使用 &amp; 和 ~，消去上式中的 |： 12345(x &amp; ~y) | (~x &amp; y)= ~~((x &amp; ~y) | (~x &amp; y))= ~((~x | y) &amp; (x | ~y))= ~(~~(~x | y) &amp; ~~(x | ~y))= ~(~(x &amp; ~y) &amp; ~(~x &amp; y)) 因此直接将上面的 return 2 改成 return ~(~(x &amp; ~y) &amp; ~(~x &amp; y)) 即可。 这道题目还可以在卡诺图化简时选择反函数，即化简出 ~(x ^ y) 的表达式。具体来说，是选择真值表中结果为0的项进行化简：~(x ^ y) = (x &amp; y) | (~x &amp; ~y)。这样的情况下，有：(x ^ y) = ~((x &amp; y) | (~x &amp; ~y)) = (~x | ~y) &amp; (x | y) = ~(x &amp; y) &amp; ~(~x &amp; ~y)。这个结果比上一个少用了1个运算符。 0x02 tmin 123456789/* * tmin - return minimum two&#x27;s complement integer * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 4 * Rating: 1 */int tmin(void) &#123; return 2;&#125; 这道非常简单，返回最小的补码值，即 0x80000000。由于不能使用除 0x00~0xff 之外的常数，直接返回 0x01 &lt;&lt; 31 即可。 0x03 isTmax 12345678910/* * isTmax - returns 1 if x is the maximum, two&#x27;s complement number, * and 0 otherwise * Legal ops: ! ~ &amp; ^ | + * Max ops: 10 * Rating: 1 */int isTmax(int x) &#123; return 2;&#125; 这道是判断参数是否是最大补码值，即 0x7fffffff。不难知道对于 0x7fffffff，其处在溢出的边缘，再加 1 就会得到最小的补码值 0x80000000。观察最小补码值和最大补码值的位模式，得知最小补码值只有最高位为 1，其余位为 0；而最大补码值相反——只有最高位为 0，其余位为 1——因此最小补码值取反应该为最大补码值。 基于以上原理，当 x = 0x7fffffff 时，!(x ^ ~(x + 1)) = 1。这就实现了最大补码值时返回 1。 实际测试时发现，当 x = 0xffffffff 时，由于也处于溢出边缘，也具有上述的性质，对结果造成误判。因此需要过滤掉该种情况。 当 x = 0xffffffff 时，其位模式是所有位都为 1。因此对其取反可以得到特殊值 0。可以使用表达式 ~x 进行过滤，这样的话总的结果就是 !(x ^ ~(x + 1)) &amp; ~x。对这个结果使用 x = 0x7fffffff 进行验证，发现 ~x = 0x80000000，这样的话最终结果就是 0x80000000 &amp; 0x1 = 0，因此还需要对 ~x 的值进行二值化，让其取值只落在 0 和 1 的范围内。即 !!~x。 最终结果为 !(x ^ ~(x + 1)) &amp; (!!~x)。 0x04 allOddBits 1234567891011/* * allOddBits - return 1 if all odd-numbered bits in word set to 1 * where bits are numbered from 0 (least significant) to 31 (most significant) * Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 2 */int allOddBits(int x) &#123; return 2;&#125; 这道题要求判断给定的参数其位模式的奇数位是否都为 1。例如，数据 0xFFFFFFFD 的最低四位是 1101，第1位为 0，所以返回 0。数据 0xAAAAAAAA 则是所有奇数位都为 1，所有偶数位都为 0，所以返回 1。 因为只需要判断奇数位上的情况，偶数位则可以忽略。因此可以使用数据 0xAAAAAAAA 作为 mask 来对参数进行校验。如果参数 x 的奇数位都为 1 的话，表达式 0xAAAAAAAA &amp; x = 0xAAAAAAAA 成立。则可以使用表达式 !(0xAAAAAAAA ^ (0xAAAAAAAA &amp; x)) 作为答案。因为不能直接使用这么大的常数，mask 需要从 0xAA 经过运算得到。以下表达式得到 mask = 0xAAAAAAAA: 123int mask = 0xAA;mask |= mask &lt;&lt; 8;mask |= mask &lt;&lt; 16; 所以答案是： 123456int allOddBits(int x) &#123; int mask = 0xAA; mask |= mask &lt;&lt; 8; mask |= mask &lt;&lt; 16; return !((x &amp; mask) ^ mask);&#125; 0x05 negate 12345678910/* * negate - return -x * Example: negate(1) = -1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 5 * Rating: 2 */int negate(int x) &#123; return 2;&#125; 这道题要求使用5个以内给定种类的运算符实现补码相反数操作。 对于补码整数，其编码与无符号数编码一致；对于补码负数，其编码方式则是最高位取负权，其余位取正权。 通过学习CS:APP第二章我们知道，补码的非有两种聪明的求法： 对每一位求补，再对结果加 1。在C语言中，对于任意整数 x，计算表达式 -x 和 ~x+1 得到的结果完全一样。 将位向量分为两部分，假设 kkk 是位向量中最右边的 111 的位置，因而 xxx 的位级表示形如 [x(w−1),x(w−2),…,x(k+1),1,0,…,0][x_{(w−1)}, x_{(w−2)}, …,x_{(k+1)},1,0,…,0][x(w−1)​,x(w−2)​,…,x(k+1)​,1,0,…,0]，这个值的非的二进制形式为：[ x(w−1), x(w−2),…, x(k+1),1,0,…,0][~x_{(w−1)},~x_{(w−2)},…,~x_{(k+1)},1,0,…,0][ x(w−1)​, x(w−2)​,…, x(k+1)​,1,0,…,0]。即：对位 kkk 左边的所有位取反。 这题中应用第一种求法，得到答案：~x + 1。 0x06 isAsciiDigit 123456789101112/* * isAsciiDigit - return 1 if 0x30 &lt;= x &lt;= 0x39 (ASCII codes for characters &#x27;0&#x27; to &#x27;9&#x27;) * Example: isAsciiDigit(0x35) = 1. * isAsciiDigit(0x3a) = 0. * isAsciiDigit(0x05) = 0. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 15 * Rating: 3 */int isAsciiDigit(int x) &#123; return 2;&#125; 这道题判断所给参数是否是 ASCII 数字，即判断给定参数是否在 0x30 ~ 0x39 范围内。 观察 0x30 ~ 0x39 这十个数字的位模式，发现其高28位都相同，低4位中涵盖了从 0 到 9 的范围。下面对照4位二进制数的真值表观察其特征： 位模式 整数值 位模式 整数值 0000 0 0101 5 0001 1 0110 6 0010 2 0111 7 0011 3 1000 8 0100 4 1001 9 可以看到范围内的值除了 8 和 9 之外最高位都为 0，且最高位为 0 时，低3位所有的取值情况都在范围内。对应到32位整数就是除了 0x38 和 0x39 之外，高29位都相同，且低3位可以为任意值。 我们将 0x38 和 0x39 这两个数字作为特例，除这两个值外，判断 x 高29位是否是合法值，如果是，那么该数字一定是 ASCII Code。如果不是，再判断是否是两个特例值中的一个。 判断高29位的表达式为：!(((x &gt;&gt; 3) &lt;&lt; 3) ^ 0x30)，判断 x 是否是特殊值的表达式是：!((x ^ 0x38) &amp; (x ^ 0x39))，最终结果为：!(((x &gt;&gt; 3) &lt;&lt; 3) ^ 0x30) | !((x ^ 0x38) &amp; (x ^ 0x39)) 0x07 conditional 12345678910/* * conditional - same as x ? y : z * Example: conditional(2,4,5) = 4 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 16 * Rating: 3 */int conditional(int x, int y, int z) &#123; return 2;&#125; 使用给定的几种不超过16个运算实现三元运算符，即：x ? y : z，若 x == 0，返回 z，否则返回 y。 在不能使用条件分支的情况下，根据 x 的取值进行返回，首先比较容易想到使用 x 作为 mask 对 y 和 z 进行过滤。比如当 x == 0 时，可以用表达式 x &amp; y 过滤掉 y；当 x != 0 时，根据相反的原则，首先将 x 做成所有位都为 1 的样子：~!!x + 1，然后表达式 x | z 可以过滤掉 z。 根据上面所说的情况，可以知道使用 x 的值作为 mask 的思路大概是可行的。不过要首先将 x 转换为全 0 或全 1 的形式——这里使用额外的变量表示——int isXZero = ~!!x + 1。对于两种过滤表达式，考察其取值： 由于： isXZero={0whenx=0−1whenx≠0\\mathrm{isXZero} = \\begin{cases} 0 &amp; when &amp; x = 0 \\\\ -1 &amp; when &amp; x \\neq 0 \\end{cases} isXZero={0−1​whenwhen​x=0x=0​ 有： isXZero &amp; y={0whenx=0ywhenx≠0\\mathrm{isXZero\\ \\&amp;\\ y} = \\begin{cases} 0 &amp; when &amp; x = 0 \\\\ y &amp; when &amp; x \\neq 0 \\end{cases}isXZero &amp; y={0y​whenwhen​x=0x=0​ 和： isXZero ∣ z={−1whenx≠0zwhenx=0\\mathrm{isXZero\\ |\\ z} = \\begin{cases} -1 &amp; when &amp; x \\neq 0 \\\\ z &amp; when &amp; x = 0 \\end{cases}isXZero ∣ z={−1z​whenwhen​x=0x=0​ 可以看到两种滤网滤出的数值根据 x 的取值分别取有效值和较为规则的无效值，且取有效值的情况彼此之间错开。这样的情况很容易想到将其相加： (isXZero &amp; y)+(isXZero ∣ z)={zwhenx=0y−1whenx≠0\\mathrm{(isXZero\\ \\&amp;\\ y) + (isXZero\\ |\\ z)} = \\begin{cases} z &amp; when &amp; x = 0 \\\\ y - 1 &amp; when &amp; x \\neq 0 \\end{cases}(isXZero &amp; y)+(isXZero ∣ z)={zy−1​whenwhen​x=0x=0​ 这样答案就较为明显了，只需在 x != 0 时将上述的和式加上 1 就可以。可以认为 x == 0 时上式加上 0，这样加上的实际上是 x 的某种变形，即 ~!!x。所以答案为：(isXZero | z) + (isXZero &amp; y) + !!x 0x08 isLessOrEqual 12345678910/* * isLessOrEqual - if x &lt;= y then return 1, else return 0 * Example: isLessOrEqual(4,5) = 1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 24 * Rating: 3 */int isLessOrEqual(int x, int y) &#123; return 2;&#125; 这道题要求实现 &lt;= 运算。十分自然的想法是利用减法运算 x - y，然后判断结果的符号位，看了一下不允许使用减法，又想了一下想起可以用取相反数的方法计算减法，而相反数则可以通过按位取反后加1获得。不知道这样算不算投机取巧。于是首先试一下 (x + (~y + 1) &gt;&gt; 31) &amp; 0x1，发现 isLessOrEqual(-2147483648[0x80000000],-2147483648[0x80000000]) 过不了，这才发觉有溢出时，符号位会丢失。因此下面需要将溢出的情况单列。 众所周知 ~y + 1 与 -y 对于任意整数 y 结果都相同，那么溢出的情况实际上就是正常的减法溢出。如较大(绝对值)的负数减去较大的正数，或者较大的正数减去较大(绝对值)的负数，而对于符号相同的情况，就绝不会溢出。总而言之，要被单列的情况一定是 x 和 y 异号的情况，而对于异号的情况，其大小关系又是确定的，因此本题就容易解了。 对于有符号整数 x 和 y，除去 x == y == 0，他们的正负共有以下四种情况： {x&lt;0,y&gt;0 (1)x&gt;0,y&lt;0 (2)x&lt;0,y&lt;0 (3)x&gt;0,y&gt;0 (4)\\begin{cases} \\mathrm{x &lt; 0, y &gt; 0} \\ \\text{(1)} \\\\ \\mathrm{x &gt; 0, y &lt; 0} \\ \\text{(2)}\\\\ \\mathrm{x &lt; 0, y &lt; 0} \\ \\text{(3)}\\\\ \\mathrm{x &gt; 0, y &gt; 0} \\ \\text{(4)} \\end{cases}⎩⎪⎪⎪⎪⎨⎪⎪⎪⎪⎧​x&lt;0,y&gt;0 (1)x&gt;0,y&lt;0 (2)x&lt;0,y&lt;0 (3)x&gt;0,y&gt;0 (4)​ 对于1, 2两种情况，其大小关系是显然的，对于3, 4两种情况，其大小关系可以使用表达式 (x + (~y + 1) &gt;&gt; 31) &amp; 0x1 判断，于是可以得到答案，由于情况分支较多，直接写位级运算表达式不方便，这里使用临时变量分步计算： 123456int xPositive = !((x &gt;&gt; 31) &amp; 0x1); // 1 if x &gt; 0int yPositive = !((y &gt;&gt; 31) &amp; 0x1); // 1 if y &gt; 0int xPosAndyNeg = xPositive &amp; (!yPositive); // 1 if x &gt; 0, y &lt; 0int xNegAndyPos = (!xPositive) &amp; yPositive; // 1 if x &lt; 0, y &gt; 0int xyPosOrNegAndxLessy = ((x + (~y + 1)) &gt;&gt; 31) &amp; 0x1; // 1 if x*y &gt;= 0, x &lt;= yreturn (!xPosAndyNeg) &amp; (xNegAndyPos | xyPosOrNegAndxLessy); 这样的答案再次测试发现还是无法通过上面那个 0x80000000 的用例，原来是因为只注意了 ~y + 1 与 -y 的等价性，忘记了 -y 本身就可能溢出，这种溢出情况就是对于最小的负数 0x80000000 才会出现的，其原因在于补码正负域的不对称性。因为当且仅当 y = 0x80000000 时，才会出现溢出，而且此时当且仅当 x = 0x80000000 时才返回 1，所以可以将这种情况使用 x == y 的条件过滤掉。最终答案是： 1234567int xPositive = !((x &gt;&gt; 31) &amp; 0x1); // 1 if x &gt; 0int yPositive = !((y &gt;&gt; 31) &amp; 0x1); // 1 if y &gt; 0int xPosAndyNeg = xPositive &amp; (!yPositive); // 1 if x &gt; 0, y &lt; 0int xNegAndyPos = (!xPositive) &amp; yPositive; // 1 if x &lt; 0, y &gt; 0int xyPosOrNegAndxLessy = ((x + (~y + 1)) &gt;&gt; 31) &amp; 0x1; // 1 if x*y &gt;= 0, x &lt;= yint xyEqual = !(x ^ y); // 1 if x == yreturn (!xPosAndyNeg) &amp; (xyEqual | xNegAndyPos | xyPosOrNegAndxLessy); 0x09 logicalNeg 1234567891011/* * logicalNeg - implement the ! operator, using all of * the legal operators except ! * Examples: logicalNeg(3) = 0, logicalNeg(0) = 1 * Legal ops: ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 4 */int logicalNeg(int x) &#123; return 2;&#125; 这道题要求实现逻辑非，即若 x == 0，返回 1，否则返回 0。考虑补码整数的位模式，容易想到当 x == 0 时，所有位都为 0。针对这一特征，若所有位相互取或结果为 0，那么该 x 一定是 0。 想起leftmost_one或者判断整数位模式中是否有偶数个1所提到的将所有位彼此做逻辑运算的方法，可以知道以下的操作将 x 的最低位设置成 x 的所有位的或： 12345x |= x &gt;&gt; 16;x |= x &gt;&gt; 8;x |= x &gt;&gt; 4;x |= x &gt;&gt; 2;x |= x &gt;&gt; 1; 这样若 x == 0，则 (~x) &amp; 0x1 == 1，反之仍然成立。因此答案是： 123456x |= x &gt;&gt; 16;x |= x &gt;&gt; 8;x |= x &gt;&gt; 4;x |= x &gt;&gt; 2;x |= x &gt;&gt; 1;return (~x) &amp; 0x1; 0x0A howManyBits 123456789101112131415/* howManyBits - return the minimum number of bits required to represent x in * two&#x27;s complement * Examples: howManyBits(12) = 5 * howManyBits(298) = 10 * howManyBits(-5) = 4 * howManyBits(0) = 1 * howManyBits(-1) = 1 * howManyBits(0x80000000) = 32 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 90 * Rating: 4 */int howManyBits(int x) &#123; return 2;&#125; 这道题计算要表示给定的整数，用补码方式最少需要多少位。例如 12 = 0x0000000C，仅看低4位是 1100。由于补码的最高位是负权，如果仅用4位，那么这个位模式表示的就不是 12 而是 -4，所以表示 12 需要 4 + 1 = 5 位。再如 -5 = 0xFFFFFFFB，其位模式低四位是 1011，这个位模式最高位按负权解释刚好是 -5，因此 -5 需要4位。根据这种解释方式，其他数据所需的最小位数也可以推导得到，不赘述。同时可以发现一个规律：对于正数，所需的位数就是位模式中最左侧那个 1 的位置再加1(符号位)，而对于负数，其所需的位数则是该负数位模式最左侧的 0 所在的位置再加1，可以对负数按位取反，这样就可以统一为最左侧 1 的位置再加1。 基于这个思想，目前答案的关键是求出位模式最左侧的 1。再次根据leftmost_one所使用的方式和第七题所使用的无分支实现conditional方法，可以算出 x 的最左侧的1的形式： 123456789101112int isXNeg = x &gt;&gt; 31; // 0xffffffff if x &lt; 0; 0 if x &gt;= 0int xPositive = isXNeg | x + (isXNeg &amp; 0x1); // x if x &gt;= 0; -1 if x &lt; 0int xNegitive = (isXNeg &amp; (~x)); // ~x if x &lt; 0; 0 if x &gt;= 0int xAbs = xPositive + xNegitive; // x if x &gt;= 0; ~x if x &lt; 0// get leftmost_oneint leftmost_one = xAbs |= xAbs &gt;&gt; 1;leftmost_one |= leftmost_one &gt;&gt; 2;leftmost_one |= leftmost_one &gt;&gt; 4;leftmost_one |= leftmost_one &gt;&gt; 8;leftmost_one |= leftmost_one &gt;&gt; 16;leftmost_one &gt;&gt;= 1;leftmost_one += 1; 这时还剩下一个关键的问题：如何将最左侧的1形式的二进制数转换为10进制整数的形式。具体来说：以四位数为例，就是把下表中由 Y3Y2Y1Y0\\mathrm{Y_3 Y_2 Y_1 Y_0}Y3​Y2​Y1​Y0​ 组成的二进制数转换为由 X2X1X0\\mathrm{X_2 X_1 X_0}X2​X1​X0​ 组成的二进制无符号整数。 Y3\\mathrm{Y_3}Y3​ Y2\\mathrm{Y_2}Y2​ Y1\\mathrm{Y_1}Y1​ Y0\\mathrm{Y_0}Y0​ X2\\mathrm{X_2}X2​ X1\\mathrm{X_1}X1​ X0\\mathrm{X_0}X0​ 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 这个真值表如果反过来——将 X2X1X0\\mathrm{X_2 X_1 X_0}X2​X1​X0​ 作为输入参数，而 Y3Y2Y1Y0\\mathrm{Y_3 Y_2 Y_1 Y_0}Y3​Y2​Y1​Y0​ 作为输出参数——有过数字电路学习经验的就会看出这实际上是一个译码过程，更具体的说，是一个3-8译码器的真值表(X2X1X0\\mathrm{X_2 X_1 X_0}X2​X1​X0​ 需要扩展到 Y7\\mathrm{Y_7}Y7​)。 对于上表所示的4位真值表，对 X2,X1,X0\\mathrm{X_2, X_1, X_0}X2​,X1​,X0​ 分别进行卡诺图化简（实际上不需要使用卡诺图，这种类型的解码器逻辑式是相当容易推到的），可以算出： {X0=Y0+Y2X1=Y1+Y2X2=Y3\\begin{cases} X_0 &amp; = &amp; Y_0 + Y_2 \\\\ X_1 &amp; = &amp; Y_1 + Y_2 \\\\ X_2 &amp; = &amp; Y_3 \\end{cases}⎩⎪⎪⎨⎪⎪⎧​X0​X1​X2​​===​Y0​+Y2​Y1​+Y2​Y3​​ 这个形式规律较为混乱，猜测主要原因是对于4位的译码输出，只需要2位的整数输入就够了。然而这样的话就要求将 0000 的情况单列，从 0001 开始映射为2位输出的 00。即如下的真值表： Y3\\mathrm{Y_3}Y3​ Y2\\mathrm{Y_2}Y2​ Y1\\mathrm{Y_1}Y1​ Y0\\mathrm{Y_0}Y0​ X1\\mathrm{X_1}X1​ X0\\mathrm{X_0}X0​ 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 这种情况下 X1,X0\\mathrm{X_1, X_0}X1​,X0​ 的逻辑式为： {X0=Y1+Y3X1=Y2+Y3\\begin{cases} X_0 &amp; = &amp; Y_1 + Y_3 \\\\ X_1 &amp; = &amp; Y_2 + Y_3 \\\\ \\end{cases}{X0​X1​​==​Y1​+Y3​Y2​+Y3​​ 做到这里还是没有什么明显的头绪，想直接画出32位的真值表又感觉太惊世骇俗，看来这道题给了90个运算符限额真不是太简单的。（2021.7.30 21:50） （2021.7.31 11:54）过了一夜也没想出解法，我先试试真值表能不能解出来： Y32...Y1Y0\\mathrm{Y_{32}...Y_1Y_0}Y32​...Y1​Y0​ X4\\mathrm{X_4}X4​ X3\\mathrm{X_3}X3​ X2\\mathrm{X_2}X2​ X1\\mathrm{X_1}X1​ X0\\mathrm{X_0}X0​ 0000 0000 0000 0000 0000 0000 0000 0001 0 0 0 0 0 0000 0000 0000 0000 0000 0000 0000 0010 0 0 0 0 1 0000 0000 0000 0000 0000 0000 0000 0100 0 0 0 1 0 0000 0000 0000 0000 0000 0000 0000 1000 0 0 0 1 1 0000 0000 0000 0000 0000 0000 0001 0000 0 0 1 0 0 0000 0000 0000 0000 0000 0000 0010 0000 0 0 1 0 1 0000 0000 0000 0000 0000 0000 0100 0000 0 0 1 1 0 0000 0000 0000 0000 0000 0000 1000 0000 0 0 1 1 1 0000 0000 0000 0000 0000 0001 0000 0000 0 1 0 0 0 0000 0000 0000 0000 0000 0010 0000 0000 0 1 0 0 1 0000 0000 0000 0000 0000 0100 0000 0000 0 1 0 1 0 0000 0000 0000 0000 0000 1000 0000 0000 0 1 0 1 1 0000 0000 0000 0000 0001 0000 0000 0000 0 1 1 0 0 0000 0000 0000 0000 0010 0000 0000 0000 0 1 1 0 1 0000 0000 0000 0000 0100 0000 0000 0000 0 1 1 1 0 0000 0000 0000 0000 1000 0000 0000 0000 0 1 1 1 1 0000 0000 0000 0001 0000 0000 0000 0000 1 0 0 0 0 0000 0000 0000 0010 0000 0000 0000 0000 1 0 0 0 1 0000 0000 0000 0100 0000 0000 0000 0000 1 0 0 1 0 0000 0000 0000 1000 0000 0000 0000 0000 1 0 0 1 1 0000 0000 0001 0000 0000 0000 0000 0000 1 0 1 0 0 0000 0000 0010 0000 0000 0000 0000 0000 1 0 1 0 1 0000 0000 0100 0000 0000 0000 0000 0000 1 0 1 1 0 0000 0000 1000 0000 0000 0000 0000 0000 1 0 1 1 1 0000 0001 0000 0000 0000 0000 0000 0000 1 1 0 0 0 0000 0010 0000 0000 0000 0000 0000 0000 1 1 0 0 1 0000 0100 0000 0000 0000 0000 0000 0000 1 1 0 1 0 0000 1000 0000 0000 0000 0000 0000 0000 1 1 0 1 1 0001 0000 0000 0000 0000 0000 0000 0000 1 1 1 0 0 0010 0000 0000 0000 0000 0000 0000 0000 1 1 1 0 1 0100 0000 0000 0000 0000 0000 0000 0000 1 1 1 1 0 1000 0000 0000 0000 0000 0000 0000 0000 1 1 1 1 1 此表可以推出X4X3X2X1X0\\mathrm{X_4 X_3 X_2 X_1 X_0}X4​X3​X2​X1​X0​的逻辑表达式： {X0=Y1+Y3+Y5+Y7+Y9+Y11+Y13+Y15+Y17+Y19+Y21+Y23+Y25+Y27+Y29+Y31X1=Y2+Y3+Y6+Y7+Y10+Y11+Y14+Y15+Y18+Y19+Y22+Y23+Y26+Y27+Y30+Y31X2=Y4+Y5+Y6+Y7+Y12+Y13+Y14+Y15+Y20+Y21+Y22+Y23+Y28+Y29+Y30+Y31X3=Y8+Y9+Y10+Y11+Y12+Y13+Y14+Y15+Y24+Y25+Y26+Y27+Y28+Y29+Y30+Y31X4=Y16+Y17+Y18+Y19+Y20+Y21+Y22+Y23+Y24+Y25+Y26+Y27+Y28+Y29+Y30+Y31\\begin{cases} \\mathrm{X_0} = \\mathrm{Y_{1} + Y_{3} + Y_{5} + Y_{7} + Y_{9} + Y_{11} + Y_{13} + Y_{15} + Y_{17} + Y_{19} + Y_{21} + Y_{23} + Y_{25} + Y_{27} + Y_{29} + Y_{31}} \\\\ \\mathrm{X_1} = \\mathrm{Y_{2} + Y_{3} + Y_{6} + Y_{7} + Y_{10} + Y_{11} + Y_{14} + Y_{15} + Y_{18} + Y_{19} + Y_{22} + Y_{23} + Y_{26} + Y_{27} + Y_{30} + Y_{31}} \\\\ \\mathrm{X_2} = \\mathrm{Y_{4} + Y_{5} + Y_{6} + Y_{7} + Y_{12} + Y_{13} + Y_{14} + Y_{15} + Y_{20} + Y_{21} + Y_{22} + Y_{23} + Y_{28} + Y_{29} + Y_{30} + Y_{31}} \\\\ \\mathrm{X_3} = \\mathrm{Y_{8} + Y_{9} + Y_{10} + Y_{11} + Y_{12} + Y_{13} + Y_{14} + Y_{15} + Y_{24} + Y_{25} + Y_{26} + Y_{27} + Y_{28} + Y_{29} + Y_{30} + Y_{31}} \\\\ \\mathrm{X_4} = \\mathrm{Y_{16} + Y_{17} + Y_{18} + Y_{19} + Y_{20} + Y_{21} + Y_{22} + Y_{23} + Y_{24} + Y_{25} + Y_{26} + Y_{27} + Y_{28} + Y_{29} + Y_{30} + Y_{31}} \\end{cases}⎩⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎧​X0​=Y1​+Y3​+Y5​+Y7​+Y9​+Y11​+Y13​+Y15​+Y17​+Y19​+Y21​+Y23​+Y25​+Y27​+Y29​+Y31​X1​=Y2​+Y3​+Y6​+Y7​+Y10​+Y11​+Y14​+Y15​+Y18​+Y19​+Y22​+Y23​+Y26​+Y27​+Y30​+Y31​X2​=Y4​+Y5​+Y6​+Y7​+Y12​+Y13​+Y14​+Y15​+Y20​+Y21​+Y22​+Y23​+Y28​+Y29​+Y30​+Y31​X3​=Y8​+Y9​+Y10​+Y11​+Y12​+Y13​+Y14​+Y15​+Y24​+Y25​+Y26​+Y27​+Y28​+Y29​+Y30​+Y31​X4​=Y16​+Y17​+Y18​+Y19​+Y20​+Y21​+Y22​+Y23​+Y24​+Y25​+Y26​+Y27​+Y28​+Y29​+Y30​+Y31​​ 接下来就是逐个计算出各个位的值。计算的过程实际上还是与leftmost_one或者判断整数位模式中是否有偶数个1中提到的逐位做逻辑运算的方法一致，具体来说，就是逐步移位并与移位前的值运算。 关于计算方法，还有进一步解释的必要。以 X0\\mathrm{X_0}X0​ 的表达式为例，可以看出它的式子中是 Y1∣Y3∣Y5∣Y7...\\mathrm{Y_1 | Y_3 | Y_5 | Y_7...}Y1​∣Y3​∣Y5​∣Y7​... 即所有的偶数(从第0位开始算)位之间取或。这种模式就可以先移动两位与原数字取或，做到 Y1∣Y3\\mathrm{Y_1 | Y_3}Y1​∣Y3​ 和 Y5∣Y7\\mathrm{Y_5 | Y_7}Y5​∣Y7​ 等。然后移动四位与移动两位取或的结果取或，做到 Y1∣Y3∣Y5∣Y7\\mathrm{Y_1 | Y_3 | Y_5 | Y_7}Y1​∣Y3​∣Y5​∣Y7​ 等。最终移动16位与上一次移位取或的结果取或，做到所有偶数位取或。最终取或的结果总是在位向量的最左侧或者最右侧(取决于移位是向左移还是向右移)。 观察上面5个位的表达式，会发现都具有类似的规律，可以用下面的过程分别求出其值： 12345678910111213141516171819202122232425262728293031323334int X_0 = (leftmost_one | (leftmost_one &gt;&gt; 16));X_0 |= X_0 &gt;&gt; 8;X_0 |= X_0 &gt;&gt; 4;X_0 |= X_0 &gt;&gt; 2;X_0 &gt;&gt;= 1;X_0 &amp;= 0x1;int X_1 = leftmost_one | (leftmost_one &lt;&lt; 1);X_1 |= X_1 &lt;&lt; 4;X_1 |= X_1 &lt;&lt; 8;X_1 |= X_1 &lt;&lt; 16;X_1 &gt;&gt;= 31;X_1 &amp;= 0x1;int X_2 = leftmost_one | (leftmost_one &lt;&lt; 8);X_2 |= X_2 &lt;&lt; 16;X_2 |= X_2 &lt;&lt; 2;X_2 |= X_2 &lt;&lt; 1;X_2 &gt;&gt;= 31;X_2 &amp;= 0x1;int X_3 = leftmost_one | (leftmost_one &lt;&lt; 16);X_3 |= X_3 &lt;&lt; 4;X_3 |= X_3 &lt;&lt; 2;X_3 |= X_3 &lt;&lt; 1;X_3 &gt;&gt;= 31;X_3 &amp;= 0x1;int X_4 = leftmost_one | (leftmost_one &lt;&lt; 8);X_4 |= X_4 &lt;&lt; 4;X_4 |= X_4 &lt;&lt; 2;X_4 |= X_4 &lt;&lt; 1;X_4 &gt;&gt;= 31;X_4 &amp;= 0x1; 求出每一位的值后，将这5个位组装到一个整数中： 1234X_4 = X_3 | (X_4 &lt;&lt; 1);X_4 = X_2 | (X_4 &lt;&lt; 1);X_4 = X_1 | (X_4 &lt;&lt; 1);X_4 = X_0 | (X_4 &lt;&lt; 1); 这样 X4\\mathrm{X_4}X4​ 就是解码得到的数。但是由于真值表是从 00000 开始的，因此该值还需要加上 1 才是最左侧的1真实的位置。 根据上文说到的，最终结果实际上是最左侧1的位置再加1，因此答案应该是 X_4 + 1 + 1。但是还有特例：0 和 -1。对于 0，其只需要1位即可表示，但是求解过程中 X4==0\\mathrm{X_4} == 0X4​==0，最终返回 2。对于 -1，其也是只需一位，但是对其求反后它的位模式与 0相同，因此计算的最左侧1的位置也是0，最终返回 2。可以发现对于 xAbs 位模式为全零的值，真值表中没有体现，实际计算时得到的 X4\\mathrm{X_4}X4​ 值与位模式为最低位为 1 的情况一样，都是 0。因此这种情况应该特殊处理，即当 xAbs != 0 时返回 X_4 + 1 + 1，其他时候返回 X_4 + 1。最终答案是 X_4 + 1 + (!!xAbs)。 最终使用了81个运算符才解出答案，而且过程也十分曲折和愚蠢，记录的也非常拖沓缺乏条理。如果有更好的答案，也许我会学习一个。 0x0B floatScale2 1234567891011121314/* * floatScale2 - Return bit-level equivalent of expression 2*f for * floating point argument f. * Both the argument and result are passed as unsigned int&#x27;s, but * they are to be interpreted as the bit-level representation of * single-precision floating point values. * When argument is NaN, return argument * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */unsigned floatScale2(unsigned uf) &#123; return 2;&#125; 这一题是计算浮点数的2倍，不过是以无符号数的形式进行位级的计算，这样更能够理解当浮点数乘以2时，在位级的层面究竟发生了什么。 根据IEEE754浮点数的定义，一个32位的单精度浮点数可以分为符号位，阶码和尾数三部分。其具体的解释可以看这篇文章。 123int sign = uf &gt;&gt; 31;int exp = (uf &gt;&gt; 23) &amp; 0xFF;int frac = (uf &lt;&lt; 9) &gt;&gt; 9; 首先关注特殊情况，即阶码全为 1 或全为 0 时： 全为 1 时：该浮点数是无穷大或 NaN。不管哪种情况，在乘2的情形下都应该原样返回： 1if(exp == 0xFF)return uf; 全为 0 时：该浮点数的尾数相当的小，至少还没有用完尾数的表达空间，此时我们将尾数左移1位作为乘2： 1if(exp == 0) frac &lt;&lt;= 1; 当阶码既非全 1 也非全 0 时，即取规格化的值时，此时尾数的编码方式是隐含的以1开头的表示，其值一定大于1而小于2，此时无法通过改变尾数来实现乘2，不过根据浮点数的公式：V=(−1)S×M×2EV = (-1)^S \\times M\\times 2^EV=(−1)S×M×2E，通过阶码 EEE 加1可以实现乘以2： 1else exp++; 对这三种情况，情况1最为简单，也没有什么问题。情况2则需要考虑溢出的问题：当对尾数左移1位时，有可能造成原来属于尾数的1被移出尾数的范围。这说明移位后尾数值大于1，无法再用阶码全为 0 这种方式表示，应该将被移出的位放进阶码，否则乘2的计算就是错误的，因此最终的答案应该加上： 12345exp += (frac &gt;&gt; 23);uf = s;uf = (uf &lt;&lt; 8) | exp;uf = (uf &lt;&lt; 23) | frac;return uf; 0x0C floatFloat2Int 123456789101112131415/* * floatFloat2Int - Return bit-level equivalent of expression (int) f * for floating point argument f. * Argument is passed as unsigned int, but * it is to be interpreted as the bit-level representation of a * single-precision floating point value. * Anything out of range (including NaN and infinity) should return * 0x80000000u. * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */int floatFloat2Int(unsigned uf) &#123; return 2;&#125; 这道题要求实现一个位级的浮点数转整数。还是先将浮点数分为三个部分： 123int exp = (uf &gt;&gt; 23) &amp; 0xFF;int frac = (uf &lt;&lt; 9) &gt;&gt; 9;int sign = uf &gt;&gt; 31; 然后根据这篇文章可以知道：当阶码小于 0x7F 时，浮点数的绝对值都小于 1。因此： 1if(exp &lt; 0x7F) return 0; 同时我们知道最大的补码整数是 0x7FFFFFFF，即 1×231−11\\times 2^{31} - 11×231−1，对应到浮点数是 2147483647.0=1×2312147483647.0 = 1\\times 2^{31}2147483647.0=1×231，因此当指数(E = e - 127)大于31时，应该返回题目要求的0x80000000u。 除去以上两种情况，剩下的就是本题的普遍情况。我们知道浮点数的编码方式是 V=M×2EV = M\\times 2^{E}V=M×2E，其中 MMM 就是尾数，EEE 就是阶码。我们还知道尾数的编码方式是负权的，也就是对于 M=100M = 100M=100 这种编码，实际上可以看作 M=1.100=1×2−1M = 1.100 = 1\\times 2^{-1}M=1.100=1×2−1。结合阶码来看，当阶码的值为正时(如果阶码为负根本不会进入这种情况)，例如阶码为 2，那么该浮点数的绝对值就是：∣V∣=1×2−1×22=110.000|V| = 1\\times 2^{-1} \\times 2^2 = 110.000∣V∣=1×2−1×22=110.000。也就是说，虽然尾数的编码是负权的，但是每一位阶码都可以“解救”一位尾数成为正权。反过来说，对于值为 E 的阶码和 23 位的尾数，将有 23 - E 位的尾数(低位)所表示的信息会由于阶码不够而被舍入。更具体地说，就是尾数向右移动 23 - E 位得到的就是浮点数对应整数的绝对值。当然不能忘记该种情况下尾数是隐含1的，也就是尾数的最高位前面还要增加一个1。 由于该题可以使用 while 语句，因此这部分可以这样实现： 1234int result = frac | (1 &lt;&lt; 23);int shift = 23 - E;while (shift--) result &gt;&gt;= 1; 最后只需要处理符号部分，当浮点数为负时返回负的补码数就可以。 12if(s)return ~result + 1;return result; 0x0D floatPower2 12345678910111213141516/* * floatPower2 - Return bit-level equivalent of the expression 2.0^x * (2.0 raised to the power x) for any 32-bit integer x. * * The unsigned value that is returned should have the identical bit * representation as the single-precision floating-point number 2.0^x. * If the result is too small to be represented as a denorm, return * 0. If too large, return +INF. * * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. Also if, while * Max ops: 30 * Rating: 4 */unsigned floatPower2(int x) &#123; return 2;&#125; 这题要求实现位级2的幂。2的幂与浮点数的表示方式十分亲和，因此这题也较为简单。我们知道单精度浮点数能够表示的范围在 (1−ϵ)×2−126∽(2−ϵ)×2127(1 - \\epsilon)\\times 2^{-126} \\backsim (2 - \\epsilon)\\times 2^{127}(1−ϵ)×2−126∽(2−ϵ)×2127之间，因此当 x 在 [-126, 127] 范围之外时，应属于特殊情况： 12if(x &gt; 127) return 0x7f800000;if(x &lt; -126) return 0; 其余情况下，对于浮点数 2.0x2.0^x2.0x，它的符号和尾数应该全部是 0，而阶数则随着 x 变化。具体的说，由于阶码即2的幂，而且阶数 E = e - 127，因此阶码 e = x + 127。将其移动到阶码的位置，不需要动其他的位，就是正确答案了： 1return (127 + x) &lt;&lt; 23; 0x0E 前后花了3天，甚至正在学习的第三章的内容都被我放下，优先做这个Lab，感觉还是十分有意思的，尤其是做的过程中主动复习了很多逻辑函数方面的知识，对第二章中的重点：数据的编码方式和位级行为的认识也更加清楚。最后放一张全部通过的截图吧，希望能够坚持下去做完所有的Lab。","categories":[{"name":"题解类","slug":"题解类","permalink":"https://blog.zhuwenq.cc/categories/%E9%A2%98%E8%A7%A3%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"位运算","slug":"位运算","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"逻辑代数","slug":"逻辑代数","date":"2021-07-28T10:39:35.000Z","updated":"2023-06-21T06:51:28.948Z","comments":true,"path":"逻辑代数/","link":"","permalink":"https://blog.zhuwenq.cc/%E9%80%BB%E8%BE%91%E4%BB%A3%E6%95%B0/","excerpt":"0x00 定义 逻辑代数是代数的一个分支，与普通代数相比，逻辑代数主要定义了与，或，非三种运算，是用普通代数描述数字之间关系的方式来描述逻辑关系的形式主义[^wikipedia]。 逻辑代数是乔治·布尔（George Boole）在他的第一本书《逻辑的数学分析》（1847年）中引入的，并在他的《思想规律的研究》（1854年）中更充分的提出了逻辑代数。[^BooleGeorge]","text":"0x00 定义 逻辑代数是代数的一个分支，与普通代数相比，逻辑代数主要定义了与，或，非三种运算，是用普通代数描述数字之间关系的方式来描述逻辑关系的形式主义[^wikipedia]。 逻辑代数是乔治·布尔（George Boole）在他的第一本书《逻辑的数学分析》（1847年）中引入的，并在他的《思想规律的研究》（1854年）中更充分的提出了逻辑代数。[^BooleGeorge] 0x01 起因 在学习 CS:APP 这本书的第2章时，常常用到位级运算，甚至它的习题还有只允许使用部分位级逻辑运算的规矩。当时在笔记上记下了要复习数字逻辑电路的知识，不过一直没有行动。今天在做 CS:APP 的 DataLab 时，更是感觉数字逻辑电路中常用的组合逻辑函数十分有用。在后悔大学数字电路没有好好学习而且没有笔记之后，还是得重新学习这方面的知识。 逻辑代数与计算机电子电路的行为紧密相关，实际上，香农在它的硕士学位论文中证明了两者之间的等价性[^Shannon]。在非硬件的场合，也非常有用，可以用它对复合条件表达式化简，写出最高效的逻辑表达式；或者可以作为完成CS:APP作业的后备知识。 由于课本早已丢失，以下的内容均来自互联网。具体来说，主要来自这篇博客。 0x02 基本规律 在逻辑代数中，我们使用大写字母 A, B, C... 等作为基本单元，它们可以是变量或者逻辑表达式。在编程语言中，常用 &amp;, |, ~ 来表示位级的与，或，非，在逻辑代数中，AB 代表 A &amp; B；A + B 代表 A | B；A' 则代表 ~A。 0/1律 0 + A = A 0·A = 0 1 + A = 1 1·A = A 这几条十分简单，不多解释。但是其常用于复杂表达式的化简过程中。 还原律/重叠律/互补律 A'' = A A + A = A, A·A = A A + A' = 1, A·A' = 0 交换律/结合律/分配律 将与和或看作普通代数中的乘和加，这些规律也与普通代数中具有的类似。 A + B = B + A A·B = B·A (A + B) + C = A + (B + C) (AB)·C = A·(BC) A(B + C) = AB + AC A + BC = (A + B)(A + C) 较为特殊的是最后一条或对与的分配律，它与倒数第二条与对或的分配律形式上同构，但是与普通代数差异较大。 证明：(A + B)(A + C) = A + AB + AC + BC = A(1 + B + C) + BC = A + BC 摩根定律 (A + B)' = A'B' (AB)' = A' + B' 这一规律可以总结为：或的非是非的与，与的非是非的或，或者用集合的概念讲： 并集的补集是补集的交集，交集的补集是补集的并集 这个公式十分重要，在逻辑函数化简和逻辑转换中必不可缺。例如，可以使用它将与逻辑转换为或逻辑和非逻辑——AB = (A' + B')'，也可以反过来——A + B = (A'B')' 根据这一定律的思想可以推出两条规则 反演规则：在对逻辑函数取反时，或变为与，与变为或，然后对每个量取反，但要保留组合量的非号。 如：(((A + B)C + D)(E + F))' = ((A'B' + C')D') + E'F' 对偶规则：这个规则是上面提到的与或逻辑转换的一个严谨定义。它的内容是：若两个逻辑式F1=F2F_1 = F_2F1​=F2​成立，那么它们的对偶式F1D=F2DF_1^D = F_2^DF1D​=F2D​也成立。 如：A(B + C) = AB + AC，分别求左右两边的对偶式：A + BC = (A + B)(A + C)。 取对偶式的规则是：或变与，与变或，保证运算顺序不变；常数项0变1，1变0 常用公式 这一部分中的公式是根据上面基本公式推导而来，在逻辑函数化简时经常用到。 吸收公式：A + AB = A 消因子公式：A + A'B = A + B 并项公式：AB + AB' = A 消项公式：AB + A'C + BC = AB + A'C 其中消因子公式利用了或对与的分配律，吸收公式和并项公式利用了与对或的分配律的逆运算，消项公式的证明如下： 12345AB + A&#x27;C + BC= AB + A&#x27;C + (A + A&#x27;)BC= AB + A&#x27;C + ABC + A&#x27;BC= AB(1 + C) + A&#x27;C(1 + B)= AB + A&#x27;C 逻辑函数化简与卡诺图 （这部分等到需要用了再写… 0xFF 参考 [^wikipedia] 维基百科-逻辑代数 [^BooleGeorge] Boole, George. An Investigation of the Laws of Thought. Prometheus Books. 2003 [1854]. ISBN 978-1-59102-089-9. [^Shannon] Shannon C E. A symbolic analysis of relay and switching circuits[J]. Electrical Engineering, 1938, 57(12): 713-723.","categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"}],"tags":[{"name":"逻辑代数","slug":"逻辑代数","permalink":"https://blog.zhuwenq.cc/tags/%E9%80%BB%E8%BE%91%E4%BB%A3%E6%95%B0/"}]},{"title":"leftmost_one","slug":"leftmost-one","date":"2021-07-22T11:16:50.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"leftmost-one/","link":"","permalink":"https://blog.zhuwenq.cc/leftmost-one/","excerpt":"CS:APP homework 2.66 题： 写出代码实现如下函数： /* Generate mask indicating leftmost 1 in x, Assume w = 32. For example, 0xFF00 -&gt; 0x8000, and 0x6000 -&gt; 0x4000. If x = 0, return 0 */ int leftmost_one(unsigned x); 函数应该遵循位级整数编码规则，不过可以假设数据类型 int 有 w=32 位。 你的代码最多只能包含15个算术运算，位运算和逻辑运算。 提示：先将 x 转换成形如[0…011…1]的位向量。","text":"CS:APP homework 2.66 题： 写出代码实现如下函数： /* Generate mask indicating leftmost 1 in x, Assume w = 32. For example, 0xFF00 -&gt; 0x8000, and 0x6000 -&gt; 0x4000. If x = 0, return 0 */ int leftmost_one(unsigned x); 函数应该遵循位级整数编码规则，不过可以假设数据类型 int 有 w=32 位。 你的代码最多只能包含15个算术运算，位运算和逻辑运算。 提示：先将 x 转换成形如[0…011…1]的位向量。 一开始没有注意到提示是什么意思，后来才反应过来是要将从左侧第一个1开始向右都设置为1。看了答案发现解法与2.65题偶数个1十分相似： 123456789int leftmost_one(unsigned x)&#123; x |= x &gt;&gt; 1; x |= x &gt;&gt; 2; x |= x &gt;&gt; 4; x |= x &gt;&gt; 8; x |= x &gt;&gt; 16; return (x &gt;&gt; 1) + (x &amp;&amp; 0x1);&#125; 结合2.65题的启示，这道题的解法可能是要做到每一位的或运算。经过推导 w = 4 的情况： x=[x3,x2,x1,x0]x = [x_3, x_2, x_1, x_0] x=[x3​,x2​,x1​,x0​] 则最终 (x |= x &gt;&gt; 16) 的结果为： x=[x0,x0∣x1,x0∣x1∣x2,x0∣x1∣x2∣x3]x = [x_0, x_0 | x_1, x_0 | x_1 | x_2, x_0 | x_1 | x_2 | x_3] x=[x0​,x0​∣x1​,x0​∣x1​∣x2​,x0​∣x1​∣x2​∣x3​] 这一结果可以将从最左侧的1的位置以右的所有位都置为1，具体的情形是：这一结果的位模式从最左侧开始分别是原位向量的最左侧向右逐步取或，这样一旦原位模式中有1存在，则该位置开始向右都将变为1。 完成上述过程后，将所得的结果向右逻辑移动1位，相当于最左侧位置再向右一位开始的位都为1，此时若再加1，则由于进位，原位向量中最左侧1的位置将进位为1，而其余位置为0，这样就达到了题目要求。当 x == 0 时，x &amp;&amp; 0x1 得到结果0，最终返回0。 关于最左侧的1的情况就是这样。","categories":[{"name":"题解类","slug":"题解类","permalink":"https://blog.zhuwenq.cc/categories/%E9%A2%98%E8%A7%A3%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"位运算","slug":"位运算","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"判断整数位模式中是否有偶数个1","slug":"判断整数位模式中是否有偶数个1","date":"2021-07-19T11:02:08.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"判断整数位模式中是否有偶数个1/","link":"","permalink":"https://blog.zhuwenq.cc/%E5%88%A4%E6%96%AD%E6%95%B4%E6%95%B0%E4%BD%8D%E6%A8%A1%E5%BC%8F%E4%B8%AD%E6%98%AF%E5%90%A6%E6%9C%89%E5%81%B6%E6%95%B0%E4%B8%AA1/","excerpt":"CS:APP 家庭作业 2.65 题： 写出代码实现如下函数： /* Return 1 when x contains an odd number of 1s; 0 otherwise Assume w=32*/ int odd_ones(unsigned x); 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。 你的代码最多只能包含12个算术运算，位运算和逻辑运算。","text":"CS:APP 家庭作业 2.65 题： 写出代码实现如下函数： /* Return 1 when x contains an odd number of 1s; 0 otherwise Assume w=32*/ int odd_ones(unsigned x); 函数应该遵循位级整数编码规则，不过你可以假设数据类型int有w=32位。 你的代码最多只能包含12个算术运算，位运算和逻辑运算。 这道题左边有四颗星，实际的难度也确实很高。在苦思冥想没有结果之后，我在github上找到了答案。令人备受打击的是答案一时也没有看懂，在纸上推导之后才明白答案为何是正确的。如果让我来做，断然是想不到这种巧妙的方法。 这道题目是要判断一个32位整数的位模式中是否含有偶数个1，需要做的就是在该整数确实有偶数个1时返回1，否则返回0。由于偶数个1的位模式有太多种形式，必然无法用掩码的方式提取出这种特征。由于我确实没有更好的思路，下面直接看别人的解答： 12345678910int odd_ones(unsigned x)&#123; x ^= x &gt;&gt; 16; x ^= x &gt;&gt; 8; x ^= x &gt;&gt; 4; x ^= x &gt;&gt; 2; x ^= x &gt;&gt; 1; x &amp;= 0x1; return !x;&#125; 这个答案一眼看上去就给人极大的启发，在这种极大启发带来的欣喜下我以为可以看出这种做法的合理性，最后还是得推导每一步才明白。 题目是关于32位整数的，为了推导时的方便，不妨以8位为例： 12345678int odd_ones(uint8_t x)&#123; x ^= x &gt;&gt; 4; x ^= x &gt;&gt; 2; x ^= x &gt;&gt; 1; x &amp;= 0x1; return !x;&#125; 输入8位整数x的位模式可以写作：x0=[x7,x6,x5,x4,x3,x2,x1,x0]x^0 = [x_7, x_6, x_5, x_4, x_3, x_2, x_1, x_0]x0=[x7​,x6​,x5​,x4​,x3​,x2​,x1​,x0​]，则第一行x ^= x &gt;&gt; 4即是： [x7,x6,x5,x4,x3,x2,x1,x0]∧[0,0,0,0,x7,x6,x5,x4]\\begin{aligned} [&amp;x_7, &amp;x_6, &amp;x_5, &amp;x_4, &amp;x_3, &amp;x_2, &amp;x_1, &amp;x_0&amp;] \\wedge \\\\ [&amp;0, &amp;0, &amp;0, &amp;0, &amp;x_7, &amp;x_6, &amp;x_5, &amp;x_4&amp;] \\end{aligned}[[​x7​,0,​x6​,0,​x5​,0,​x4​,0,​x3​,x7​,​x2​,x6​,​x1​,x5​,​x0​x4​​]∧]​ 这个运算的结果可以简述如下：x的高四位保持不变，第四位变为原x的第四位和高四位按位异或。这个结果又可以详细描述如下： x1=[x7,x6,x5,x4,x3∧x7,x2∧x6,x1∧x5,x0∧x4]x^1 = [x_7, x_6, x_5, x_4, x_3 \\wedge x_7, x_2 \\wedge x_6, x_1 \\wedge x_5, x_0 \\wedge x_4] x1=[x7​,x6​,x5​,x4​,x3​∧x7​,x2​∧x6​,x1​∧x5​,x0​∧x4​] 如此种过程向下推广，则第二行运算是 [x7,x6,x5,x4,x3∧x7,x2∧x6,x1∧x5,x0∧x4]∧[0,0,x7,x6,x5,x4,x3∧x7,x2∧x6]\\begin{aligned} [&amp;x_7, &amp;x_6, &amp;x_5, &amp;x_4, &amp;x_3 \\wedge x_7, &amp;x_2 \\wedge x_6, &amp;x_1 \\wedge x_5, &amp;x_0 \\wedge x_4&amp;] \\wedge \\\\ [&amp;0, &amp;0, &amp;x_7, &amp;x_6, &amp;x_5, &amp;x_4, &amp;x_3 \\wedge x_7, &amp;x_2 \\wedge x_6&amp;] \\end{aligned}[[​x7​,0,​x6​,0,​x5​,x7​,​x4​,x6​,​x3​∧x7​,x5​,​x2​∧x6​,x4​,​x1​∧x5​,x3​∧x7​,​x0​∧x4​x2​∧x6​​]∧]​ 这个结果是： x2=[x7,x6,x5∧x7,x4∧x6,x3∧x5∧x7,x2∧x4∧x6,x1∧x3∧x5∧x7,x0∧x2∧x4∧x6]x^2 = [x_7, x_6, x_5 \\wedge x_7, x_4 \\wedge x_6, x_3 \\wedge x_5 \\wedge x_7, x_2 \\wedge x_4 \\wedge x_6, x_1 \\wedge x_3 \\wedge x_5 \\wedge x_7, x_0 \\wedge x_2 \\wedge x_4 \\wedge x_6] x2=[x7​,x6​,x5​∧x7​,x4​∧x6​,x3​∧x5​∧x7​,x2​∧x4​∧x6​,x1​∧x3​∧x5​∧x7​,x0​∧x2​∧x4​∧x6​] 第三行运算是： [x7,x6,x5∧x7,x4∧x6,x3∧x5∧x7,x2∧x4∧x6,x1∧x3∧x5∧x7,x0∧x2∧x4∧x6]∧[0,x7,x6,x5∧x7,x4∧x6,x3∧x5∧x7,x2∧x4∧x6,x1∧x3∧x5∧x7]\\begin{aligned} [&amp;x_7, &amp;x_6, &amp;x_5 \\wedge x_7, &amp;x_4 \\wedge x_6, &amp;x_3 \\wedge x_5 \\wedge x_7, &amp;x_2 \\wedge x_4 \\wedge x_6, &amp;x_1 \\wedge x_3 \\wedge x_5 \\wedge x_7, &amp;x_0 \\wedge x_2 \\wedge x_4 \\wedge x_6&amp;] \\wedge \\\\ [&amp;0, &amp;x_7, &amp;x_6, &amp;x_5 \\wedge x_7, &amp;x_4 \\wedge x_6, &amp;x_3 \\wedge x_5 \\wedge x_7, &amp;x_2 \\wedge x_4 \\wedge x_6, &amp;x_1 \\wedge x_3 \\wedge x_5 \\wedge x_7&amp;] \\end{aligned}[[​x7​,0,​x6​,x7​,​x5​∧x7​,x6​,​x4​∧x6​,x5​∧x7​,​x3​∧x5​∧x7​,x4​∧x6​,​x2​∧x4​∧x6​,x3​∧x5​∧x7​,​x1​∧x3​∧x5​∧x7​,x2​∧x4​∧x6​,​x0​∧x2​∧x4​∧x6​x1​∧x3​∧x5​∧x7​​]∧]​ 对于这一步运算，但看其最低位，值为： x1∧x3∧x5∧x7∧x0∧x2∧x4∧x6\\begin{aligned} &amp;x_1 \\wedge x_3 \\wedge x_5 \\wedge x_7 \\wedge \\\\ &amp;x_0 \\wedge x_2 \\wedge x_4 \\wedge x_6 \\end{aligned}​x1​∧x3​∧x5​∧x7​∧x0​∧x2​∧x4​∧x6​​ 可以看到，这一位的值实际上是该整数x的所有位的异或。也就是说，只要该整数的位模式有偶数个1(同时0也是偶数个)，那么该位最终的异或结果就一定是1 ^ 1 = 0，否则就一定为1 ^ 0 = 1。也就是说，当x有偶数个1时，该最低位是0，反之则为1。因此可以通过提取该最低为再取反来作为函数的返回结果。 总结 当出现判断某值是否在一个集合中出现偶数次或奇数次时，应该考虑异或的使用。 当需要做到将一个集合中所有的数值做异或，且这个所有值的异或应出现在集合的起始或结尾位置时，应考虑这种逐步移位，每次减半的做法。","categories":[{"name":"题解类","slug":"题解类","permalink":"https://blog.zhuwenq.cc/categories/%E9%A2%98%E8%A7%A3%E7%B1%BB/"}],"tags":[{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"位运算","slug":"位运算","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"节选自《全文》","slug":"hello-world","date":"2021-07-08T02:57:59.000Z","updated":"2023-06-21T06:51:28.932Z","comments":true,"path":"hello-world/","link":"","permalink":"https://blog.zhuwenq.cc/hello-world/","excerpt":"","text":"我把石头还给石头 今夜青稞只属于她自己 一切都在生长 今夜我只有美丽的戈壁 空空 姐姐，今夜我不关心人类，我只想你 海子 1988.7.25 火车经德令哈","categories":[{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/categories/poem/"}],"tags":[{"name":"hello","slug":"hello","permalink":"https://blog.zhuwenq.cc/tags/hello/"},{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/tags/poem/"}]},{"title":"hexo建站并部署到GitHub Pages","slug":"hello-world_old","date":"2020-03-01T16:00:00.000Z","updated":"2023-06-21T06:51:28.936Z","comments":true,"path":"hello-world_old/","link":"","permalink":"https://blog.zhuwenq.cc/hello-world_old/","excerpt":"GitHub Pages 是一项静态站点托管服务，它直接从 GitHub 上的仓库获取 HTML、CSS 和 JavaScript 文件，（可选）通过构建过程运行文件，然后发布网站。对于没有服务器和域名的广大人民群众来说，它简直就是搭建个人博客的首选托管平台。 Hexo是一个博客类的快速建站，快速部署脚手架，通过它，即使不具有任何前端知识，也可以快速地建立精美的个人博客网页。","text":"GitHub Pages 是一项静态站点托管服务，它直接从 GitHub 上的仓库获取 HTML、CSS 和 JavaScript 文件，（可选）通过构建过程运行文件，然后发布网站。对于没有服务器和域名的广大人民群众来说，它简直就是搭建个人博客的首选托管平台。 Hexo是一个博客类的快速建站，快速部署脚手架，通过它，即使不具有任何前端知识，也可以快速地建立精美的个人博客网页。 1-在GitHub上新建repository并设置GitHub-Pages 首先登陆你的GitHub帐号，新建一个repository 注意Repository name应该以自己的GitHub用户名+.github.io命名即：（这里因为我已经有了同名仓库所以报错） .github.io 进入repository的设置页面，设置GitHub Pages 在Theme Chooser 中随便选择一个主题或者不选择都可以。 如果你有其他的域名，也可以绑定在Custom domain中。 2-安装hexo并在本地建站 作为github用户，git的安装不再赘述，可以直接到Git官网下载安装。 安装Node.js。可直接到Node.js官网下载安装。 安装hexo并初始化blog文件夹 使用npm包管理工具安装hexo. 打开命令行，输入命令： $ npm install -g hexo-cli 安装完成之后，命令行进入你预备的博客文件夹(必须是空文件夹)，输入： $ hexo init 此时会发现文件夹中呈以下结构： 123456789└─&lt;yourblogfolder&gt; ├─node_modules/ ├─scaffolds/ ├─source/ ├─themes/ ├─.gitignore ├─_config.yml ├─package.json └─package_lock.json 更换主题 在hexo的主题页面上有许多漂亮的博客主题，选择一个进入它的github页面，找到项目地址。 输入以下命令安装:(这里我用的ayer主题) $ git clone https://github.com/Shen-Yu/hexo-theme-ayer.git themes/ayer 完成后打开themes文件夹，发现多了文件夹ayer/，里面便是ayer主题的文件。 回到blog文件夹下，打开_config.yml文件，找到 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: &#x27;&#x27; 代码块，将字段theme改成你下载的主题名。 其他更多配置都在文件_config.yml中，访问hexo文档查看详情。 3-网站部署 在_config.yml文件中，找到： 123456# Deployment## Docs: https://hexo.io/docs/deployment.html |deploy:type: gitrepo: https://github.com/Leonezz/Leonezz.github.io.gitbranch: master 其中字段repo填写你刚刚新建的GitHub repository地址。 该代码段实际上是在配置Hexo Deploy，它描述了要网站托管的GitHub地址和分支。但是hexo需要安装额外的插件才能正确执行该deploy。在blog文件夹下输入以下命令： $ npm install hexo-deployer-git --save 完成后，hexo的git 部署插件便安装到了你的blog文件夹。 输入以下三条命令： 123hexo clean #清除hexo缓存hexo g #生成网站hexo d #将网站部署到托管服务器 便完成了网站的部署，随后便可在域名https://&lt;你的github用户名&gt;.github.io访问你的博客网站了。 4-hexo-常用命令 1234567&gt; hexo init #初始化网站文件夹&gt; hexo clean #清除缓存&gt; hexo g #生成网站&gt; hexo s #在localHost预览网站&gt; hexo d #将网站部署到服务器&gt; hexo new page &quot;newMenu&quot; #新增的菜单项文件夹&gt; hexo n &quot;博客名字&quot; #新建博客 References hexo Doc GitHub+Hexo搭建个人网站详细教程 其他 刚刚建立好博客网站，作为第一篇博文，写下我建站时遇到的问题和查阅的资料以及解决的方法再合适不过了。","categories":[],"tags":[]}],"categories":[{"name":"知识类","slug":"知识类","permalink":"https://blog.zhuwenq.cc/categories/%E7%9F%A5%E8%AF%86%E7%B1%BB/"},{"name":"Photograph","slug":"Photograph","permalink":"https://blog.zhuwenq.cc/categories/Photograph/"},{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/categories/poem/"},{"name":"Paper Note","slug":"Paper-Note","permalink":"https://blog.zhuwenq.cc/categories/Paper-Note/"},{"name":"记录备忘","slug":"记录备忘","permalink":"https://blog.zhuwenq.cc/categories/%E8%AE%B0%E5%BD%95%E5%A4%87%E5%BF%98/"},{"name":"题解类","slug":"题解类","permalink":"https://blog.zhuwenq.cc/categories/%E9%A2%98%E8%A7%A3%E7%B1%BB/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://blog.zhuwenq.cc/tags/C/"},{"name":"Concurrency","slug":"Concurrency","permalink":"https://blog.zhuwenq.cc/tags/Concurrency/"},{"name":"atomic","slug":"atomic","permalink":"https://blog.zhuwenq.cc/tags/atomic/"},{"name":"CPU Cache","slug":"CPU-Cache","permalink":"https://blog.zhuwenq.cc/tags/CPU-Cache/"},{"name":"thread","slug":"thread","permalink":"https://blog.zhuwenq.cc/tags/thread/"},{"name":"storage duration","slug":"storage-duration","permalink":"https://blog.zhuwenq.cc/tags/storage-duration/"},{"name":"basic concepts","slug":"basic-concepts","permalink":"https://blog.zhuwenq.cc/tags/basic-concepts/"},{"name":"C++ Template","slug":"C-Template","permalink":"https://blog.zhuwenq.cc/tags/C-Template/"},{"name":"黑魔法","slug":"黑魔法","permalink":"https://blog.zhuwenq.cc/tags/%E9%BB%91%E9%AD%94%E6%B3%95/"},{"name":"多态","slug":"多态","permalink":"https://blog.zhuwenq.cc/tags/%E5%A4%9A%E6%80%81/"},{"name":"内存模型","slug":"内存模型","permalink":"https://blog.zhuwenq.cc/tags/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"name":"Memory Order","slug":"Memory-Order","permalink":"https://blog.zhuwenq.cc/tags/Memory-Order/"},{"name":"CS:APP","slug":"CS-APP","permalink":"https://blog.zhuwenq.cc/tags/CS-APP/"},{"name":"Computer System","slug":"Computer-System","permalink":"https://blog.zhuwenq.cc/tags/Computer-System/"},{"name":"Compute System","slug":"Compute-System","permalink":"https://blog.zhuwenq.cc/tags/Compute-System/"},{"name":"Linkage","slug":"Linkage","permalink":"https://blog.zhuwenq.cc/tags/Linkage/"},{"name":"Operating System","slug":"Operating-System","permalink":"https://blog.zhuwenq.cc/tags/Operating-System/"},{"name":"xv6","slug":"xv6","permalink":"https://blog.zhuwenq.cc/tags/xv6/"},{"name":"poem","slug":"poem","permalink":"https://blog.zhuwenq.cc/tags/poem/"},{"name":"PLM","slug":"PLM","permalink":"https://blog.zhuwenq.cc/tags/PLM/"},{"name":"NLP","slug":"NLP","permalink":"https://blog.zhuwenq.cc/tags/NLP/"},{"name":"BERT","slug":"BERT","permalink":"https://blog.zhuwenq.cc/tags/BERT/"},{"name":"Interpretability","slug":"Interpretability","permalink":"https://blog.zhuwenq.cc/tags/Interpretability/"},{"name":"FFN","slug":"FFN","permalink":"https://blog.zhuwenq.cc/tags/FFN/"},{"name":"Nueral Memory","slug":"Nueral-Memory","permalink":"https://blog.zhuwenq.cc/tags/Nueral-Memory/"},{"name":"Transfer Learning","slug":"Transfer-Learning","permalink":"https://blog.zhuwenq.cc/tags/Transfer-Learning/"},{"name":"Parameter-Efficient","slug":"Parameter-Efficient","permalink":"https://blog.zhuwenq.cc/tags/Parameter-Efficient/"},{"name":"Multimodal","slug":"Multimodal","permalink":"https://blog.zhuwenq.cc/tags/Multimodal/"},{"name":"Visual Knowledge","slug":"Visual-Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Visual-Knowledge/"},{"name":"Aoustic Knowledge","slug":"Aoustic-Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Aoustic-Knowledge/"},{"name":"遍历","slug":"遍历","permalink":"https://blog.zhuwenq.cc/tags/%E9%81%8D%E5%8E%86/"},{"name":"算法","slug":"算法","permalink":"https://blog.zhuwenq.cc/tags/%E7%AE%97%E6%B3%95/"},{"name":"线索树","slug":"线索树","permalink":"https://blog.zhuwenq.cc/tags/%E7%BA%BF%E7%B4%A2%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","permalink":"https://blog.zhuwenq.cc/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"图","slug":"图","permalink":"https://blog.zhuwenq.cc/tags/%E5%9B%BE/"},{"name":"最短路径","slug":"最短路径","permalink":"https://blog.zhuwenq.cc/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84/"},{"name":"哈夫曼树","slug":"哈夫曼树","permalink":"https://blog.zhuwenq.cc/tags/%E5%93%88%E5%A4%AB%E6%9B%BC%E6%A0%91/"},{"name":"堆","slug":"堆","permalink":"https://blog.zhuwenq.cc/tags/%E5%A0%86/"},{"name":"二叉树","slug":"二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"平衡二叉树","slug":"平衡二叉树","permalink":"https://blog.zhuwenq.cc/tags/%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"二叉搜索树","slug":"二叉搜索树","permalink":"https://blog.zhuwenq.cc/tags/%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"name":"Latex","slug":"Latex","permalink":"https://blog.zhuwenq.cc/tags/Latex/"},{"name":"Citation Style","slug":"Citation-Style","permalink":"https://blog.zhuwenq.cc/tags/Citation-Style/"},{"name":"Knowledge Injection","slug":"Knowledge-Injection","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Injection/"},{"name":"Entity Signal","slug":"Entity-Signal","permalink":"https://blog.zhuwenq.cc/tags/Entity-Signal/"},{"name":"QA","slug":"QA","permalink":"https://blog.zhuwenq.cc/tags/QA/"},{"name":"树","slug":"树","permalink":"https://blog.zhuwenq.cc/tags/%E6%A0%91/"},{"name":"队列","slug":"队列","permalink":"https://blog.zhuwenq.cc/tags/%E9%98%9F%E5%88%97/"},{"name":"Entity Represention","slug":"Entity-Represention","permalink":"https://blog.zhuwenq.cc/tags/Entity-Represention/"},{"name":"Entity Lookup Table","slug":"Entity-Lookup-Table","permalink":"https://blog.zhuwenq.cc/tags/Entity-Lookup-Table/"},{"name":"栈","slug":"栈","permalink":"https://blog.zhuwenq.cc/tags/%E6%A0%88/"},{"name":"Adapter","slug":"Adapter","permalink":"https://blog.zhuwenq.cc/tags/Adapter/"},{"name":"Knowledge","slug":"Knowledge","permalink":"https://blog.zhuwenq.cc/tags/Knowledge/"},{"name":"Knowledge Represention","slug":"Knowledge-Represention","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Represention/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Graph/"},{"name":"Embedding","slug":"Embedding","permalink":"https://blog.zhuwenq.cc/tags/Embedding/"},{"name":"ML","slug":"ML","permalink":"https://blog.zhuwenq.cc/tags/ML/"},{"name":"Knowledge Embedding","slug":"Knowledge-Embedding","permalink":"https://blog.zhuwenq.cc/tags/Knowledge-Embedding/"},{"name":"Text Generation","slug":"Text-Generation","permalink":"https://blog.zhuwenq.cc/tags/Text-Generation/"},{"name":"TransE","slug":"TransE","permalink":"https://blog.zhuwenq.cc/tags/TransE/"},{"name":"Sequence Prediction","slug":"Sequence-Prediction","permalink":"https://blog.zhuwenq.cc/tags/Sequence-Prediction/"},{"name":"CGEC","slug":"CGEC","permalink":"https://blog.zhuwenq.cc/tags/CGEC/"},{"name":"Transformer","slug":"Transformer","permalink":"https://blog.zhuwenq.cc/tags/Transformer/"},{"name":"XLNet","slug":"XLNet","permalink":"https://blog.zhuwenq.cc/tags/XLNet/"},{"name":"Autoregressive","slug":"Autoregressive","permalink":"https://blog.zhuwenq.cc/tags/Autoregressive/"},{"name":"Prompt","slug":"Prompt","permalink":"https://blog.zhuwenq.cc/tags/Prompt/"},{"name":"Survey","slug":"Survey","permalink":"https://blog.zhuwenq.cc/tags/Survey/"},{"name":"Qt","slug":"Qt","permalink":"https://blog.zhuwenq.cc/tags/Qt/"},{"name":"浮点数","slug":"浮点数","permalink":"https://blog.zhuwenq.cc/tags/%E6%B5%AE%E7%82%B9%E6%95%B0/"},{"name":"位级表示","slug":"位级表示","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E7%BA%A7%E8%A1%A8%E7%A4%BA/"},{"name":"位运算","slug":"位运算","permalink":"https://blog.zhuwenq.cc/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"逻辑代数","slug":"逻辑代数","permalink":"https://blog.zhuwenq.cc/tags/%E9%80%BB%E8%BE%91%E4%BB%A3%E6%95%B0/"},{"name":"hello","slug":"hello","permalink":"https://blog.zhuwenq.cc/tags/hello/"}]}